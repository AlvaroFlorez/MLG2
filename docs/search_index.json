[["selección-de-variables.html", "Capítulo 4 Selección de variables 4.1 Ejemplos 4.2 Problema de selección de variables 4.3 Métodos para la selección de variables 4.4 Comparación de los modelos", " Capítulo 4 Selección de variables 4.1 Ejemplos 4.1.1 Unidad quirúrgica Una unidad quirúrgica de un hospital está interesada en predecir la supervivencia de los pacientes sometidos a un tipo particular de operación hepática. Se dispuso de una selección aleatoria de \\(108\\) pacientes para el análisis. De cada registro del paciente, se extrajo la siguiente información de la evaluación preoperatoria: bcs: coagulación sanguínea. pindex: índice de pronóstico. enzyme: función enzimática. liver_test: función hepática. age: edad. gender: genero (0 = masculino, 1 = femenino). alc_mod: historial de consumo de alcohol (0 = Ninguno, 1 = Moderado). alc_heavy: &amp; historial de consumo de alcohol (0 = Ninguno, 1 = Fuerte). y: tiempo de supervivencia. El objetivo del estudio es determinar los factores que influyen sobre el tiempo de supervivencia (que se determinó posteriormente) en función de las demás variables. El modelo propuesto es el siguiente: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0}+\\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2}+ \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{liver}_{i}\\beta_{4} + \\mbox{age}_{i}\\beta_{5} + \\mbox{gender}_{i}\\beta_{6}+ \\\\ &amp; \\mbox{alc_mod}_{i}\\beta_{7} + \\mbox{alc_heavy}_{i}\\beta_{8} + \\varepsilon_{i} \\end{split} \\nonumber \\end{equation}\\] El ajuste del modelo es: library(olsrr) data(surgical) mod.surgical.completo = lm(log(y)~.,data=surgical) summary(mod.surgical.completo) ## ## Call: ## lm(formula = log(y) ~ ., data = surgical) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35555 -0.13849 -0.05179 0.14912 0.46349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.050949 0.251741 16.092 &lt; 2e-16 *** ## bcs 0.068551 0.025420 2.697 0.00982 ** ## pindex 0.013459 0.001947 6.913 1.37e-08 *** ## enzyme_test 0.014948 0.001809 8.261 1.44e-10 *** ## liver_test 0.007931 0.046706 0.170 0.86592 ## age -0.003567 0.002751 -1.296 0.20145 ## gender 0.084151 0.060746 1.385 0.17279 ## alc_mod 0.057313 0.067480 0.849 0.40019 ## alc_heavy 0.388190 0.088374 4.393 6.73e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2093 on 45 degrees of freedom ## Multiple R-squared: 0.8461, Adjusted R-squared: 0.8187 ## F-statistic: 30.93 on 8 and 45 DF, p-value: 7.823e-16 4.1.2 Grasa corporal La medición de la grasa corporal es un proceso complejo. Dado que los músculos y los huesos son más densos, el calculo de % de grasa corporal se basa, entre otros aspectos, en la medición de la densidad corporal la cuál requiere sumergir a las personas en el agua. Por esta razón se quiere buscar un método más sencillo para determinar el % de grasa corporal. Para esto, se registraron la edad, el peso, la altura y \\(10\\) medidas de la circunferencia corporal de \\(252\\) hombres. De igual forma, a cada uno de estos hombres se les midió el % de grasa corporal de forma precisa (usando la ecuación de Brozek, medición a partir de la densidad). Cómo variable respuesta se utiliza la medición por el método de Brozek, y las posibles covariables son: age: edad (en años). weight: peso (en libras). height: altura (en pulgadas). neck: circunferencia del cuello (en centímetros). chest: circunferencia del pecho (en centímetros). abdom: circunferencia del abdomen (en centímetros). hip: circunferencia de la cadera (en centímetros). thigh:circunferencia del muslo (en centímetros). knee:circunferencia de la rodilla (en centímetros). ankle:circunferencia del tobillo (en centímetros). biceps: circunferencia del bíceps extendido (en centímetros). forearm: circunferencia del antebrazo (en centímetros). ``wrist```: circunferencia de la muñeca (en centímetros). El modelo propuesto es el siguiente: \\[ \\mbox{brozek}_i = \\beta_{0} + \\mbox{age}_i\\beta_1+ \\mbox{weight}_i\\beta_2 + \\ldots + \\mbox{wrist}_i\\beta_{13} + \\varepsilon_i. \\] El ajuste del modelo es: library(faraway) data(fat) mod.fat &lt;- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat) summary(mod.fat) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + chest + ## abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, ## data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.264 -2.572 -0.097 2.898 9.327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -15.29255 16.06992 -0.952 0.34225 ## age 0.05679 0.02996 1.895 0.05929 . ## weight -0.08031 0.04958 -1.620 0.10660 ## height -0.06460 0.08893 -0.726 0.46830 ## neck -0.43754 0.21533 -2.032 0.04327 * ## chest -0.02360 0.09184 -0.257 0.79740 ## abdom 0.88543 0.08008 11.057 &lt; 2e-16 *** ## hip -0.19842 0.13516 -1.468 0.14341 ## thigh 0.23190 0.13372 1.734 0.08418 . ## knee -0.01168 0.22414 -0.052 0.95850 ## ankle 0.16354 0.20514 0.797 0.42614 ## biceps 0.15280 0.15851 0.964 0.33605 ## forearm 0.43049 0.18445 2.334 0.02044 * ## wrist -1.47654 0.49552 -2.980 0.00318 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.988 on 238 degrees of freedom ## Multiple R-squared: 0.749, Adjusted R-squared: 0.7353 ## F-statistic: 54.63 on 13 and 238 DF, p-value: &lt; 2.2e-16 4.2 Problema de selección de variables En problemas de regresión se tiene un conjunto grande de potenciales covariables. Si ajustamos un modelo considerandolas todas podemos estar incluyendo covariables que son irrelevante. Por el otro lado, si no las incluimos todas es posible que estemos omitiendo covariables importantes. En ambos casos hay consecuencias negativas. Para illustrar esto, considere el siguiente modelo: \\[\\begin{equation} \\begin{split} y_{i} &amp;= \\beta_{0} + \\sum_{j=1}^{p-1}\\beta_{j}x_{ij} + \\varepsilon_{i} \\\\ &amp;= \\beta_{0} + \\sum_{j=1}^{r}\\beta_{j}x_{ij} + \\sum_{j=r+1}^{p-1}\\beta_{j}x_{ij} + \\varepsilon_{i} \\\\ &amp;= \\boldsymbol x_{1i}&#39;\\boldsymbol \\beta_1 + \\boldsymbol x_{2i}&#39;\\boldsymbol \\beta_2 + \\varepsilon_i, \\end{split} \\label{eq:modelogral} \\end{equation}\\] donde \\(\\boldsymbol x_{1i} = (1,x_{1i},x_{2i},\\ldots,x_{ri})\\), \\(\\boldsymbol x_{2i} = (x_{r+1,i},x_{r+2,i},\\ldots,x_{p-1,i})\\), \\(\\boldsymbol \\beta_1 = (\\beta_0,\\beta_1,\\beta_2,\\ldots,\\beta_r)&#39;\\), \\(\\boldsymbol \\beta_2 = (\\beta_{r+1},\\beta_{r+2},\\ldots,\\beta_{p-1})&#39;\\), y \\(\\varepsilon_i \\sim N(0,\\sigma^{2})\\). Es decir, se hace una partición de las covariables y los coeficientes de regressión en dos componentes. En forma matricial, el modelo es: \\[ \\boldsymbol y= \\boldsymbol X_{1}\\boldsymbol \\beta_{1} + \\boldsymbol X_{2}\\boldsymbol \\beta_{2}+ \\boldsymbol \\varepsilon, \\] donde \\(\\boldsymbol X_{1}\\) es una matriz \\(n \\times r\\) con la \\(i\\)-ésima fila igual a \\(\\boldsymbol x_{1i}\\) y \\(\\boldsymbol X_{2}\\) es una matriz \\(n \\times (p-r-1)\\) con la \\(i\\)-ésima fila igual a \\(\\boldsymbol x_{2i}\\). 4.2.1 ¿Qué pasa si ignoramos covariables importantes? Ahora, considere que el modelo de regresión real es (??), pero decidimos estimar: \\[ y_{i} = \\boldsymbol x_{1i}&#39;\\boldsymbol \\beta_1 + \\varepsilon_i. \\] Por lo tanto, estamos omitiendo las covariables \\(\\boldsymbol x_{2i}\\) del modelo (puesto que \\(\\boldsymbol \\beta_2 \\neq 0\\)). El estimador por MCO de \\(\\boldsymbol \\beta_1\\) es: \\[ \\widehat{\\boldsymbol \\beta}_{1} = (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol y. \\] De aquí tenemos que \\(E(\\widehat{\\boldsymbol \\beta}_{1}) = \\boldsymbol \\beta_{1} + (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\boldsymbol \\beta_{2}\\). Es decir que \\(\\widehat{\\boldsymbol \\beta}_{1}\\) es un estimador sesgado, a menos que \\(\\boldsymbol X_{1}&#39;\\boldsymbol X_{2} = \\boldsymbol 0\\) (las columnas de \\(X_{1}\\) son ortogonales a las columnas de \\(X_{2}\\)). De igual forma, las predicciones también serán sesgadas. La predicción en el punto \\(\\boldsymbol x_{01}\\) es: \\[ \\widehat{y}_{0} = \\boldsymbol x_{01}&#39;\\widehat{\\boldsymbol \\beta}_{1}. \\] Su valor esperado es: \\[ E(\\widehat{y}_{0}) = \\boldsymbol x_{01}&#39;\\boldsymbol \\beta_{1} + \\boldsymbol x_{01}&#39;(\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\boldsymbol \\beta_{2} \\neq \\boldsymbol x_{01}&#39;\\beta_{1} + \\boldsymbol x_{02}&#39;\\beta_{2}. \\] Por lo tanto, si omitimos variables relevantes obtenemos sesgo en las estimaciones. 4.2.2 ¿Que pasa si incluimos covariables irrelevantes? Ahora, consideremos el caso en que \\(\\boldsymbol \\beta_2=0\\), es decir, las covariables \\(\\boldsymbol x_{2}\\) no tienen un aporte significativo en el modelo. Pero decidimos estimar el modelo completo. En este caso, el estimador de \\(\\boldsymbol \\beta\\) es: \\[ \\widehat{\\boldsymbol \\beta}= (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol y= \\begin{pmatrix} \\boldsymbol X_{1}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{1}\\boldsymbol X_{2} \\\\ \\boldsymbol X_{2}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{2}&#39;\\boldsymbol X_{2} \\end{pmatrix}^{-1} \\begin{pmatrix} \\boldsymbol X_{1}&#39; \\\\ \\boldsymbol X_{2}&#39; \\end{pmatrix}\\boldsymbol y. \\] El Valor esperado de \\(\\widehat{\\boldsymbol \\beta}\\) es: \\[\\begin{equation} \\begin{split} E(\\widehat{\\boldsymbol \\beta}) =&amp; (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;E(\\boldsymbol y) = (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol X_{1}\\boldsymbol \\beta_1 \\\\ = &amp; (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;(\\boldsymbol X_{1} \\ \\boldsymbol X_{2}) \\begin{pmatrix} \\boldsymbol \\beta_1 \\\\ \\boldsymbol 0 \\end{pmatrix} = \\begin{pmatrix} \\boldsymbol \\beta_1 \\\\ \\boldsymbol 0 \\end{pmatrix}. \\end{split} \\nonumber \\end{equation}\\] Es decir que \\(\\widehat{\\boldsymbol \\beta}\\) es un estimador insesgado. La varianza de \\(\\widehat{\\boldsymbol \\beta}\\) es: \\[\\begin{equation} \\begin{split} V(\\widehat{\\boldsymbol \\beta}) =&amp; \\sigma^{2}(\\boldsymbol X&#39;\\boldsymbol X)^{-1} = \\sigma^{2}\\begin{pmatrix} \\boldsymbol X_{1}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{1}\\boldsymbol X_{2} \\\\ \\boldsymbol X_{2}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{2}&#39;\\boldsymbol X_{2} \\end{pmatrix}^{-1} \\\\ =&amp; \\sigma^{2} \\begin{pmatrix} (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1} + \\boldsymbol L\\boldsymbol M\\boldsymbol L&amp; - \\boldsymbol L\\boldsymbol M\\\\ -\\boldsymbol M\\boldsymbol L&#39; &amp; \\boldsymbol M \\end{pmatrix}, \\end{split} \\nonumber \\end{equation}\\] donde \\(\\boldsymbol L= (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\) y \\(\\boldsymbol M= \\boldsymbol X_{2}&#39;(\\boldsymbol I- \\boldsymbol H_{1})\\boldsymbol X_{2}\\). Particularmente, para \\(\\widehat{\\boldsymbol \\beta}_1\\) tenemos que: \\[ V(\\widehat{\\boldsymbol \\beta}_{1}) = \\sigma^{2} \\left[ (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1} + \\boldsymbol L\\boldsymbol M\\boldsymbol L\\right]. \\] Dado que \\(\\boldsymbol M\\) (y por lo tanto \\(\\boldsymbol L\\boldsymbol M\\boldsymbol L\\)) es positiva-definida, la varianza de \\(\\widehat{\\boldsymbol \\beta}_{1}\\) se infla al incluir las covariables irrelevantes al modelo. La única excepción es cuando \\(\\boldsymbol X_{1}\\) y \\(\\boldsymbol X_{2}\\) son ortogonales (\\(\\boldsymbol X_{1}&#39;\\boldsymbol X_{2} = \\boldsymbol 0\\)). De igual forma, las predicciones en el punto \\(\\boldsymbol x_{0}&#39; = (\\boldsymbol x_{01}&#39; \\ \\boldsymbol x_{02}&#39;)\\) son insesgadas: \\[ E(\\widehat{y}_{0}) = E(\\boldsymbol x_{0}&#39;\\widehat{\\boldsymbol \\beta}) = (\\boldsymbol x_{01}&#39; \\ \\boldsymbol x_{02}&#39;)\\begin{pmatrix} \\boldsymbol \\beta_{1} \\\\ \\boldsymbol 0 \\end{pmatrix} = \\boldsymbol x_{01}&#39;\\boldsymbol \\beta_{1}. \\] Pero su varianza también se infla debido a incluir las covariables irrelevantes: \\[ V(\\widehat{y}_{0}) = \\sigma^{2} \\boldsymbol x_{0}&#39;(\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol x_{0}. \\] En conclusión: Cuando omitimos covariables relevantes, obtenemos sesgos en las estimaciones. Cuando incluimos covariables irrelevantes, se inflan las varianzas de las estimaciones. Adicionalmente, incluir más covariables puede llevar a problemas de multicolinealidad. 4.3 Métodos para la selección de variables Si tenemos \\((p-1)\\) covariables, entonces tenemos \\((p-1)^2\\) potenciales modelos. Por lo que podemos ajustar todos los posibles modelos y hacer una comparación entre ellos usando algunos criterios de decisión. Existen varios criterios para determinar que modelo es ``mejor que otro y este debe escogerse teniendo en cuenta cuál es el objetivo que se tiene al ajustar el modelo (descripción de la relación, predicción, control, etc.). Algunos de estos críterios son: Coeficiente de determinación (\\(R^{2}\\) y \\(R^{2}_{adj}\\)). Estadístico \\(C_{p}\\) de Mallows. Estadístico PRESS y el \\(R^{2}\\) de predicción. Criterios de información (AIC y BIC). 4.3.1 Coeficiente de determinación Esté indicador está definido como: \\[ R^{2} = \\frac{SS_{\\mbox{reg}}}{SS_{\\mbox{T}}} = 1 - \\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}}. \\] El \\(R^{2}\\) cuantifica la cantidad de variabilidad de la variable respuesta que es explicada por el modelo. Se tiene que \\(0 \\leq R^{2} \\leq 1\\). Valores más cercanos a \\(1\\) implican que el modelo explica gran parte de la variabilidad de \\(y\\). Hay que tener en cuenta que el \\(R^{2}\\) siempre crece a medida que se adicionan más covariables al modelo. Por lo tanto, se puede puede agregar regresores hasta el punto en que una covariable adicional no propociona un aumento considerable en el \\(R^{2}\\). 4.3.2 Coeficiente de determinación ajustado Para evitar el incoviente del \\(R^{2}\\), se puede utlizar el el coeficiente de determinación ajustado definido como: \\[ R^{2}_{adj} = 1 - \\frac{n-1}{n-p}\\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}} = 1- \\frac{MS_{\\mbox{res}}}{SS_{\\mbox{T}}/(n-1)} = 1- \\frac{n-1}{n-p}(1-R^{2}). \\] El \\(R^{2}_{adj}\\) no necesariamente aumenta al adicionar nuevos términos al modelo. Este solo aumenta si hay una disminución del \\(MS_{\\mbox{res}}\\). 4.3.3 C\\(_p\\) de Mallows Mallows propone un criterio basado en el error cuadrático medio (ECM) de \\(\\widehat{y}_i\\), esto es: \\[ E[\\widehat{y}_{i}- E(y_{i})]^2 = [E(y_{i}) - E(\\widehat{y}_{i})]^2 + V(\\widehat{y}_{i}), \\] donde \\(E(y_{i})\\) es el valor esperado de la respuesta (modelo real), y \\(E(\\widehat{y}_{i})\\) es el valor esperado de la respuesta basado en el modelo propuesto (basado en \\(p\\) covariables). El ECM total estandarizado está definido como: \\[\\begin{equation} \\begin{split} \\Gamma_{p} =&amp; \\frac{1}{\\sigma^{2}}\\left\\{\\sum_{i=1}^{n}[E(y_{i}) - E(\\widehat{y}_{i})]^2 + \\sum_{i=1}^{n} V(\\widehat{y}_{i}) \\right\\} \\\\ =&amp; \\frac{1}{\\sigma^{2}}\\left\\{SS_{B}(p) + \\sum_{i=1}^{n} V(\\widehat{y}_{i}) \\right\\} = \\frac{1}{\\sigma^{2}}\\left\\{SS_{B}(p) + p\\sigma^{2} \\right\\} \\\\ =&amp; \\frac{1}{\\sigma^{2}}\\left\\{ E[SS_{\\mbox{res}}(p)] - (n-p)\\sigma^{2} + p\\sigma^{2} \\right\\} \\\\ =&amp; \\frac{E[SS_{\\mbox{res}}(p)]}{\\sigma^{2}} - n + 2p. \\end{split} \\nonumber \\end{equation}\\] Reemplazando \\(E[SS_{\\mbox{res}}(p)]\\) por \\(SS_{\\mbox{res}}(p)\\), y asumiendo que \\(MS_{\\mbox{res}}(p^{*})\\) (calculado usando el modelo completo) es un buen estimador de \\(\\sigma^{2}\\): \\[ C_{p} = \\frac{SS_{\\mbox{res}}(p)}{MS_{\\mbox{res}}(p^{*})} - n + 2p. \\] Por lo tanto, para el modelo completo \\(C_{p} = p^{*}\\). Si \\(E[SS_{\\mbox{res}}(p)] = (n-p)\\sigma^{2}\\) (asumiendo que \\(SS_{B}(p)=0\\)), tenemos que: \\[ E[C_{p}| \\mbox{Sesgo}=0] = \\frac{(n-p)\\sigma^{2}}{\\sigma^{2}} - n +2p = p. \\] Si el modelo propuesto es insesgado se espera que el \\(C_p\\) esté cercano a \\(p\\). Aunque se espera que el \\(C_p=p\\), es deseable que \\(C_p &lt; p\\). Por lo tanto, modelos con valores pequeños de \\(C_p\\) son mejores. 4.3.4 Estadístico PRESS El estadístico PRESS (prediction error sum of squares) está definido como: \\[ \\mbox{PRESS} = \\sum_{i=1}^{n} (y_{i} - \\widehat{y}_{(i)})^{2} = \\sum_{i=1}^{n} \\left( \\frac{\\epsilon_{i}}{1-h_{ii}} \\right)^{2}. \\] Para comparar modelos, menor valor del PRESS indica que el modelo es mejor para hacer predicciones. A partir del PRESS se puede calcular el \\(R^{2}\\) de predicción: \\[ R^{2}_{pred} = 1 - \\frac{PRESS}{SST}. \\] Basado en este criterio, mayor es el valor de \\(R^{2}_{pred}\\) mejor es el modelo para hacer predicciones. La ventaja del PRESS y \\(R_{pred}^{2}\\) es que evitan el sobreajuste dado que se calculan utilizando observaciones no incluidas en la estimación del modelo. 4.3.5 Criterios de información La idea es comparar modelos estimados teniendo en cuenta la bondad de ajuste del modelo (verosimilitud, \\(L\\)) y su complejidad (número de parámetros). El criterio de información de Akaike está definido como: \\[ \\mbox{AIC} = -2\\log (L) + 2p. \\] El criterio de información bayesiano (o de Schwarz - SBC): \\[ \\mbox{BIC} = -2\\log (L) + p\\log n. \\] Es preferible modelos con valores menores de AIC o BIC. Dado que la penalización del BIC es mayor (si \\(n &gt; 7\\)), este indicador tiende a preferir modelos con menor número de covariables. Recordemos que la log-verosimilitud es: \\[ \\log L(\\boldsymbol \\beta,\\sigma^{2}) = - \\frac{n}{2}\\log (2\\pi) - n\\log(\\sigma) - \\frac{1}{2\\sigma^{2}}(\\boldsymbol y- \\boldsymbol X\\boldsymbol \\beta)&#39;(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta). \\] El estimador por máxima verosimilitud de \\(\\sigma^{2}\\) es \\(\\widehat{\\sigma}=SS_{\\mbox{res}}/n\\). Por lo tanto, el máximo valor de la log-verosimilitud es: \\[ \\log L(\\widehat{\\boldsymbol \\beta},\\widehat{\\sigma}^{2}) = -\\frac{n}{2}\\log (2\\pi) - \\frac{n}{2}\\log\\widehat{\\sigma}^{2} - \\frac{1}{2\\widehat{\\sigma}^{2}}SS_{\\mbox{res}}= -\\frac{n}{2}\\log (SS_{\\mbox{res}}/n) + \\mbox{constante}. \\] Por lo tanto: \\[ AIC \\propto n\\log(SS_{\\mbox{res}}/n) + 2p \\mbox{ y } BIC \\propto n\\log(SS_{\\mbox{res}}/n) + p\\log n. \\] Hay varias adaptaciones de estos criterios de información definiendo diferentes penalizaciones. 4.4 Comparación de los modelos 4.4.1 Todos los posibles modelos Con la función ols_step_all_possible() de la librería olsrr es posible ajustar todos posibles modelos y determinar el mejor bajo diferentes criterios. Otra alternativa es la función regsubsets() de la librería leaps. Está función es más rápida (se basa en un algoritmo más eficiente), pero no es user-friendly. 4.4.1.1 Datos de unidad quirúrgica A través de la función ols_step_all_possible podemos ajustar los \\(255\\) modelos que se pueden ajustar usando las ocho posibles covariables: surgical.all.mods=ols_step_all_possible(mod.surgical.completo) Además de ajustar los modelos, se calculan varios criterios (\\(R^{2}\\),\\(R^{2}_{adj}\\), \\(R^{2}_{pred}\\),AIC,BIC,) para cada uno de ellos. Puesto que son muchos modelos, podemos organizar los resultados de tal forma que obtengamos los mejores modelos basándonos en cada uno de los criterios. Por ejemplo, los 5 mejores ajustes según el \\(R^{2}_{adj}\\) son: R2adj.order = order(surgical.all.mods$adjr,decreasing = T) as.data.frame(surgical.all.mods)[R2adj.order[1:5],c(2:8,10)] ## n predictors rsquare ## 226 6 bcs pindex enzyme_test age gender alc_heavy 0.8434664 ## 251 7 bcs pindex enzyme_test age gender alc_mod alc_heavy 0.8460095 ## 171 5 bcs pindex enzyme_test gender alc_heavy 0.8374622 ## 248 7 bcs pindex enzyme_test liver_test age gender alc_heavy 0.8436412 ## 169 5 bcs pindex enzyme_test age alc_heavy 0.8358522 ## adjr predrsq cp aic sbc ## 226 0.8234834 0.7836037 5.772458 -8.612898 7.298974 ## 251 0.8225761 0.7806940 7.028837 -7.497389 10.403468 ## 171 0.8205312 0.7827597 5.528174 -8.580332 5.342556 ## 248 0.8198474 0.7749807 7.721367 -6.673207 11.227649 ## 169 0.8187535 0.7862369 5.998959 -8.048073 5.874815 Entonces, basándonos en el \\(R^{2}_{adj}\\) el mejor ajuste se obtiene con el modelo considerando las covariables bcs, pindex, enzyme_test, age, gender, y alc_heavy. Es decir, eliminando las covariables función hepática y consumo de alcohol moderado. Note que no todos los demás críterios sugieren el mismo modelo. Si eliminamos las covariables gender obtenemos un modelo con un \\(R^{2}_{pred}\\) más alto. Ahora, si nos apoyamos en el AIC, los mejores 5 ajustes son: AIC.order = order(surgical.all.mods$aic,decreasing = F) as.data.frame(surgical.all.mods)[AIC.order[1:5],c(2:8,10)] ## n predictors rsquare adjr ## 226 6 bcs pindex enzyme_test age gender alc_heavy 0.8434664 0.8234834 ## 171 5 bcs pindex enzyme_test gender alc_heavy 0.8374622 0.8205312 ## 97 4 bcs pindex enzyme_test alc_heavy 0.8299187 0.8160345 ## 169 5 bcs pindex enzyme_test age alc_heavy 0.8358522 0.8187535 ## 251 7 bcs pindex enzyme_test age gender alc_mod alc_heavy 0.8460095 0.8225761 ## predrsq cp aic sbc ## 226 0.7836037 5.772458 -8.612898 7.298974 ## 171 0.7827597 5.528174 -8.580332 5.342556 ## 97 0.7862922 5.733992 -8.130569 3.803335 ## 169 0.7862369 5.998959 -8.048073 5.874815 ## 251 0.7806940 7.028837 -7.497389 10.403468 Con este críterio se escoge el mismo modelo que con el \\(R^{2}_{adj}\\). Sin embargo, podemos observar que el BIC sugiere eliminar la covariable asociada a la edad. En la Figura ?? muestra los \\(R^{2}\\),\\(R^{2}_{adj}\\), \\(R^{2}_{pred}\\),C\\(_p\\), AIC y BIC (SBC) para todos los posibles ajustes. Note que, dentro de cada subgrupo de modelos (determinado por el número de covariables), los criterios eligen los modelos en el mismo orden. La diferencia está en el número de covariables a elegir. Generalmente, el BIC prefiere modelos más parsimoniosos. Esto no ocurre con criterios de validación cruzada, como el PRESS o \\(R^{2}_{pred}\\). plot(surgical.all.mods) Figure 4.1: Valores de los criterios de selección calculados para cada uno de todos los posibles modelos. Figure 4.2: Valores de los criterios de selección calculados para cada uno de todos los posibles modelos. Teniendo en cuenta esto, con la función ols_step_best_subset() selecciona el mejor modelo para cada subconjunto de número de covariables basándose en los diferentes criterios: ols_step_best_subset(mod.surgical.completo) ## Best Subsets Regression ## ----------------------------------------------------------------------------- ## Model Index Predictors ## ----------------------------------------------------------------------------- ## 1 enzyme_test ## 2 pindex enzyme_test ## 3 pindex enzyme_test alc_heavy ## 4 bcs pindex enzyme_test alc_heavy ## 5 bcs pindex enzyme_test gender alc_heavy ## 6 bcs pindex enzyme_test age gender alc_heavy ## 7 bcs pindex enzyme_test age gender alc_mod alc_heavy ## 8 bcs pindex enzyme_test liver_test age gender alc_mod alc_heavy ## ----------------------------------------------------------------------------- ## ## Subsets Regression Summary ## -------------------------------------------------------------------------------------------------------------------------------- ## Adj. Pred ## Model R-Square R-Square R-Square C(p) AIC SBIC SBC MSEP FPE HSP APC ## -------------------------------------------------------------------------------------------------------------------------------- ## 1 0.4273 0.4162 0.3496 117.4783 51.4343 -105.4395 57.4013 7.6160 0.1463 0.0028 0.6168 ## 2 0.6632 0.6500 0.6044 50.4918 24.7668 -131.5971 32.7228 4.5684 0.0893 0.0017 0.3765 ## 3 0.7780 0.7647 0.7291 18.9015 4.2432 -150.4023 14.1881 3.0718 0.0610 0.0012 0.2575 ## 4 0.8299 0.8160 0.7863 5.7340 -8.1306 -160.5329 3.8033 2.4030 0.0486 9e-04 0.2048 ## 5 0.8375 0.8205 0.7828 5.5282 -8.5803 -160.2288 5.3426 2.3453 0.0482 9e-04 0.2032 ## 6 0.8435 0.8235 0.7836 5.7725 -8.6129 -159.4064 7.2990 2.3077 0.0482 9e-04 0.2032 ## 7 0.8460 0.8226 0.7807 7.0288 -7.4974 -157.6344 10.4035 2.3207 0.0492 0.0010 0.2076 ## 8 0.8461 0.8187 0.7711 9.0000 -5.5320 -155.2573 14.3579 2.3719 0.0511 0.0010 0.2154 ## -------------------------------------------------------------------------------------------------------------------------------- ## AIC: Akaike Information Criteria ## SBIC: Sawa&#39;s Bayesian Information Criteria ## SBC: Schwarz Bayesian Criteria ## MSEP: Estimated error of prediction, assuming multivariate normality ## FPE: Final Prediction Error ## HSP: Hocking&#39;s Sp ## APC: Amemiya Prediction Criteria A partir de estos resultados, y con la ayuda de expertos en el tema, se puede hacer una selección del mejor modelo para hacer las predicciónes. 4.4.2 Algorítmos de selección Para el proceso de selección, la mejor opción es evaluar todos los posibles modelos. Sin embargo, en la presencia de muchas posibles covariables este proceso puede requerir una carga computacional muy alta. Por esta razón, se han desarrollado varios algoritmos para evaluar solo un subconjunto de modelos agregando o eliminando covariables una a la vez. 4.4.2.1 Selección hacia delante (forward selection) Este algoritmo parte del modelo sin ninguna covariable (es decir, solo el intercepto) y el ajuste ``óptimo se encuentra ingresando covariables una a la vez basándose en algún criterio (por ejemplo AIC). La primera covariable se escoge luego de ajustar los \\((p-1)\\) modelos simples con cada uno de los regresores. Por ejemplo, seleccionado la covariable que proporciona el mejor AIC. Luego se ajustan los modelos combinando la covariable previamente seleccionada con cada una de los restantes \\((p-2)\\) regresores. Si el mejor ajuste con dos covariables proporcina un menor AIC que en el paso anterior, continuamos seleccinando la tercer covariable de la misma forma. El algoritmo continua seleccionando covariables hasta que se satisface un criterio de parada (por ejemplo, hasta que el AIC aumente). 4.4.2.2 Selección hacia atrás (backward selection) Con este algoritmo se empieza evaluando el modelo con todas las covariables candidatas y se van eliminando covariables una a una hasta que un criterio de parada se satisface (por ejemplo, hasta que el AIC aumenta). 4.4.2.3 Selección por segmentos (stepwise selection) Aquí se siguen los mismos pasos que la selección hacia delante. Pero en cada paso se evalúan de nuevo los candidatos que ya habían ingresado en el modelo. Por lo tanto, una covariable que ya esté en el modelo puede ser eliminada en algún paso posterior. 4.4.2.4 Unidad quirúrgica Consideremos el modelo anterior adicionando las interacciones de las covariables continuas con las categóricas: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0}+\\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2}+ \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{liver}_{i}\\beta_{4} + \\mbox{age}_{i}\\beta_{5} + \\mbox{gender}_{i}\\beta_{6}+ \\mbox{alc_mod}_{i}\\beta_{7} + \\\\ &amp; \\mbox{alc_heavy}_{i}\\beta_{8} \\mbox{bcs}_{i}\\mbox{gender}_{i}\\beta_{9} + \\mbox{pindex}_{i}\\mbox{gender}_{i}\\beta_{10} + \\mbox{enzyme}_{i}\\mbox{gender}_{i}\\beta_{11} + \\mbox{liver}_{i}\\mbox{gender}_{i}\\beta_{12} + \\\\ &amp; \\mbox{age}_{i}\\mbox{gender}_{i}\\beta_{13} + \\mbox{bcs}_{i}\\mbox{alc_mod}_{i}\\beta_{14} + \\mbox{pindex}_{i}\\mbox{alc_mod}_{i}\\beta_{15} + \\mbox{enzyme}_{i}\\mbox{alc_mod}_{i}\\beta_{16} + \\\\ &amp; \\mbox{liver}_{i}\\mbox{alc_mod}_{i}\\beta_{17} + \\mbox{age}_{i}\\mbox{alc_mod}_{i}\\beta_{18} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{19} + \\mbox{pindex}_{i}\\mbox{alc_heavy}_{i}\\beta_{20} + \\\\ &amp; \\mbox{enzyme}_{i}\\mbox{alc_heavy}_{i}\\beta_{21} + \\mbox{liver}_{i}\\mbox{alc_heavy}_{i}\\beta_{22} + \\mbox{age}_{i}\\mbox{alc_heavy}_{i}\\beta_{23} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] En este caso tenemos \\(2^{23}=8&#39;388,608\\) posibles modelos. Lo que hace que sea difícil ajustarlos todos (aunque es posible usando la librería leaps). Por lo tanto, vamos a utilizar los algortimos de selección. Selección hacia delante. Podemos utilizar la función ols_step_forward_aic de la librería olsrr: mod.surgical.completo2 = lm(log(y)~bcs*gender+pindex*gender+enzyme_test*gender+liver_test*gender+age*gender+ bcs*alc_mod+pindex*alc_mod+enzyme_test*alc_mod+liver_test*alc_mod+age*alc_mod+bcs*alc_heavy+pindex*alc_heavy+enzyme_test*alc_heavy+liver_test*alc_heavy+age*alc_heavy,data=surgical) res =ols_step_forward_aic(mod.surgical.completo2,details = F) Con el argumento details = T se puede ver la selección con más detalle. Usando este algoritmo el modelo óptimo es: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{age}_{i}\\beta_{4} + \\mbox{gender}_{i}\\beta_{5} + \\\\ &amp;\\mbox{gender}_{i}\\mbox{pindex}_{i}\\beta_{6} + \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{7} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{8} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Selección hacia atrás. Podemos utilizar la función ols_step_backward_aic de la librería olsrr: ols_step_backward_aic(mod.surgical.completo2,details = F) ## ## ## Backward Elimination Summary ## --------------------------------------------------------------------------- ## Variable AIC RSS Sum Sq R-Sq Adj. R-Sq ## --------------------------------------------------------------------------- ## Full Model -9.135 1.058 11.747 0.91740 0.85408 ## age:alc_heavy -11.134 1.058 11.747 0.91740 0.85878 ## alc_heavy -13.133 1.058 11.747 0.91740 0.86319 ## bcs:alc_mod -15.057 1.059 11.745 0.91728 0.86715 ## age:alc_mod -16.852 1.063 11.741 0.91697 0.87057 ## gender:pindex -18.639 1.067 11.737 0.91664 0.87377 ## alc_mod -19.980 1.080 11.724 0.91562 0.87577 ## enzyme_test:alc_heavy -20.698 1.106 11.698 0.91359 0.87622 ## liver_test:alc_heavy -20.988 1.142 11.662 0.91081 0.87560 ## pindex:alc_heavy -22.223 1.158 11.646 0.90954 0.87706 ## pindex:alc_mod -22.957 1.186 11.619 0.90739 0.87729 ## bcs:gender -22.981 1.230 11.575 0.90394 0.87583 ## gender:age -23.038 1.275 11.529 0.90042 0.87434 ## gender:liver_test -23.549 1.311 11.494 0.89764 0.87383 ## age -23.745 1.355 11.449 0.89416 0.87251 ## --------------------------------------------------------------------------- Por lo tanto, el modelo seleccionado es: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{gender}_{i}\\beta_{4} + \\mbox{liver}_{i}\\beta_{5} + \\\\ &amp; \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{6} + \\mbox{enzyme}_{i}\\mbox{alc_mod}_{i}\\beta_{7} + \\mbox{liver}_{i}\\mbox{alc_mod}_{i}\\beta_{8} + \\mbox{liver}_{i}\\mbox{gender}_{i}\\beta_{9} \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Con este algoritmo no se considera la edad del paciente pero si la función hepática y otras interacciones. Selección por segmentos. Aquí tenemos la función ols_step_both_aic de la librería olsrr: ols_step_both_aic(mod.surgical.completo2,details = F) ## ## ## Stepwise Summary ## ------------------------------------------------------------------------------------ ## Variable Method AIC RSS Sum Sq R-Sq Adj. R-Sq ## ------------------------------------------------------------------------------------ ## enzyme_test addition 51.434 7.334 5.471 0.42725 0.41624 ## pindex addition 24.767 4.313 8.492 0.66318 0.64997 ## bcs:alc_heavy addition -3.014 2.485 10.320 0.80596 0.79432 ## bcs addition -9.430 2.126 10.678 0.83396 0.82041 ## gender:pindex addition -10.781 1.998 10.806 0.84395 0.82770 ## gender:enzyme_test addition -19.676 1.633 11.171 0.87246 0.85618 ## gender addition -23.040 1.479 11.326 0.88452 0.86695 ## gender:pindex removal -23.631 1.518 11.287 0.88147 0.86634 ## age addition -25.088 1.424 11.381 0.88882 0.87190 ## ------------------------------------------------------------------------------------ Note que este método sigue los mismos pasos que la selección hacia delante hasta el paso 8 donde se elimina la interacción entre el índice de pronostico y el genero. Por lo que aquí obtenemos el siguiente modelo: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{age}_{i}\\beta_{4} + \\mbox{gender}_{i}\\beta_{5} + \\\\ &amp; \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{6} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{7} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Dado que los algoritmos hacen la busqueda del modelo ``óptimo evaluando diferentes subconjuntos de covariables, se obtuvieron diferentes ajustes. Si observamos el AIC de las tres opciones, el modelo obtenido con el algoritmo stepwise presenta el mejor resultado. 4.4.3 Regresión de LASSO El estimador LASSO (Least Absolute Selection and Shrinkage Operator) minimiza la siguiente expresión: \\[\\begin{equation} \\begin{split} S_{lasso}(\\beta)=&amp; \\sum_{i=1}^{n}(y_{i}-x_{i}^{}\\beta)^{2}+ \\lambda\\sum_{j=1}^{p-1}|\\beta_{j}| \\end{split} \\nonumber \\end{equation}\\] La penalización sobre \\(\\sum_{j=1}^{p-1}|\\beta_{j}|\\) tiene como efecto forzar a que los coeficientes tiendan a cero. A medida que se incrementa, mayor es la penalización y más coeficientes tomarán el valor de cero (excluimos covariables que no son relevantes). Al igual que en la regresión ridge, se recomienda escalar las covariables 4.4.3.1 Métodos de regularización De forma de equivalente, \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\) minimiza: \\[ \\sum_{i=1}^{n}(y_{i}-x_{i}^{}\\beta)^{2} \\quad \\mbox{ sujeto a } \\quad \\sum_{j=1}^{p-1}|\\beta_{j}\\leq t. \\] Mientras que, en la regresión de ridge,\\(\\widehat{\\boldsymbol \\beta}_{ridge}\\) minimiza: \\[ \\sum_{i=1}^{n}(y_{i}-x_{i}^{}\\beta)^{2} \\quad \\mbox{ sujeto a } \\quad \\sum_{j=1}^{p-1}\\beta_{j}^2\\leq t. \\] Cuando \\(\\lambda \\rightarrow 0\\) , entonces \\(\\widehat{\\boldsymbol \\beta}_{LASSO} \\rightarrow \\widehat{\\boldsymbol \\beta}\\), y cuando \\(\\lambda \\rightarrow \\infty\\), entonces \\(\\widehat{\\boldsymbol \\beta}_{LASSO} \\rightarrow 0\\). También, cuando \\(\\lambda \\rightarrow \\infty\\), la varianza de \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\) disminuye, pero el sesgo aumenta. No hay una estimación analítica para \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\), pero hay algoritmos eficientes para su estimación. #se debe especificar alpha=1 X = model.matrix(mod.fat)[,-1] lasso.mod &lt;- glmnet(X, fat$brozek, alpha = 1,nlambda = 100) plot(lasso.mod,xvar=&#39;lambda&#39;,label=T,lwd=2) abline(h=0,lty=2) Figure 4.3: Estimación de los coeficientes vs log(lambda) 4.4.3.2 Validación cruzada La validación cruzada se utiliza para evaluar y comparar modelos. Consiste en dividir la muestra en dos grupos: Entrenamiento: se usa para ajusta el modelo. Validación: se utiliza para validar el modelo. Para no perder información, en la validación cruzada se divide la muestra en dos (o más) partes y se hace la validación en cada una. División de la muestra de forma aleatoria en \\(k = 5\\) grupos \\((k-fold)\\) Para cada división,\\(k = 1, . . . , K\\) , y para cada valor de \\(\\lambda\\), se estima el modelo basado en la muestra de entrenamiento. Mientras que con cada muestra de validación, y para cada valor de \\(\\lambda\\), se utiliza para calcular el error cuadrático medio: \\[ EMC_{k}(\\lambda) = \\sum_{i=1}^{n_k} \\frac{[y_{i}^{(k)}-x_{i}^{(k)}\\widehat{\\boldsymbol \\beta}_{lasso}^{(k)}(\\lambda)]^2}{n} \\] donde \\(y_{i}^{(k)}\\) son las observaciones de la muestra de validación \\(k\\), y \\(\\widehat{\\boldsymbol \\beta}_{lasso}^{(k)}(\\lambda)\\) es la estimación utilizando la muestra de entrenamiento \\(k\\). Para cada \\(\\lambda\\), se calcula la validación cruzada como: \\[ CV(\\lambda) = \\frac{1}{K}\\sum_{i=1}^{K}EMC_{k}(\\lambda) \\] y la desviación estándar: \\[ SD(\\lambda) = \\sqrt{\\sum_{i=1}^{K} \\frac{[EMC_{k}(\\lambda)-CV(\\lambda)]^2}{K-1}} \\] Luego, selección el \\(\\lambda\\) que minimiza \\(CV(\\lambda)\\): \\[ \\hat{\\lambda}_{cv}=arg\\quad mín_{\\lambda}-CV(\\lambda) \\] También, se puede aplicar la regla de una desviación estánda: \\[ \\hat{\\lambda}_{cv1sd}=máx \\{\\lambda:CV(\\hat{\\lambda})&lt;CV(\\hat{\\lambda}_{cv})+SD(\\hat{\\lambda}_{cv})\\} \\] 4.4.3.3 Ejemplo grasa corporal Validación cruzada con \\(k=10\\). lasso.cv &lt;-cv.glmnet(X, fat$brozek, nfolds = 252, alpha = 1,nlambda = 100) plot(lasso.cv) Figure 4.4: Validación cruzada grasa corporal Las covariables seleccionadas al estimar el lambda óptimo (regla una desviación estándar): est = glmnet(X, fat$brozek, alpha = 1,lambda = lasso.cv$lambda.1se) est$beta ## 13 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## age 0.04227147 ## weight . ## height -0.14779997 ## neck . ## chest . ## abdom 0.60302949 ## hip . ## thigh . ## knee . ## ankle . ## biceps . ## forearm . ## wrist -1.02250135 Observamos que las variables seleccionadas son age, height,abdom y wrist: mod.lasso = lm(brozek ~ age+height+abdom+wrist,data=fat) summary(mod.lasso) ## ## Call: ## lm(formula = brozek ~ age + height + abdom + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.2571 -2.8824 -0.2919 3.0419 9.3464 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.24853 6.27849 0.040 0.9685 ## age 0.06707 0.02196 3.055 0.0025 ** ## height -0.16461 0.07821 -2.105 0.0363 * ## abdom 0.67569 0.03120 21.654 &lt; 2e-16 *** ## wrist -1.93709 0.38343 -5.052 8.51e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.118 on 247 degrees of freedom ## Multiple R-squared: 0.7223, Adjusted R-squared: 0.7178 ## F-statistic: 160.6 on 4 and 247 DF, p-value: &lt; 2.2e-16 Las covariables selecionas en este modelo son todas significativas, el \\(R^2=0.7223\\) y el \\(R^2_{adj}=0.7178\\) han disminuido ligeramente respecto al modelo con todas las covariables. Método de selección AIC y PRESS BIC LASSO Est er. std. valor-p Est er. std. valor-p Est er. std. valor-p int. -20.062 10.8465 0.0656 -31.297 6.7089 0 0.249 6.2785 0.9685 age 0.059 0.0285 0.0388 0.067 0.022 0.0025 weight -0.084 0.037 0.0237 0.126 0.0229 0 height -0.165 0.0782 0.0363 neck -0.432 0.208 0.0389 abdom 0.877 0.0666 0 0.921 0.0519 0 0.676 0.0312 0 hip -0.186 0.1282 0.1473 thigh 0.286 0.1195 0.1473 forearm 0.4825 0.1725 0.0056 0.446 0.1682 0.0085 wrist -1.4049 0.4717 0.0032 -1.392 0.4099 8e-04 -1.937 0.3834 0 Modelo \\(R^2\\) PRESS AIC BIC AIC y PRESS 0.7467 4139.682 1420.225 1455.520 BIC 0.7351 4209.140 1423.471 1444.647 LASSO 0.7223 4447.052 1435.389 1456.566 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
