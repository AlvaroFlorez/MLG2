[["index.html", "Notas de clase: Modelo lineal general II Introducción", " Notas de clase: Modelo lineal general II Alvaro J. Flórez 2022-04-19 Introducción Estas son las notas de clase del curso Modelo Lineal General II. Los temas que se tratan son: Variables indicadoras. Modelos polinomiales. Multicolinealidad. Selección de variables. Introducción a modelos no lineales. Introducción al modelo lineal generalizado (modelo logístico - modelo Poisson). Introducción al modelo lineal mixto Tenga en cuenta que el propósito de estas notas de clase no es reemplazar los textos guías. Para el estudio más detallado de los temas revisados, se recomiendan las siguientes lecturas: Introduction to Linear Regression Analysis, Fifth Ed., 2012, by Montgomery, D. C., Peck, E. A. and Vining, G. G. (Texto guía) Applied Regression Analysis, Third Ed., 1998, by Draper, N. R. and Smith, H., Wiley. Theory and Applications of the Linear Models, 2000, by Graybill, F. A., Duxbury. Applied Linear Statistical Models, Fifth Ed., 2005, by Kutner, M. H, Nachtsheim, C. J., Neter, J. and Li, W., McGraw-Hill. Análisis de Regresión. Introducción Teórica y Práctica basada en R, 2011, by F. Tusell. Applied Linear Regression, Fourth Ed., 2014, by S. Weisberg. Applied Regression Analysis &amp; Generalized Linear Models, 2016, by J. Fox. "],["variables-indicadoras-o-dummies.html", "Capítulo 1 Variables indicadoras (o dummies) 1.1 Ejemplos 1.2 Variables indicadoras", " Capítulo 1 Variables indicadoras (o dummies) 1.1 Ejemplos 1.1.1 Datos de atletas australianos Los datos ais de la libreria alr4 tiene información sobre 202 atletas de élite de Australia. 102 hombres y 100 mujeres. Se quiere evaluar la relación entre la concentración de hemoglobina (Hg, g/dl) y el índice de masa corporal (BMI, kg/m\\(^2\\)). data(ais) par(mfrow=c(1,2)) plot(density(ais$Hg[ais$Sex==0]),xlim=c(11,20),lwd=2,main = &#39;&#39;,ylab=&#39;Densidad&#39;,xlab=&#39;Hg (g/dl)&#39;) lines(density(ais$Hg[ais$Sex==1]),col=2,lwd=2) plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) Figure 1.1: Datos de atletas. Densidad de la hemoglobina para hombres y mujeres (derecha) y diagrama de dispersión entre la homoglobina y el índice de masa corporal (izquierda). Negro para hombres y rojo para mujeres. En la Figura ?? (izquierda) vemos que los niveles de hemoglobina son mayores para hombres que para mujeres. En la Figura ?? (derecha) observamos que hay una relación positiva entre la hemoglobina y el índice de masa corporal tanto para hombres como para mujeres. Esto nos puede indicar que ingresar el sexo del atleta en el modelo puede mejorarnos el ajuste. 1.1.2 Datos de la ONU Retomemos los datos de la ONU (UN11 de la librería alr4). Las variables de interés son: fertility: Número esperado de nacidos vivos por mujer. ppgdp: producto nacional bruto per cápita (PNB, en dólares). Purban: el porcentaje de la población que vive en un área urbana. lifeExpF: esperanza de vida femenina (años). group: si el país pertenece a la OCDE (Organización para la Cooperación y el Desarrollo Económicos), África o otros. Por ahora consideremos la relación entre la fertilidad y el PNB per cápita, teniendo en cuenta el grupo al que pertenece cada país. data(UN11) plot(log(fertility)~log(ppgdp),data=UN11,col=UN11$group,xlab=&#39;log PNB per cápita (dólares)&#39;, ylab=&#39;log # esperado de nacidos vivos por mujer&#39;) Figure 1.2: Datos de la ONU. Relación entre la fertilidad y el PNB per cápita para los países de OCDE (puntos negros), países africanos (puntos verdes) y los otros países (rojos) En la Figura ?? podemos observar que, en general, cuando el PNB aumenta, la tasa de fertilidad disminuye. Sin embargo, esta relación puede variar según la categoría del país. Para los países de la OCDE, esta relación no es fuerte. Mientras que para los demás se mantiene esta relación negativa. Por esta razón sería de gran importancia incluir esta variable categórica dentro del modelo. 1.2 Variables indicadoras Las covariables categóricas entran en un modelo como variables indicadoras (o también llamadas dummies). En el caso que la covariable \\((X)\\) tenga dos categorías, entonces se crea una variable indicadora. Por ejemplo, para los datos de los atletas, el sexo requiere una indicadora: \\[ u_{i} = \\begin{cases} 1 &amp; \\mbox{ si la observación i es hombre}, \\\\ 0 &amp; \\mbox{ si la observación i es mujer}. \\\\ \\end{cases} \\] Aquí mujer es llamada la categoría de referencia. En caso que la covariable categórica tenga \\(k\\) categorías, se tienen que crear \\(k-1\\) variables indicadoras. Por ejemplo, para la variable grupo de país en los datos de la ONU se requieren 2 variables indicadoras \\((u_{j})\\) como lo muestra la Tabla 1.1. Table 1.1: Variables indicadoras para la variable group de los datos de la ONU Categoría \\(u_{1}\\) \\(u_{2}\\) OECD 0 0 otro 1 0 África 0 1 1.2.1 Modelos con covariables categóricas Suponga que se quiere ajustar un modelo para una variable respuesta \\(Y\\) en función de dos covariables: una continua \\(X\\) y una indicadora \\(Z\\) (es decir una variable categórica con dos 2 categorías). El modelo propuesto es el siguiente: \\[\\begin{equation} y_{i} = \\beta_{0} + x_{i}\\beta_{1} + z_{i}\\beta_{2} + x_{i}z_{i}\\beta_{3} + \\varepsilon_{i}, \\label{eq:modInter} \\end{equation}\\] donde \\(\\varepsilon_{i} \\sim N(0,\\sigma^{2})\\). Tenemos que, si \\(z_i=0\\): \\[ E(Y | X=x_i, Z=0) = \\beta_{0} + x_{i}\\beta_{1}. \\] Mientras que, si \\(z_i=1\\): \\[ E(Y | X=x_i, Z=1) = (\\beta_{0}+\\beta_{2}) + x_{i}(\\beta_{1}+\\beta_{3}). \\] Por lo que el modelo ((??)) genera dos rectas, una para cada categoría. \\(\\beta_2\\) indica la diferencia de intercepto entre las dos categorías y \\(\\beta_3\\) la diferencia entre pendientes. En la Figura ??(izquierda) se observan las dos rectas que se obtienen a partir de este modelo. Si se elimina la interacción entre variables, se obtienen dos rectas paralelas (Figura ??, derecha). Figure 1.3: Efecto de la interacción entre variable continua e indicadora. Modelo general (izquierda) y modelo de líneas paralelas (derecha). 1.2.2 Modelo para los datos de atletas australianos Para los datos de los atletas, se sugiere el siguiente modelo: \\[\\begin{equation} \\begin{split} \\mbox{Hg}_{i} =&amp; \\beta_{0} + \\mbox{Sex}_{i}\\beta_{1} + \\mbox{BMI}_{i}\\beta_{2} + \\mbox{Sex}_{i}\\mbox{BMI}_{i}\\beta_{3} + \\varepsilon_{i}, \\end{split} \\nonumber \\end{equation}\\] donde: \\[ \\mbox{Sex}_{i} = \\begin{cases} 1 &amp; \\mbox{ si la observación i corresponde a una mujer}, \\\\ 0 &amp; \\mbox{ de otra forma}. \\\\ \\end{cases} \\] Note que en la base de datos ais, la variable sex ya está codificada de esta forma. Si la variable no está codificada de forma numérica, R eligirá la categoría de referencia de forma automática. Para ajustar el modelo utilizamos la función lm (Sex*BMI aquí estamos incluyendo los efectos de BMI y Sex, así como la interacción): mod.ais = lm(Hg~Sex*BMI, data=ais) summary(mod.ais) ## ## Call: ## lm(formula = Hg ~ Sex * BMI, data = ais) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9997 -0.6625 -0.0478 0.5784 3.5583 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.21316 0.78662 16.797 &lt;2e-16 *** ## Sex -0.66140 1.09835 -0.602 0.5477 ## BMI 0.09788 0.03269 2.994 0.0031 ** ## Sex:BMI -0.05203 0.04761 -1.093 0.2758 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9092 on 198 degrees of freedom ## Multiple R-squared: 0.5613, Adjusted R-squared: 0.5546 ## F-statistic: 84.44 on 3 and 198 DF, p-value: &lt; 2.2e-16 Aquí tenemos que \\(\\widehat{\\beta}_{2} =0.1\\), lo que nos indica que el valor esperado del nivel de hemoglobina aumenta en \\(0.1\\) g/dl por cada aumento unitario en el índice de masa corporal de los hombres. Para las mujeres, el efecto del índice de masa corporal sobre el nivel de hemoglobina también es positivo, pero con una pendiente menor \\(\\widehat{\\beta}_{2} +\\widehat{\\beta}_{3} = -0.66 - 0.1 = -0.56\\). La representación gráfica del modelo se puede observar en la Figura @ref[fig:AtletesAjuste]. Aquí vemos que la pendiente para las mujeres es un poco menor que para los hombres. plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) abline(a=mod.ais$coefficients[1],b=mod.ais$coefficients[3],lwd=2) abline(a=mod.ais$coefficients[1]+mod.ais$coefficients[2],b=mod.ais$coefficients[3]+mod.ais$coefficients[4],col=2,lwd=2) Figure 1.4: Datos de atletas. Ajuste del modelo para la homoglobina en función del índice de masa corporal y sexo. Línea negra para hombres y línea roja para mujeres. Si miramos la significancia del \\(\\widehat{\\beta}_3\\) (valor-\\(p\\) igual a 0.276), podemos concluir que las diferencias en pendiente no son significativas. Por lo que, el efecto del índice de masa corporal sobre los niveles de hemoglobina es el mismo para mujeres que para hombres. Si queremos evaluar si hay diferencias entre hombres y mujeres, debemos evaluar la siguiente hipótesis: \\[ H_{0}: \\beta_{1} = \\beta_3 = 0. \\] En R, esto lo podemos realizar usando la función anova() (prueba F) comparando el modelo completo contra el modelo reducido (sin la variable Sex): mod.ais.red = lm(Hg~BMI, data=ais) anova(mod.ais.red,mod.ais) ## Analysis of Variance Table ## ## Model 1: Hg ~ BMI ## Model 2: Hg ~ Sex * BMI ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 200 318.52 ## 2 198 163.69 2 154.82 93.637 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Aquí vemos que se rechaza \\(H_0\\) por lo que hay diferencias en la relación de la homoglobina y el índice de masa corporal entre hombres y mujeres (\\(\\beta_1\\) o \\(\\beta_3\\) es diferente de cero, pero no ambos). El modelo con lineas paralelas es: \\[ \\mbox{Hg}_{i} = \\beta_{0} + \\mbox{Sex}_{i}\\beta_{1} + \\mbox{BMI}_{i}\\beta_{2} + \\varepsilon_{i}, \\] donde \\(\\varepsilon_{i} \\sim N(0, \\sigma^{2})\\), y se calcula de la siguiente forma: mod.ais.lp = lm(Hg~Sex+BMI, data=ais) summary(mod.ais.lp) ## ## Call: ## lm(formula = Hg ~ Sex + BMI, data = ais) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0131 -0.6530 -0.0263 0.6249 3.5806 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.79954 0.57549 23.979 &lt; 2e-16 *** ## Sex -1.85251 0.13587 -13.634 &lt; 2e-16 *** ## BMI 0.07335 0.02378 3.085 0.00233 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9097 on 199 degrees of freedom ## Multiple R-squared: 0.5586, Adjusted R-squared: 0.5542 ## F-statistic: 125.9 on 2 and 199 DF, p-value: &lt; 2.2e-16 La estimación del efecto asociado al sexo indica la diferencia media del nivel de hemoglobina entre hombres y mujeres. En la Figura ?? vemos que el modelo de líneas paralelas tiene casi el mismo ajuste que el modelo general. plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) abline(a=mod.ais$coefficients[1],b=mod.ais$coefficients[3],lwd=2) abline(a=mod.ais$coefficients[1]+mod.ais$coefficients[2],b=mod.ais$coefficients[3]+mod.ais$coefficients[4],col=2,lwd=2) abline(a=mod.ais.lp$coefficients[1],b=mod.ais.lp$coefficients[3],lwd=2,lty=2) abline(a=mod.ais.lp$coefficients[1]+mod.ais.lp$coefficients[2],b=mod.ais.lp$coefficients[3],col=2,lwd=2,lty=2) Figure 1.5: Datos de atletas. Ajuste del modelo general (línea solida) y el modelo líneas paralelas (línea discontinua) para la homoglobina en función del índice de masa corporal y sexo. Líneas negra para hombres y líneas roja para mujeres. 1.2.3 Modelo para los datos de la ONU El modelo propuesto es el siguiente: \\[\\begin{equation} \\begin{split} \\log\\mbox{fertility}_{i} =&amp; \\beta_{0} + \\mbox{u}_{1i}\\beta_{1}+\\mbox{u}_{2i}\\beta_{2} + \\log\\mbox{ppgdp}_{i}\\beta_{3} + \\\\ &amp; \\mbox{u}_{1i}\\log\\mbox{ppgdp}_{i}\\beta_{4} + \\mbox{u}_{2i}\\log\\mbox{ppgdp}_{i}\\beta_{5} + \\varepsilon_{i}, \\end{split} \\nonumber \\end{equation}\\] donde: \\[ \\mbox{u}_{1i} = \\begin{cases} 1 &amp; \\mbox{ si el país i pertenece a la categoría otro}, \\\\ 0 &amp; \\mbox{ de otra forma}, \\\\ \\end{cases} \\quad \\mbox{ y } \\quad \\mbox{u}_{2i} = \\begin{cases} 1 &amp; \\mbox{ si el país i pertenece a África}, \\\\ 0 &amp; \\mbox{ de otra forma}. \\\\ \\end{cases} \\] Por lo tanto, OECD es la categoría de referencia. El modelo ajustado es: mod.UN11 = lm(log(fertility)~group*log(ppgdp), data=UN11) summary(mod.UN11) ## ## Call: ## lm(formula = log(fertility) ~ group * log(ppgdp), data = UN11) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69358 -0.16963 0.02005 0.16838 0.73633 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.21836 0.81290 0.269 0.78851 ## groupother 1.87888 0.83290 2.256 0.02520 * ## groupafrica 2.54072 0.84299 3.014 0.00293 ** ## log(ppgdp) 0.03217 0.07832 0.411 0.68170 ## groupother:log(ppgdp) -0.18418 0.08106 -2.272 0.02417 * ## groupafrica:log(ppgdp) -0.22637 0.08430 -2.685 0.00788 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2739 on 193 degrees of freedom ## Multiple R-squared: 0.6305, Adjusted R-squared: 0.6209 ## F-statistic: 65.86 on 5 and 193 DF, p-value: &lt; 2.2e-16 A partir de estos resultados obtenemos tres rectas que describen el valor esperado del logarítmo de la fertilidad en función del logarítmo del PNB per cápita, una para cada tipo de país. Para los países de la OCDE, tenemos que: \\[ E(\\log\\mbox{fertility}) = 0.218 + 0.032\\log \\mbox{ppgdp}. \\] Para los países Africanos: \\[ E(\\log\\mbox{fertility}) = 2.759 -0.194\\log \\mbox{ppgdp}. \\] Finalmente, para los otros países: \\[ E(\\log\\mbox{fertility}) = 2.097 -0.152\\log \\mbox{ppgdp}. \\] Aquí vemos que para los países que no son de la OCDE, el producto nacional bruto tiene un efecto significativo negativo sobre la tasa de fertilidad. Mientras que para los países de la OCDE, este efecto es positivo, aunque no es significativo. Esto mismo lo podemos ver gráficamente en la Figura ??. Beta.UN11 = mod.UN11$coefficients plot(log(fertility)~log(ppgdp),data=UN11,col=UN11$group,xlab=&#39;log PNB per cápita (dólares)&#39;, ylab=&#39;log # esperado de nacidos vivos por mujer&#39;) abline(a=Beta.UN11[1],b=Beta.UN11[4],lwd=2) abline(a=Beta.UN11[1]+Beta.UN11[2],b=Beta.UN11[4]+Beta.UN11[5],col=2,lwd=2) abline(a=Beta.UN11[1]+Beta.UN11[3],b=Beta.UN11[4]+Beta.UN11[6],col=3,lwd=2) Figure 1.6: Datos de la ONU. Ajuste del modelo para la fertilidad en función del PNB y tipo de país. Países de OCDE (línea negra), países africanos (línea verde) y los otros países (línea roja) Se podría hacer la siguiente pregunta, ¿el efecto del PNB sobre la fertilidad es el mismo para cada tipo de país?. Para resolver esta pregunta, se plantea la siguiente hipótesis: \\[ H_0: \\beta_4 = \\beta_5 = 0. \\] Usando la función anova() (prueba F) en R: mod.UN11.red = lm(log(fertility)~group+log(ppgdp), data=UN11) anova(mod.UN11.red,mod.UN11) ## Analysis of Variance Table ## ## Model 1: log(fertility) ~ group + log(ppgdp) ## Model 2: log(fertility) ~ group * log(ppgdp) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 195 15.033 ## 2 193 14.484 2 0.54848 3.6542 0.02769 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Este resultado indica que hay evidencia suficiente para concluir que el efecto del PNB sobre la fertilidad no es el mismo para cada tipo de país. Ahora, podríamos preguntarnos: ¿las pendientes son las mismas para las categorías de otro y África?. Para esto, se plantea la siguiente hipótesis: \\[ H_0: \\beta_4 = \\beta_5. \\] También se puede expresar de la siguiente forma: \\[ H_{0}: \\boldsymbol L\\boldsymbol \\beta= \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{pmatrix} = 0 \\] Esto, en R es: L = matrix(c(0,0,0,0,1,-1),1,6,byrow = T) linearHypothesis(mod.UN11, hypothesis.matrix=L) ## Linear hypothesis test ## ## Hypothesis: ## groupother:log(ppgdp) - groupafrica:log(ppgdp) = 0 ## ## Model 1: restricted model ## Model 2: log(fertility) ~ group * log(ppgdp) ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 194 14.579 ## 2 193 14.484 1 0.094808 1.2633 0.2624 Dado que no se rechaza \\(H_0\\), el efecto del PNB sobre la fertilidad es el mismo para los países Africano y los de la categoría de otros. En resumen, a partir del análisis de regresión: - El efecto del PNB per cápita es diferente cada tipo de país. - Para los países miembros de la OECD, el efecto es positivo (aunque no es significativo). - Para los países de áfrica y otros, el efecto es negativo (y no es significativamente diferente). "],["modelos-polinomiales.html", "Capítulo 2 Modelos polinomiales 2.1 Ejemplos 2.2 Modelos polinomiales 2.3 Regresión por segmentos", " Capítulo 2 Modelos polinomiales 2.1 Ejemplos 2.1.1 Pasteles Con los datos cakes de la libreria alr4 se tienen dos objetivos. Primero, evaluar el efecto de la temperatura y tiempo de horneado sobre la palatabilidad de mazclas de pasteles para hornear. Segundo, encontrar la combinación de estos factores que maximizan la palatabilidad. Como variable respuesta (Y) se tiene el promedio de calificación de la palatabilidad de cuatro pasteles horneados. Mientras que las covariables son: el tiempo de horneado (X1, en minutos) y la temperatura (X2, en grados Fahrenheit) library(alr4) data(cakes) plot(cakes[,-1]) Figure 2.1: Datos de pasteles. Diagrama de dispersión. 2.1.2 Datos de Boston La base de datos Boston de la libreria MASS contiene información sobre 506 suburbios del area metropolitana de Boston. El objetivo del estudio es evaluar la relación del precio de las viviendas y la concentración de contaminación ambienta. En esta sección evaluaremos la relación entre la concentración anual de óxido de nitrógeno (\\(y\\), en partes por diez millones) y la distancia a cinco centros de empleo. library(MASS) data(Boston) plot(nox~dis,data=Boston,ylab=&#39;NOx&#39;,xlab=&#39;distancia a centros de empleo&#39;) Figure 2.2: Datos de Boston. Relación entre el óxido de nitrógeno y la distancia a centros de empleo. 2.2 Modelos polinomiales El modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_i, \\] describe la relación lineal entre \\(y\\) y \\(x_1\\). Si las relación entre las variables presentan curvaturas, se puede considerar un modelo polinómico de la forma: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + \\ldots + + \\beta_{k}x_{i}^{k} + \\varepsilon_i. \\] Este modelo se sigue considerando como un modelo lineal, dado que es lineal en los parámetros \\(\\boldsymbol \\beta\\). En la Figura 2.3 se puede observar diferentes curvas para modelos lineales (orden 1), cuadráticos (orden 2) y cúbicos (orden 3). Aquí vemos que este tipo de modelos son muy versatiles. Caulquier función suave se puede ajustar meidante un polinomio de grado suficientemente alto. Por esta razón, los modelos polinomicos son usados en casos donde las relaciones entre las variables son no-lineales y se pueden aproximar por un polinomio. Figure 2.3: Polinomio de grado 1 (linea negra), grado 2 (linea roja) y grado 3 (linea verde). En el caso que se tengan dos covariables, un modelo de orden 2 se expresa de la forma: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}^2 + \\beta_{4}x_{2i}^2 + \\beta_{5}x_{1i}x_{2i} + \\varepsilon_i. \\] En las Figuras 2.4-2.6 muestran el valor esperado de \\(Y\\) en un modelo lineal (asumiendo \\(\\beta_3=\\beta_4=\\beta_5=0\\)), cuadrático y con interacción (asumiendo \\(\\beta_3=\\beta_4=0\\)). Figure 2.4: Valor esperado de \\(Y\\) en un modelo lineal (izquierda) y gráfico de contorno (derecha). El número de parámetros incrementa rápidamente con el número de covariables. Con \\(k\\) covariables, tenemos: un intercepto, \\(k\\) términos lineales, \\(k\\) términos cuadráticos, y \\(k(k - 1)/2\\) interacciones. Por esta razón, en muchos casos, no se toman en cuentas las interacción cuando el número de covariables es grande. Figure 2.5: Valor esperado de \\(Y\\) en un modelo cuadrático (izquierda) y gráfico de contorno (derecha). Figure 2.6: Valor esperado de \\(Y\\) en un modelo lineal con interacción (izquierda) y gráfico de contorno (derecha). Hay aspectos que se deben tener en la práctica cuando se implementa un modelo polinomial: Selección del orden: La idea es mantener el orden del polinomio bajo. Si embargo, si es muy bajo no logra capturar la curvatura presente en los datos. En caso que el orden sea grande, el modelo es innecesariamente más complejo y puede haber problemas de multicolinealidad. Si los datos exige un modelo de orden alto \\((k &gt; 3)\\), se pueden hacer transformación sobre las variables, y así, poder ajustar un modelo polinomial de orden bajo (por ejemplo cuadrático). La selección del orden puede hacerse de dos formas. (1) ** Hacia delante: empezar con un modelo de orden \\(1\\) e incrementar el orden uno a uno hasta que un término mayor ya no sea significativo. Hacia atrás:** empezar con el modelo más complejo y eliminar los términos mayores uno a uno hasta que todos sean significativos. Extrapolación: La extrapolación con modelos polinomiales puede ser muy peligrosa. Por ejemplo en la Figura 2.7 podemos ajustar un modelo de orden dos a los datos (linea negra). Si hacemos una predicción fuera del rango de los datos, el valor esperado predicho sigue el comportamiento cuadrático propuesto. Sin embargo, el valor esperado de \\(Y\\) puede seguir un comportamiento diferente (linea roja discontinua). Lo que lleva a tener una predicción sesgada. Figure 2.7: Problema de extrapolación. Multicolinealidad: Al aumentar el polinomio, la matriz \\(\\boldsymbol X&#39;\\boldsymbol X\\) se vuelve mal acondicionada. Es decir, las estimaciones pueden ser inestables y los errores estándar se inflan. Este problema se puede solucionar centrando las covariables. Por ejemplo en un modelo de orden 2: \\[ E(Y | X=x) = \\beta_{0} + \\beta_{1}(x-\\bar{x}) + \\beta_{2}(x-\\bar{x})^{2}. \\] Otra solución es usando polinomios ortogonales. 2.2.1 Interpretación de los coeficientes Considere un modelo de orden 2. El valor esperado de \\(Y\\) está dado por: \\[ E(Y| X_{1}=x_1,X_{2}=x_2) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}^{2} + \\beta_{4}x_{2}^{2} + \\beta_{5}x_{1}x_{2}. \\] Si \\(x_{1}\\) cambia en \\(\\delta\\) unidades \\((x_{1} + \\delta)\\), tenemos que: \\[ E(Y| X_{1}=x_1+\\delta,X_{2}=x_2) = \\beta_{0} + \\beta_{1}(x_{1}+\\delta) + \\beta_{2}x_{2} + \\beta_{3}(x_{1} + \\delta)^{2} + \\beta_{4}x_{2}^{2} + \\beta_{5}(x_{1}+\\delta)x_{2}. \\] Ahora calculando la diferencia: \\[ E(Y| X_{1}=x_1+\\delta,X_{2}=x_2) - E(Y| X_{1}=x_1,X_{2}=x_2) = (\\beta_{1}\\delta + \\beta_{3}\\delta^{2}) + 2\\beta_{3}\\delta x_{1} + \\beta_{5}\\delta x_{2}. \\] Aquí podemos observar que el efecto del cambio \\(\\delta\\) en \\(X_1\\) depende de ambas covariables y del valor de \\(\\delta\\). Por esta razón es complicado interpretar coeficientes en modelos polinomiales. 2.2.2 Pasteles Para los datos de los pasteles, se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}^2 + \\beta_{4}x_{2i}^{2} + \\beta_{5}x_{1i}x_{2i} + \\varepsilon_{i}. \\] El ajuste del modelo es: mod.cakes = lm(Y ~ X1*X2 + I(X1^2)+I(X2^2),data=cakes) summary(mod.cakes) ## ## Call: ## lm(formula = Y ~ X1 * X2 + I(X1^2) + I(X2^2), data = cakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4912 -0.3080 0.0200 0.2658 0.5454 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.204e+03 2.416e+02 -9.125 1.67e-05 *** ## X1 2.592e+01 4.659e+00 5.563 0.000533 *** ## X2 9.918e+00 1.167e+00 8.502 2.81e-05 *** ## I(X1^2) -1.569e-01 3.945e-02 -3.977 0.004079 ** ## I(X2^2) -1.195e-02 1.578e-03 -7.574 6.46e-05 *** ## X1:X2 -4.163e-02 1.072e-02 -3.883 0.004654 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4288 on 8 degrees of freedom ## Multiple R-squared: 0.9487, Adjusted R-squared: 0.9167 ## F-statistic: 29.6 on 5 and 8 DF, p-value: 5.864e-05 Además, los factores de inflación de varianza son los siguientes: car::vif(mod.cakes) ## X1 X2 I(X1^2) I(X2^2) X1:X2 ## 3778.083 5921.833 1328.089 5309.339 3063.500 Aquí vemos que los VIF presentan valores muy altos, producto de ajustar un modelo cuadrático. Ahora consideremos el modelo con las covariables centradas: cakes$X1c = cakes$X1 - mean(cakes$X1) cakes$X2c = cakes$X2 - mean(cakes$X2) modc.cakes = lm(Y ~ X1c*X2c + I(X1c^2)+I(X2c^2),data=cakes) summary(modc.cakes) ## ## Call: ## lm(formula = Y ~ X1c * X2c + I(X1c^2) + I(X2c^2), data = cakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4912 -0.3080 0.0200 0.2658 0.5454 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.070000 0.175044 46.103 5.41e-11 *** ## X1c 0.367558 0.075796 4.849 0.001273 ** ## X2c 0.096392 0.015159 6.359 0.000219 *** ## I(X1c^2) -0.156875 0.039446 -3.977 0.004079 ** ## I(X2c^2) -0.011950 0.001578 -7.574 6.46e-05 *** ## X1c:X2c -0.041625 0.010719 -3.883 0.004654 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4288 on 8 degrees of freedom ## Multiple R-squared: 0.9487, Adjusted R-squared: 0.9167 ## F-statistic: 29.6 on 5 and 8 DF, p-value: 5.864e-05 Los VIFs del modelo con las covariables centradas son: car::vif(modc.cakes) ## X1c X2c I(X1c^2) I(X2c^2) X1c:X2c ## 1.000000 1.000000 1.005952 1.005952 1.000000 En este caso, los VIF decrecieron considerablemente con respecto al modelo con las covariables originales. En la figura ?? podemos observar el valor esperado estimado de la palatabilidad para diferentes valores de tiempo y temperatura de horneado. Figure 2.8: Datos de pasteles. Valor esperado de la palatabilidad para diferentes valores de tiempo y temperatura de horneado. En la Figura ?? (izquierda) vemos que, cuando la temperatura es de 350 grados Fahrenheit, el máximo de palatabilidad que se obtiene en un tiempo entre 36 y 37 minutos. Sin embargo, cuando la temperatura se incrementa a 360, la máxima palatibilidad que se puede lograr es menor. Además, se obtiene en un tiempo también menor. El efecto de la interacción también se puede observar en la Figura 2.9 (derecha), pero ahora fijando el tiempo de horneado y variando la temperatura. El gráfico de contornos (Figura 2.9) muestra que la máxima palatabilidad se observa cuando la temperatura está alrededor de 355 y el tiempo de horneado está entre 35 y 36 minutos. X1 = seq(32, 38, length.out = 50) X2 = seq(335, 365, length= 50) y &lt;- outer(X= X1, Y = X2, FUN = function(x, y) { predict(modc.cakes, newdata = data.frame(X1c = x-mean(cakes$X1), X2c = y-mean(cakes$X2))) }) contour(X1, X2, y,xlab=&#39;tiempo de horneado (minutos)&#39;, ylab=&#39;temperatura de horneado (Fahrenheit)&#39;) Figure 2.9: Datos de pasteles. Gráfica de contornos. Para determinar en que combinación de tiempo (X1) y temperatura de horneado (X2) se obtiene la máxima palatabilidad. Por lo tanto, debemos resolver la siguiente ecuación: \\[ \\frac{\\partial E(Y)}{\\partial \\boldsymbol x} = \\frac{\\partial}{\\partial \\boldsymbol x} \\left( \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}^2 + \\beta_{4}x_{2}^{2} + \\beta_{5}x_{1}x_{2} \\right) = \\boldsymbol 0, \\] verificando que se obtiene un máximo. Resolviendo la ecuación anterior, el máximo valor esperado de palatabilidad \\((8.3)\\) se obtiene en un tiempo de horneado de 35.8 minutos y a una temperatura de F. El intervalo del 95% de confianza en este punto es: \\((7.9, 8.7)\\). 2.2.3 Datos de Boston Para los datos de contaminación se puede proponer el siguiente modelo: \\[ \\mbox{NOx}_{i}^{-1.5} = \\beta_{0} + \\beta_{1}\\mbox{dis}_{i} + \\beta_{2}\\mbox{dis}_{i}^{2} + \\beta_{3}\\mbox{dis}_{i}^{3} + \\varepsilon_{i}. \\] La potencia en la variable respuesta se seleccionó usando el método de Box-Cox. El ajuste del modelo es el siguiente: Boston$disc = Boston$dis - mean(Boston$dis) mod3.Boston = lm(I(nox)^{-1}~disc+I(disc^2)+I(disc^3),data=Boston) summary(mod3.Boston) ## ## Call: ## lm(formula = I(nox)^{ ## -1 ## } ~ disc + I(disc^2) + I(disc^3), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45677 -0.10233 0.00922 0.13091 0.32643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9785763 0.0121059 163.440 &lt; 2e-16 *** ## disc 0.1788433 0.0051507 34.722 &lt; 2e-16 *** ## I(disc^2) -0.0257929 0.0029431 -8.764 &lt; 2e-16 *** ## I(disc^3) 0.0013534 0.0004814 2.811 0.00513 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.173 on 502 degrees of freedom ## Multiple R-squared: 0.7765, Adjusted R-squared: 0.7752 ## F-statistic: 581.5 on 3 and 502 DF, p-value: &lt; 2.2e-16 En la Figura @ref{figPredBoston} muestra el ajuste del modelo cúbico. Para hacer comparaciones, se muestra también el ajuste lineal y cuadrático. Se observa que el modelo cúbico presenta un buen ajuste. El modelo explica alrededor del \\(78.3\\)% de la variabilidad de la concentración anual de óxido de nitrógeno. Con el modelo cuadrático, el coeficiente de determinación es de \\(0.781\\). Sin embargo, se puede observar que este ajuste es un poco deficiente cuando las distancias son muy grandes (mayores a 11). Figure 2.10: Datos de Boston. Valor esperado de la concentración anual de óxido de nitrógeno (en partes por diez millones) en función de las distancias a cinco centros de empleo (media ponderada). Modelo lineal (negro), cuadrático (rojo) y cúbico (verde). 2.3 Regresión por segmentos El modelo por segmentos se puede expresar como: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(x_{i}-t)^{0}_{+} + \\beta_{3}(x_{i}-t)^{1}_{+} + \\varepsilon_{i}. \\] donde: \\[ (x_{i}-t)_{+}^{r} = \\begin{cases} 0 &amp; \\mbox{ si } x_{i} + t \\leq 0, \\\\ (x_{i}-t)^{r} &amp; \\mbox{ si } x_{i} + t &gt; 0. \\\\ \\end{cases} \\nonumber \\] Por lo tanto, si \\(x_{i} \\leq t\\), \\(E(y_{i}| x_{i}) = \\beta_{0} + \\beta_{1}x_{i}\\). Mientras que, si \\(x_{i} &gt; t\\), tenemos que \\(E(y_{i}| x_{i}) = (\\beta_{0} + \\beta_{2} - \\beta_{1}t) + (\\beta_{1} + \\beta_{3})x_{i}\\). Este modelo esta representado graficamente en la Figura ??. Cuando \\(\\beta_2 \\neq 0\\), el modelo presenta una discontinuidad en \\(t\\). Mientras que, cuando \\(\\beta_2 = 0\\), el modelo presenta un cambio de pendiente en el punto \\(t\\). Además, \\(\\beta_3\\) indica el cambio de pendiente. Figure 2.11: Modelo de regresión por segmentos. Modelo con discontinuidad en \\(t\\) (izquierda). Modelo con cambio de pendiente en \\(t\\) (derecha). Aquí asumimos que \\(t\\) es conocido. Si este valor se asume como desconocido, debe estimarse a partir de los datos como un parámetro adicional. Sin embargo, se tendría que recurrir a un método estimación para modelos no-lineales. 2.3.1 Ejemplo library(MPV) data(p7.11) plot(p7.11,ylab=&#39;costo de producción por unidad (USD)&#39;,xlab=&#39;Unidades por lote&#39;) Figure 2.12: Datos de costos por lote. Diagram de dispersión de el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). Considere la base de datos p7.11 de la librería MPV. Aquí se quiere modelar la relación entre el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). Este relación se puede observar en la Figura 2.12. Se puede observar que la relación entre las variables es lineal. Sin embargo, se aprecia un posible cambio de pendiente en el punto \\(x=200\\). Por esta razón se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(x_{i}-200)_{+}^{1} + \\varepsilon_{i}. \\] Note que no se asume ninguna discontinuidad. El ajuste del modelo es: p7.11$x2 = p7.11$x - 200 p7.11$x2[p7.11$x &lt; 200] = 0 mod.lotes = lm(y~.,data=p7.11) summary(mod.lotes) ## ## Call: ## lm(formula = y ~ ., data = p7.11) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.37596 -0.16641 -0.09677 0.20363 0.51734 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.116481 0.535383 28.235 2.67e-09 *** ## x -0.050199 0.003332 -15.065 3.73e-07 *** ## x2 0.038852 0.005946 6.534 0.000181 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3157 on 8 degrees of freedom ## Multiple R-squared: 0.9829, Adjusted R-squared: 0.9787 ## F-statistic: 230.4 on 2 and 8 DF, p-value: 8.474e-08 A partir del ajuste podemos concluir que, por cada unidad que incrementa el lote, el costo de producción disminuye en \\(0.05\\)USD. Si embargo, si el tamaño del lote es mayor a \\(200\\), el costo de producción disminuye solamente \\(0.01\\)USD cuando el lote aumenta en un artículo. Gráficamente, el ajuste se puede observar en la Figura 2.13. b.lotes = mod.lotes$coefficients plot(p7.11$x,p7.11$y,ylab=&#39;costo de producción por unidad (USD)&#39;,xlab=&#39;unidades por lote&#39;) x = c(100,200) lines(x,b.lotes[1]+x*b.lotes[2],lwd=2) x = c(200,300) lines(x,b.lotes[1]-200*b.lotes[3]+x*(b.lotes[2]+b.lotes[3]),lwd=2) abline(v=200,lty=2) Figure 2.13: Datos de costos por lote. Diagram de dispersión de el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). "],["multicolinealidad.html", "Capítulo 3 Multicolinealidad 3.1 Ejemplos 3.2 Multicolinealidad 3.3 Detección de multicolinealidad 3.4 Datos de cemento 3.5 Datos de grasa corporal 3.6 Solución al problema de multicolinealidad 3.7 Estimador de ridge", " Capítulo 3 Multicolinealidad 3.1 Ejemplos 3.1.1 Cemento Los datos data(cement) en la librería MASS corresponden a un experimento sobre calor emanado en el fraguado de diferentes combinaciones químicas de cemento. Se tiene una muestra de \\(13\\) fraguados de cemento Portlan. En cada muestra, se midió con precisión los porcentajes de los cuatro ingredientes químicos principales (covariables). Mientras el cemento fraguaba, también se midió la cantidad de calor desprendido (cals/gm, variable respuesta). Los cuatro ingredientes químicos son: x1 aluminato tricálcico (%). x2 silicato tricálcico (%). x3 tetra-aluminio ferrita de calcio (%). x4 silicato dicálcico (%). En la Figura ?? podemos observar que hay relaciones lineales positivas entre la variable respuesta, y las covariables aluminato tricálcico y silicato tricálcico. Mientras que la relación con la variable silicato dicálcico es negativa. También podemos notar que hay una relación negativa fuerte en las covariables aluminato tricálcico y tetra-aluminio ferrita de calcio, y entre silicato tricálcico y silicato dicálcico. library(MASS) data(cement) plot(cement[,c(5,1:4)]) Figure 3.1: Datos de cemento. Diagrama de dispersión. Para estos datos, se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + x_{1i}\\beta_{1} + x_{2i}\\beta_{2} + x_{3i}\\beta_{3} + x_{4i}\\beta_{4} + \\varepsilon_{i}. \\] Los resultados del ajuste son: mod.cement = lm(y ~ x1+x2+x3+x4,data=cement) summary(mod.cement) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3 + x4, data = cement) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1750 -1.6709 0.2508 1.3783 3.9254 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.4054 70.0710 0.891 0.3991 ## x1 1.5511 0.7448 2.083 0.0708 . ## x2 0.5102 0.7238 0.705 0.5009 ## x3 0.1019 0.7547 0.135 0.8959 ## x4 -0.1441 0.7091 -0.203 0.8441 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.446 on 8 degrees of freedom ## Multiple R-squared: 0.9824, Adjusted R-squared: 0.9736 ## F-statistic: 111.5 on 4 and 8 DF, p-value: 4.756e-07 Note que el ajuste es bueno, el \\(98.2\\)% de la variabilidad de la cantidad de calor desprendido durante la fraguado es explicada por el modelo. Sin embargo, los resultados de las pruebas de hipótesis individuales sobre los coeficientes muestran que no son significantes. 3.1.2 Grasa corporal Se tiene una muestra de \\(20\\) mujeres saludables con edades entre \\(25\\) y \\(34\\) años (data(bodyfat) en la librería isdals). La medición del porcentaje de grasa corporal es caro y engorroso, por lo tanto se quiere buscar un modelo que proporcione predicciones fiables. Como variables de este modelo se utiliza: Triceps: pliegue cutáneo del tríceps (cm). Thigh: circunferencia del muslo (cm). Midarm: circunferencia del brazo medio (cm). La Figura @(fig:bodyfatFig) muestra la relación entre variables. Podemos observar que hay una relación fuerte del % de masa corporal con las covariables Triceps y Thigh. Además hay también una relación lineal fuerte entre esas dos covariables. library(isdals) data(bodyfat) plot(bodyfat) Figure 3.2: Datos de grasa corporal. Diagrama de dispersión. El modelo propuesto es: \\[ \\mbox{Fat}_{i} = \\beta_{0} + \\mbox{Triceps}_{i}\\beta_{1} + \\mbox{Thigh}_{i}\\beta_{2} + \\mbox{Midarm}_{i}\\beta_{3} + \\varepsilon_{i}. \\] El ajuste del modelo es el siguiente: mod.bodyfat = lm(Fat ~ Triceps+Thigh+Midarm,data=bodyfat) summary(mod.bodyfat) ## ## Call: ## lm(formula = Fat ~ Triceps + Thigh + Midarm, data = bodyfat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7263 -1.6111 0.3923 1.4656 4.1277 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 117.085 99.782 1.173 0.258 ## Triceps 4.334 3.016 1.437 0.170 ## Thigh -2.857 2.582 -1.106 0.285 ## Midarm -2.186 1.595 -1.370 0.190 ## ## Residual standard error: 2.48 on 16 degrees of freedom ## Multiple R-squared: 0.8014, Adjusted R-squared: 0.7641 ## F-statistic: 21.52 on 3 and 16 DF, p-value: 7.343e-06 Asi como en el caso anterior, observamos que el modelo explica gran parte de la variabilidad del % de grasa corporal. Sin embargo, los valores \\(p\\) de las pruebas individuales sobre los coeficientes son altos. 3.2 Multicolinealidad El estimador de \\(\\boldsymbol \\beta\\) es \\(\\widehat{\\boldsymbol \\beta}= (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol y\\). Además, \\(V(\\widehat{\\boldsymbol \\beta}) = \\sigma^{2}(\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\). Por lo tanto requiere que \\(\\boldsymbol X\\) sea de rango completo. Es decir, que hayan columnas que sean linealmente dependientes. En algunos casos, las columnas de \\(\\boldsymbol X\\) son casi linealmente dependientes o colineales,lo que lleva a que \\(\\boldsymbol X&#39;\\boldsymbol X\\) se casi singular, lo que provoca problemas a la hora de hacer inferencias. Sea \\(\\boldsymbol x_j\\) la \\(j\\)-ésima columna de la matriz \\(\\boldsymbol X\\), por lo tanto \\(\\boldsymbol X= (\\boldsymbol 1,\\boldsymbol x_{1},\\ldots,\\boldsymbol x_{p-1})\\). Los vectores \\(\\boldsymbol x_{1},\\boldsymbol x_{2},\\ldots,\\boldsymbol x_{p-1}\\) son linealmente dependientes si hay conjunto de constantes \\(a_{1},a_{2},\\ldots, a_{p-1}\\) no todas igual a cero, tal que: \\[ \\sum_{j=1}^{p-1}a_{j}\\boldsymbol x_{j} = c, \\mbox{ donde }c\\mbox{ es una constante.} \\] Si esto se cumple para un subconjunto de \\(\\boldsymbol X\\), el rango de la matriz \\(\\boldsymbol X&#39;\\boldsymbol X\\) es menor que \\(p\\), y por lo tanto, \\((\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\) no existe. Si la relación es aproximada \\((\\sum_{j=1}^{p-1}a_{j}\\boldsymbol x_{j} \\approx c)\\), existe el problema de multicolinealidad. Vamos a ilustrar el efecto de la multicolinealidad con un ejemplo sencillo considerando un modelo lineal con dos covariables: \\[ \\boldsymbol y^{*} = \\boldsymbol Z\\boldsymbol \\beta+ \\boldsymbol \\varepsilon, \\] donde \\(\\boldsymbol Z\\) es la matriz de covariables escaladas con longitud unitaria, esto es: \\[ y_{i}^{*} = \\frac{y_{i}-\\bar{y}}{\\sqrt{SST}}, \\mbox{ y } z_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{\\sqrt{s_{jj}}}, \\] donde \\(s_{jj} = \\sum_{i=1}^{n}(x_{ij}-\\bar{x}_{j})^{2}\\). De esta forma la matriz \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) es la matriz de correlación de las covariables, \\[ \\boldsymbol Z&#39;\\boldsymbol Z= \\begin{pmatrix} 1 &amp; \\rho_{12} \\\\ \\rho_{12} &amp; 1 \\end{pmatrix}, \\] y \\(\\boldsymbol Z&#39;\\boldsymbol y\\) es el vector de correlaciones entre \\(y\\) y las dos covariables: \\[ \\boldsymbol Z&#39;\\boldsymbol y^{*} = \\begin{pmatrix} \\rho_{y1} \\\\ \\rho_{y2} \\end{pmatrix}. \\] Por lo que el estimador por MCO de \\(\\boldsymbol b\\) es: \\[ \\widehat{\\boldsymbol b}= \\begin{pmatrix} 1 &amp; \\rho_{12} \\\\ \\rho_{12} &amp; 1 \\end{pmatrix}^{-1}\\begin{pmatrix} \\rho_{y1} \\\\ \\rho_{y2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1-r_{12}^{2}} &amp; \\frac{-r_{12}}{1-r_{12}^{2}} \\\\ \\frac{-r_{12}}{1-r_{12}^{2}} &amp; \\frac{1}{1-r_{12}^{2}} \\end{pmatrix} \\begin{pmatrix} r_{1y} \\\\ r_{2y} \\end{pmatrix}. \\] Particularmente, tenemos: \\[ \\widehat{b}_{1} = \\frac{r_{1y}-r_{12}r_{2y}}{1-r_{12}^{2}} \\mbox{ y } \\widehat{b}_{2} = \\frac{r_{2y}-r_{12}r_{1y}}{1-r_{12}^{2}}. \\] Además, la matriz de varianzas-covarianzas de \\(\\widehat{\\boldsymbol b}\\) es: \\[\\begin{equation} V(\\widehat{\\boldsymbol b}) = \\sigma^{2} \\begin{pmatrix} \\frac{1}{1-r_{12}^{2}} &amp; \\frac{-r_{12}}{1-r_{12}^{2}} \\\\ \\frac{-r_{12}}{1-r_{12}^{2}} &amp; \\frac{1}{1-r_{12}^{2}} \\end{pmatrix}. \\label{eq:varbb} \\end{equation}\\] En @(eq:varbb) podemos observar que la varianza de los coeficientes \\(\\widehat{\\boldsymbol b}\\) tienden a infinito cuando \\(|r_{12}|\\rightarrow 1\\). Mientras que se hace mínima cuando las covariables están incorrelacionadas \\((r_{12}=0)\\). Es decir, una fuerte correlación entre \\(x_1\\) y \\(x_2\\) da como resultado grandes varianzas y covarianzas para \\(\\widehat{\\boldsymbol b}\\). Por esta razón es preferible tener covariables que sean ortogonales. Puesto que así se garantiza la menor varianza para \\(\\widehat{\\boldsymbol \\beta}\\). Sin embargo, las covariables son díficiles de controlar en estudios observacionales. Ahora consideremos un modelo con \\(p-1\\) covariables, \\[ y_{i}^{*} = b_{1}z_{i1} + b_{2}z_{i2} + \\ldots + b_{p-1}z_{i,p-1} + \\varepsilon_{i}. \\] Las ecuaciones normales son: \\[ \\begin{pmatrix} 1 &amp; r_{12} &amp; r_{13} &amp; \\ldots &amp; r_{1,p-1} \\\\ r_{12} &amp; 1 &amp; r_{23} &amp; \\ldots &amp; r_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{1,p-1} &amp; r_{2,p-1} &amp; r_{3,p-1} &amp; \\ldots &amp; 1 \\end{pmatrix} \\begin{pmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{k} \\end{pmatrix} = \\begin{pmatrix} r_{y1} \\\\ r_{y2} \\\\ \\vdots \\\\ r_{y,p-1} \\end{pmatrix}. \\] Por lo que el estimador de \\(\\boldsymbol b\\) es: \\[ \\widehat{\\boldsymbol b}= \\boldsymbol R^{-1}\\boldsymbol Z&#39;\\boldsymbol y^{*}, \\mbox{donde }\\boldsymbol R= \\boldsymbol Z&#39;\\boldsymbol Z. \\] Se puede demostrar que los elementos de la diagonal de la matriz \\(\\boldsymbol R^{-1}\\) son: \\[ \\{\\boldsymbol R^{-1}\\}_{jj} = \\frac{1}{1-R^{2}_{j}}, j=1,\\ldots,p-1, \\] donde \\(R^{2}_{j}\\) es el coeficiente de determinación de la regresión de \\(z_{j}\\) en función de las \\((p-2)\\) covariables restantes. Es decir, coeficiente de determinacón del siguiente ajuste: \\[ z_{ij} = z_{i1}\\alpha_1 + z_{i2}\\alpha_2 + \\ldots + z_{i,j-1}\\alpha_{j-1} + z_{i,j+1}\\alpha_{j+1} + \\ldots + z_{i,p-1}\\alpha_{p-1} + \\varepsilon_i. \\] Por lo que la varianza del estimador de \\(\\boldsymbol b\\) es: \\[ V(\\widehat{b}_{j}) = \\frac{\\sigma^{2}}{1-R^{2}_{j}}. \\] Por lo que, si \\(z_j\\) se puede expresar como una combinación lineal aproximada de las demás covariables, la varianza del estimador de \\(b_j\\) Si \\(R^{2}_{j} \\rightarrow 1\\), entonces \\(V(\\widehat{b}_{j})\\rightarrow \\infty\\). La multicolinealidad también produce que los estimadores \\(\\widehat{b}_{j}\\), para \\(j=1,\\ldots,p\\), sean muy grandes en valor absoluto. Considere: \\[ L_{1}^{2} = (\\widehat{\\boldsymbol b}- \\boldsymbol b)&#39;(\\widehat{\\boldsymbol b}- \\boldsymbol b) = \\sum_{j=1}^{k}(\\widehat{b}_{j}-b_{j})^{2}. \\] El valor esperado de \\(L_{1}^{2}\\) es: \\[ E(L_{1}^{2}) = \\sum_{j=1}^{p-1}E(\\widehat{b}_{j}-b_{j})^{2} = \\sum_{j=1}^{k}\\sigma^{2}\\mbox{tr}\\boldsymbol R^{-1} \\] Dado que la traza de una matriz es igual a la suma de sus valores propios, tenemos que: \\[ E(L_{1}^{2}) = \\sigma^{2}\\sum_{j=1}^{p-1}1/\\lambda_{j}, \\] donde \\(\\lambda_{j} &gt; 0\\), para \\(j=1,\\ldots,p-1\\), son los valores propios de \\(\\boldsymbol R\\). Por lo tanto, si \\(\\boldsymbol R\\) está mal condicionada debido a la multicolinealidad, al menos un \\(\\lambda_{j}\\) será muy pequeño. Cabe notar que, la multicolinealidad no viola ningún supuesto sobre los errores. Por lo tanto los estimadores por MCO siguen siendo BLUE. Pero, en presencia de multicolinealidad: Las varianzas de los coeficientes \\((\\widehat{\\beta}_{j})\\) se incrementan. Lo que reduce los valores \\(t\\) asociados. Los \\(\\widehat{\\beta}_{j}\\) son sensibles a las especificaciones. Estos pueden cambiar drásticamente cuando se agregan o eliminan covariables. El ajuste general del modelo, y por lo tanto los pronósticos y las predicciones, no se verá afectado en gran medida. Pero los intervalos de confianza que se calculen pueden ser muy amplios. 3.2.1 Causas de la multicolinealidad La multicolinealidad se puede deber a múltiples razones, entre las que están: Con frecuencia se presentan problemas donde intervienen procesos de producción o químicos, donde los regresores son los componentes de un producto y ésos suman una constante. Variables que son componentes de un sistema pueden mostrar dependencias casi lineales debido a las limitaciones biológicas, físicas o químicas del sistema. Variables que tienen una tendencia común y evolucionan de forma muy parecida en el tiempo. Inclusión de variables irrelevantes en el modelo. La información que contienen estas variables ya estarían incluidas en otras y no aportan a la explicación de la variabilidad de la variable respuesta. Por ejemplo, en los datos del cemento, tenemos que el problema de multicolinealidad se presenta porque: \\[ x_{i1} + x_{i2} + x_{i3} + x_{i4} \\approx \\mbox{constante}. \\] En los datos de grasa corporal, el problema es causado por la alta correlación entre el pliegue cutáneo del triceps \\((X_{1})\\) y la circunferencia del muslo \\((X_{2})\\). 3.3 Detección de multicolinealidad Dado que la multicolinealidad provoca una inflación de la varianza, un indicio de este problema esta en que aunque el modelo presenta un buen ajuste (un \\(R^2\\) por ejemplo), las estimaciones de coeficientes asociados a covariables releventas tienen valores \\(t\\) pequeños. Además, al eliminar covariables, las estimacioes cambian considerablemente. Los métodos mas usados para detectar multicolinealidad son: la matriz de correlación de las covariables. los factores de inflación de varianza. los índices de condiciones de \\(\\boldsymbol R\\) o \\(\\boldsymbol Z\\). 3.3.1 Factores de inflación de varianza Dado que \\(V(\\widehat{\\boldsymbol b}) = \\sigma^2\\boldsymbol R^{-1}\\), la diagonal de \\(\\boldsymbol R^{-1}\\) es un buen indicador de multicolinealidad: \\[ \\{\\boldsymbol R^{-1}\\}_{jj} = (1-R_{j}^{2})^{-1}, \\] donde \\(R_{j}^{2}\\) es el coeficiente de determinación de ajustar un modelo de \\(x_{j}\\) en función de las demás covariables. Si \\(x_{j}\\) es casi ortogonal a las demás covariables, \\(R^{2}_{j}\\) es pequeño, y por lo tanto, \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) cercano a \\(1\\). Mientras que, si \\(x_{j}\\) es casi linealmente dependiente a las demás covariables, entonces \\(R^{2}_{j}\\) es cercano a \\(1\\) y \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) grande. Es decir, \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) puede ser visto como un factor de cuanto se incrementa \\(V(\\widehat{b}_{j})\\) debido a la colinealidad entre las covariables. De aquí el nombre de factor de inflación de varianza (VIF). Generalmente, uno o mas valores grandes de VIF (\\(5\\) o \\(10\\)) es un indicador de problemas de multicolinealidad. 3.3.2 Valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) Dado que \\(\\boldsymbol R= \\boldsymbol Z&#39;\\boldsymbol Z\\) es una matriz simétrica y positiva semi-definida, entonces se puede descomponer: \\[ \\boldsymbol R= \\boldsymbol T\\boldsymbol \\Lambda\\boldsymbol T&#39;, \\] donde \\(\\boldsymbol T= (\\boldsymbol t_{1},\\ldots,\\boldsymbol t_{k})\\) es una matriz ortogonal de vectores propios, y \\(\\boldsymbol \\Lambda\\) una matriz diagonal con los valores propios \\(\\lambda_{j}\\), para \\(j=1,\\ldots,p-1\\), en la diagonal principal. Por lo tanto, \\[ V(\\widehat{\\boldsymbol \\beta}) = \\sigma^{2}\\boldsymbol T\\boldsymbol \\Lambda^{-1}\\boldsymbol T&#39;. \\] Entonces, \\[ V(\\widehat{b}_{l}) = \\sigma^{2}\\sum_{j=1}^{p-1}\\frac{t_{lj}^{2}}{\\lambda_{j}}. \\] De la expresión anterior, \\[ VIF_{j} = \\sum_{j=1}^{k}\\frac{t_{ij}^{2}}{\\lambda_{j}}, \\] Por lo tanto, uno o más valores propios pequeños pueden inflar la varianza de \\(\\widehat{b}_{j}\\). De aquí salen los indicadores llamados índices de condición y número de condición. El índice de condición está definido como: \\[ \\eta_{j} = \\frac{\\lambda_{\\mbox{max}}}{\\lambda_{j}}. \\] El número de condición está definido como el máximo índice de condición. Un número de condición mayor de 100 es un indicador de multicolinealidad. 3.3.3 Valores singulares de \\(\\boldsymbol Z\\) El número de condición también se puede calcular a partir de la descomposición en valores singulares (SVD) de la matriz de covariables \\(\\boldsymbol Z\\). Esto es: \\[ \\boldsymbol Z=\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39;, \\] donde \\(\\boldsymbol U\\) y \\(\\boldsymbol T\\) son matrices \\((n\\times p)\\) y \\((p\\times p)\\), respectivamente, que \\(\\boldsymbol U\\boldsymbol U= \\boldsymbol I\\) y \\(\\boldsymbol T&#39;\\boldsymbol T= \\boldsymbol I\\), y \\(\\boldsymbol D\\) es una matriz diagonal con elementos \\(\\mu_{j}\\), \\(j=1,\\ldots,p\\), llamados valores singulares. Hay una relación entre los valoes singulares de \\(\\boldsymbol Z\\) y los valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\): \\[ \\boldsymbol Z&#39;\\boldsymbol Z= (\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39;)&#39;\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39; = \\boldsymbol T\\boldsymbol D^2\\boldsymbol T&#39; = \\boldsymbol T\\boldsymbol \\Lambda\\boldsymbol T&#39;. \\] El índice de condición de \\(\\boldsymbol Z\\) está definido como: \\[ \\kappa_{j} = \\frac{\\mu_{\\mbox{max}}}{\\mu_{j}}. \\] El número de condición de \\(\\boldsymbol Z\\) está definido como el máximo \\(\\kappa_{j}\\). Un número de condición mayor de 10,15 o 30 es un indicador de problemas de multicolinealidad. Note que \\(\\kappa_{j} = \\sqrt{\\eta_{j}}\\). Por lo que el punto de corte de \\(10\\) para el número de condición de \\(\\boldsymbol Z\\) es equivalente al número de condición de \\(100\\) para el número de condición de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\). 3.3.4 Proporciones de descomposición de varianza Hay una relación entre el VIF, los valores propios y los valores singulares. Esta es: \\[ VIF_{j} = \\sum_{j=1}^{k}\\frac{t_{ij}^{2}}{\\lambda_{j}} = \\sum_{j=1}^{k}\\frac{t_{ij}^{2}}{\\mu_{j}^2}. \\] De aquí salen los indicadores llamados proporciones de descomposición de varianza. Estos se calculan de la siguiente forma: \\[ \\pi_{ij} = \\frac{t_{ij}^2 / \\mu_{i}^2 }{VIF_{j}}, j = 1,\\ldots, p-1. \\] Si se agrupan los \\(\\pi_{ij}\\) en una matriz \\((p \\times p)\\) \\(\\boldsymbol \\pi\\), la columna \\(j\\) son proporciones de la varianza de \\(\\widehat{\\boldsymbol b}\\). Valores \\(\\pi_{ij}\\) mayores de 0.5 indican problemas de multicolinealidad. 3.4 Datos de cemento Los VIF para el modelo ajustado para los datos de cemento son los siguientes: car::vif(mod.cement) ## x1 x2 x3 x4 ## 38.49621 254.42317 46.86839 282.51286 Aquí vemos que todos los VIFs son muy altos, mostrando que hay problemas graves de multicolinealidad. Para calcular los índices de condición (a partir de los valores singulares de \\(\\boldsymbol Z\\)) y las proporciones de descomposición de varianza se utiliza la función colldiag() de la librería perturb: library(perturb) colldiag(mod.cement,scale = TRUE,center = TRUE,add.intercept = FALSE) ## Condition ## Index Variance Decomposition Proportions ## x1 x2 x3 x4 ## 1 1.000 0.003 0.001 0.001 0.000 ## 2 1.191 0.004 0.000 0.005 0.000 ## 3 3.461 0.064 0.002 0.046 0.001 ## 4 37.106 0.930 0.997 0.947 0.998 El número de condición \\((37.106)\\) es muy alto reafirmando el problema de multicolinealidad. Además, también se puede observar que varias de las proporciones de descomposición de varianza son mayores de \\(0.5\\). 3.5 Datos de grasa corporal Para el modelo ajustado a losdatos de grasa corporal, los VIFs son: car::vif(mod.bodyfat) ## Triceps Thigh Midarm ## 708.8429 564.3434 104.6060 Además, los índices de condición y las proporciones de descomposición de varianza son: library(perturb) colldiag(mod.bodyfat,scale = TRUE,center = TRUE,add.intercept = FALSE) ## Condition ## Index Variance Decomposition Proportions ## Triceps Thigh Midarm ## 1 1.000 0.000 0.000 0.001 ## 2 1.488 0.000 0.000 0.008 ## 3 53.329 1.000 0.999 0.991 Todos estos indicadores muestran que hay problemas de multicolinealidad. 3.6 Solución al problema de multicolinealidad Una forma sencilla de resolver los problemas de multicolinealidad son: la recolección de datos adicionales: si se tiene control sobre las covariables, es posible tomar más observaciones para romper con la casi dependencia en \\(\\boldsymbol X\\). Pero esto en muchas casos es díficl por la naturaleza de las covariables. Por ejemplo, sería imposible buscar personas con circunferencia del muslo grande y pliegue cutáneo del tríceps bajo (o viceversa). la re-especificación del modelo: se pueden eliminar del modelo las covariables que me generan el problema de multicolinealidad y que pueden tener poco aporte explicativo dentro del modelo. Por ejemplo, en los datos de la grasa corporal podemos eliminar la variable asociada al muslo (o al tríceps, pues ambas proporcionan la misma información). Si quitamos esta covariable: mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat) car::vif(mod.bodyfat0) ## Triceps Midarm ## 1.265118 1.265118 los VIFs disminuyen considerablemente. Además, no hay una reducción notable del \\(R^2\\). Con las tres covariables temenos que \\(R^{2}=0.801\\). Mientras que al remover Thigh, tenemos que \\(R^{2} = 0.786\\) Considerando que el estimador por MCO es el mejor estimador lineal insesgado de \\(\\boldsymbol \\beta\\), podemos relajar la condición de insesgamiento y buscar estimadores que aunque sean sesgados tengan menor varianza que los estimadores por MCO. Dos de estas alternativas son: Estimador de ridge. Estimadores por componentes principales. 3.7 Estimador de ridge El estimador de ridge tiene como objetivo minimizar la siguiente suma de cuadrados penalizada: \\[\\begin{equation} \\begin{split} S_{R}(\\boldsymbol b) &amp;= \\sum_{i=1}^{n}(y_{i} - \\boldsymbol z_{i}&#39;\\boldsymbol b)^{2} + k \\sum_{j=1}^{p-1}\\boldsymbol b_{j}^2 \\\\ &amp;= (\\boldsymbol y- \\boldsymbol Z\\boldsymbol b)&#39;(\\boldsymbol y- \\boldsymbol Z\\boldsymbol b) + k ||\\boldsymbol b||_{2}^{2}, \\end{split} \\nonumber \\end{equation}\\] con \\(k \\geq 0\\) (el cual es seleccionado por el investigador). Lo que lleva a las siguientes ecuaciones normales: \\[ (\\boldsymbol R+ k \\boldsymbol I)\\widehat{\\boldsymbol b}_{R} = \\boldsymbol Z&#39;\\boldsymbol y. \\] La solución de las ecuaciones normales lleva al estimador ridge: \\[ \\widehat{\\boldsymbol b}_{R} = (\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol Z&#39;\\boldsymbol y. \\] Note que, incluso si \\(\\boldsymbol R\\) es no es invertible, un \\(k &gt; 0\\) resuelve el problema. Además, la solución depende de \\(k\\) (por cada \\(k\\), hay una estimación diferente). \\(k\\) es un parámetro de contracción: si \\(k \\rightarrow 0\\), encontramos que \\(\\widehat{\\boldsymbol b}_{R} \\rightarrow \\widehat{\\boldsymbol b}\\). si \\(k \\rightarrow \\infty\\), encontramos que \\(\\widehat{\\boldsymbol b}_{R} \\rightarrow \\boldsymbol 0\\) (excepto el intercepto). Ahora veamos las propiedades del estimador de ridge. El valor esperado de \\(\\widehat{\\boldsymbol b}_{R}\\) es: \\[ E(\\widehat{\\boldsymbol b}_{R}) = (\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol R\\boldsymbol b. \\] De aquí vemos que \\(\\widehat{\\boldsymbol b}_{R}\\) es sesgado, y aumenta con \\(k\\) (por lo que este parámetro también es llamado de sesgo). La varianza de \\(\\widehat{\\boldsymbol b}_{R}\\) es: \\[ V(\\widehat{\\boldsymbol b}_{R}) = \\sigma^{2}(\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol R(\\boldsymbol R+ k \\boldsymbol I)^{-1}. \\] A partir de las cantidades anteriores se puede determinar el error cuadrático medio (ECM) de \\(\\widehat{\\boldsymbol b}_{R}\\): \\[\\begin{equation} \\begin{split} \\mbox{ECM}(\\widehat{\\boldsymbol b}_{R}) &amp;= \\sigma^{2}\\mbox{tr}[(\\boldsymbol R+ k\\boldsymbol I)^{-1}\\boldsymbol R(\\boldsymbol R+ k\\boldsymbol I)^{-1}] + k^{2}\\boldsymbol b&#39;(\\boldsymbol R+ k\\boldsymbol I)^{-2}\\boldsymbol b\\\\ &amp;=\\sigma^{2}\\sum_{i=1}^{p-1}\\frac{\\lambda_{j}}{(\\lambda_{j}+ k)^2} + k^{2} \\boldsymbol b&#39;(\\boldsymbol R+ k\\boldsymbol I)^{-2}\\boldsymbol b, \\end{split} \\nonumber \\end{equation}\\] donde \\(\\lambda_{1},\\ldots,\\lambda_{k}\\) son los valores propios de \\(\\boldsymbol R\\). Finalmente, la suma de cuadrados de los residuos usando \\(\\widehat{\\boldsymbol b}_{R}\\) es: \\[ SS_{\\mbox{res}}= (\\boldsymbol y- \\boldsymbol X\\widehat{\\boldsymbol b})&#39;(\\boldsymbol y- \\boldsymbol X\\widehat{\\boldsymbol b}) + (\\widehat{\\boldsymbol b}_{R}-\\widehat{\\boldsymbol b})&#39;\\boldsymbol R(\\widehat{\\boldsymbol b}_{R}-\\widehat{\\boldsymbol b}). \\] A partir de estos resultados, vemos que: (1) Si \\(k\\) crece, disminuye la varianza, pero aumenta el sesgo. (2) Si \\(k\\) disminuye, aumenta la varianza, pero disminuye el sesgo. (3) el estimador ridge proporciona un \\(R^{2}\\) mas pequeño que el estimador por MCO. Además, \\(R^2\\) disminuye con \\(k\\). Sin embargo, la regresión ridge proporciona estimaciones de \\(\\boldsymbol b\\) más estables. La idea de la regresión de ridge es encontrar un valor de \\(k\\) tal que \\(\\mbox{ECM}(\\widehat{\\boldsymbol b}_{R}) &lt; V(\\widehat{\\boldsymbol b})\\). En la Figura ?? podemos ver la representación del ECM del estimador de ridge. Aquí vemos que a medida que aumenta \\(k\\), el sesgo incrementa y la varianza disminuye. También, hay una región de \\(k\\) donde se puede obtener un ECM del estimador de ridge menor que el que se obtiene por medio de MCO. La idea es encontrar el valor de \\(k\\) que minimiza el ECM, o por lo menos algún valor en la región donde \\(\\mbox{ECM}(\\widehat{\\boldsymbol b}_{R}) &lt; V(\\widehat{\\boldsymbol b})\\). x= seq(from=0,to=0.3,length.out = 100) sesgo = 0.25/(1+exp(-20*(x-0.001))) sesgo = sesgo - min(sesgo) vari = 0.06*exp(-70*x)+ 0.0025 ecm =sesgo+vari plot(x,sesgo,type = &#39;l&#39;,xlim=c(0,0.2),ylim=range(c(sesgo,vari,ecm)),lty=2, xaxt=&#39;n&#39;,yaxt=&#39;n&#39;, xlab=&#39;k&#39;,ylab=&#39;Sesgo, varianza y MSE&#39;) lines(x,vari,lty=3) lines(x,ecm,lwd=2) abline(h=max(vari),col=2) lines(c(x[ecm==min(ecm)],x[ecm==min(ecm)]),c(-1,min(ecm)),col=4) lines(c(-1,x[ecm==min(ecm)]),c(min(ecm),min(ecm)),col=4) axis(1,x[ecm==min(ecm)],&#39;k óptimo&#39;) Figure 3.3: Representación del sesgo al cuadrado (linea cortada), varianza (linea punteada) y error cuadrático medio (linea solida) del estimador de ridge. La linea roja representa el error cuadrático medio del estimador por MCO. Algunos métodos de selección de \\(k\\) son: Traza de ridge \\(k\\): el efecto de \\(k\\) sobre las estimaciones de \\(\\widehat{\\boldsymbol b}_{R}\\) es mas fuerte para valores bajos. De igual forma, si \\(k\\) es muy grande introducimos mucho sesgo. Por lo que se puede hacer es incrementar \\(k\\) hasta que parezca que su influencia sobre \\(\\widehat{\\boldsymbol b}_{R}\\) se atenúe. Validación cruzada (CV): sea \\(\\widehat{y}_{(i),k}\\) la estimación de \\(E(y_i)\\) por medio del estimador de ridge con el parámetro \\(k\\) y usando una muestra excluyendo la i-ésima observación. La validación cruzada está definida como: \\[ CV(k) = \\sum_{i=1}^{n} (y_i - \\widehat{y}_{(i),k})^2. \\] Por lo que la selección de \\(k\\) es: \\[ k_{CV} = \\arg \\min_{k} CV(k). \\] 3.7.1 Datos de cemento Para ajustar el modelo usando el estimador de ridge podemos usar la función lmridge del paquete lmridge. Primero, ajustamos el modelo usando diferentes valores de \\(k\\): library(lmridge) K = seq(from=0,to=0.3,length.out = 100) ridge.cement = lmridge(y~., data=cement,K=K,scaling=&#39;sc&#39;) En el objeto ridge.cement tenemos las estimaciones por el estimador de ridge para \\(100\\) valores de \\(k\\) entre \\(0\\) y \\(0.3\\). Para observar como cambian las estimaciones para los diferentes valores de \\(k\\) podemos graficar la traza de ridge así (en términos de las covariables en su escala orginal): EstRidge.cement = coef(ridge.cement) plot(K,EstRidge.cement[,2],type=&#39;l&#39;,ylim=range(EstRidge.cement[,-1]),lwd=2, ylab=&#39;Estimaciones de los coeficientes&#39;,xlab=&#39;k&#39;) lines(K,EstRidge.cement[,3],col=2,lwd=2) lines(K,EstRidge.cement[,4],col=3,lwd=2) lines(K,EstRidge.cement[,5],col=4,lwd=2) abline(h=0,lty=2) Figure 3.4: Datos de cemento. Traza de ridge. Coeficiente asociado a X1 (negro), coeficiente asociado a X2 (rojo), coeficiente asociado a X3 (verde) y coeficiente asociado a X4 (azul) En la Figura ?? podemos observar que, cuando incrementamos \\(k\\), las estimaciones cambian rápidamente y luego parecen estabilizarse cuando \\(k\\) es grande. Además hay un cambio de signo para el coeficiente asociado a X3. También puede usarse plot(mod.r) (traza de ridge para las estimaciones de los coeficientes asociados a las covariables escaladas). La selección del \\(k\\) óptimo por medio de validación cruzada (CV) se hace de la siguiente manera: Criterios.cement = kest(ridge.cement) plot(K,Criterios.cement$CV,type=&#39;l&#39;,xlab=&#39;K&#39;,ylab=&#39;validación cruzada&#39;) Figure 3.5: Datos de cemento. Validación cruzada. K[Criterios.cement$CV==min(Criterios.cement$CV)] ## [1] 0.009090909 Aquí podemos ver que el valorde \\(k\\) que minimiza la validación cruzada es \\(0.009\\). El valor óptimo de \\(k\\) por medio de otros criterios son: Criterios.cement ## Ridge k from different Authors ## ## k values ## Thisted (1976): 0.00581 ## Dwividi &amp; Srivastava (1978): 0.00291 ## LW (lm.ridge) 0.05183 ## LW (1976) 0.00797 ## HKB (1975) 0.01162 ## Kibria (2003) (AM) 0.28218 ## Minimum GCV at 0.02424 ## Minimum CV at 0.00909 ## Kibria 2003 (GM): 0.07733 ## Kibria 2003 (MED): 0.01718 ## Muniz et al. 2009 (KM2): 14.84574 ## Muniz et al. 2009 (KM3): 5.32606 ## Muniz et al. 2009 (KM4): 3.59606 ## Muniz et al. 2009 (KM5): 0.27808 ## Muniz et al. 2009 (KM6): 7.80532 ## Mansson et al. 2012 (KMN8): 14.98071 ## Mansson et al. 2012 (KMN9): 0.49624 ## Mansson et al. 2012 (KMN10): 6.63342 ## Mansson et al. 2012 (KMN11): 0.15075 ## Mansson et al. 2012 (KMN12): 8.06268 ## Dorugade et al. 2010: 0.00000 ## Dorugade et al. 2014: 0.00000 La estimación con \\(K=0.0101\\) se puede obtener usando la función lmridge usando \\(K=0.0101\\) como argumento: ridge.cement2 = lmridge(y~., data=cement,K=0.0101,scaling=&#39;sc&#39;) summary(ridge.cement2) ## ## Call: ## lmridge.default(formula = y ~ ., data = cement, K = 0.0101, scaling = &quot;sc&quot;) ## ## ## Coefficients: for Ridge parameter K= 0.0101 ## Estimate Estimate (Sc) StdErr (Sc) t-value (Sc) Pr(&gt;|t|) ## Intercept 82.7052 -267.6034 306.3344 -0.8736 0.4052 ## x1 1.3146 26.7886 3.9606 6.7637 0.0001 *** ## x2 0.3059 16.4871 5.2766 3.1246 0.0124 * ## x3 -0.1295 -2.8734 3.9360 -0.7300 0.4841 ## x4 -0.3432 -19.8985 5.4000 -3.6849 0.0051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Ridge Summary ## R2 adj-R2 DF ridge F AIC BIC ## 0.97180 0.96240 3.07629 133.92403 23.27712 58.35941 ## Ridge minimum MSE= 392.3519 at K= 0.0101 ## P-value for F-test ( 3.07629 , 9.74488 ) = 3.013147e-08 ## ------------------------------------------------------------------- Vemos algunas diferencias con las estimaciones por MCO. Hay un cambio de signo en la estimación del coeficiente asociado a x3. Si embargo, no hay mucha diferencia en las estimaciones de los otros coeficientes. También podemos observar que las covariables x1, x2 y x3 ahora tienen un aporte significativo. Como era de esperarse, hay una disminución del \\(R^{2}\\), sin embargo es muy leve. "]]
