[["index.html", "Notas de clase: Modelo lineal general II Introducción", " Notas de clase: Modelo lineal general II Alvaro J. Flórez 2023-01-31 Introducción Estas son las notas de clase del curso Modelo Lineal General II. Los temas que se tratan son: Variables indicadoras. Modelos polinomiales. Multicolinealidad. Selección de variables. Introducción a modelos no lineales. Introducción al modelo lineal generalizado (modelo logístico - modelo Poisson). Introducción al modelo lineal mixto Tenga en cuenta que el propósito de estas notas de clase no es reemplazar los textos guías. Para el estudio más detallado de los temas revisados, se recomiendan las siguientes lecturas: Introduction to Linear Regression Analysis, Fifth Ed., 2012, by Montgomery, D. C., Peck, E. A. and Vining, G. G. (Texto guía) Applied Regression Analysis, Third Ed., 1998, by Draper, N. R. and Smith, H., Wiley. Theory and Applications of the Linear Models, 2000, by Graybill, F. A., Duxbury. Applied Linear Statistical Models, Fifth Ed., 2005, by Kutner, M. H, Nachtsheim, C. J., Neter, J. and Li, W., McGraw-Hill. Análisis de Regresión. Introducción Teórica y Práctica basada en R, 2011, by F. Tusell. Applied Linear Regression, Fourth Ed., 2014, by S. Weisberg. Applied Regression Analysis &amp; Generalized Linear Models, 2016, by J. Fox. "],["variables-indicadoras-o-dummies.html", "Capítulo 1 Variables indicadoras (o dummies) 1.1 Ejemplos 1.2 Variables indicadoras", " Capítulo 1 Variables indicadoras (o dummies) 1.1 Ejemplos 1.1.1 Datos de atletas australianos Los datos ais de la libreria alr4 tiene información sobre 202 atletas de élite de Australia (102 hombres y 100 mujeres). Se quiere evaluar la relación entre la concentración de hemoglobina (Hg, g/dl) y el índice de masa corporal (BMI, kg/m\\(^2\\)). data(ais) par(mfrow=c(1,2)) plot(density(ais$Hg[ais$Sex==0]),xlim=c(11,20),lwd=2,main = &#39;&#39;,ylab=&#39;Densidad&#39;,xlab=&#39;Hg (g/dl)&#39;) lines(density(ais$Hg[ais$Sex==1]),col=2,lwd=2) plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) Figure 1.1: Datos de atletas. Densidad de la hemoglobina para hombres y mujeres (derecha) y diagrama de dispersión entre la homoglobina y el índice de masa corporal (izquierda). Negro para hombres y rojo para mujeres. En la Figura 1.1 (izquierda) vemos que los niveles de hemoglobina son mayores para hombres que para mujeres. En la Figura 1.1 (derecha) observamos que hay una relación positiva entre la hemoglobina y el índice de masa corporal tanto para hombres como para mujeres. Esto nos puede indicar que ingresar el sexo del atleta en el modelo puede mejorarnos el ajuste. 1.1.2 Datos de la ONU Retomemos los datos de la ONU (UN11 de la librería alr4). Las variables de interés son: fertility: Número esperado de nacidos vivos por mujer. ppgdp: producto nacional bruto per cápita (PNB, en dólares). Purban: el porcentaje de la población que vive en un área urbana. lifeExpF: esperanza de vida femenina (años). group: si el país pertenece a la OCDE (Organización para la Cooperación y el Desarrollo Económicos), África o otros. Por ahora consideremos la relación entre la fertilidad y el PNB per cápita, teniendo en cuenta el grupo al que pertenece cada país. data(UN11) plot(log(fertility)~log(ppgdp),data=UN11,col=UN11$group,xlab=&#39;log PNB per cápita (dólares)&#39;, ylab=&#39;log # esperado de nacidos vivos por mujer&#39;) Figure 1.2: Datos de la ONU. Relación entre la fertilidad y el PNB per cápita para los países de OCDE (puntos negros), países africanos (puntos verdes) y los otros países (rojos) En la Figura 1.2 podemos observar que, en general, cuando el PNB aumenta, la tasa de fertilidad disminuye. Sin embargo, esta relación puede variar según la categoría del país. Para los países de la OCDE, esta relación no es fuerte. Mientras que para los demás se mantiene esta relación negativa. Por esta razón sería de gran importancia incluir esta variable categórica dentro del modelo. 1.2 Variables indicadoras Las covariables categóricas entran en un modelo como variables indicadoras (o también llamadas dummies). En el caso que la covariable \\((X)\\) tenga dos categorías, entonces se crea una variable indicadora. Por ejemplo, para los datos de los atletas, el sexo requiere una indicadora: \\[ u_{i} = \\begin{cases} 1 &amp; \\mbox{ si la observación i es hombre}, \\\\ 0 &amp; \\mbox{ si la observación i es mujer}. \\\\ \\end{cases} \\] Aquí mujer es llamada la categoría de referencia. En caso que la covariable categórica tenga \\(k\\) categorías, se tienen que crear \\(k-1\\) variables indicadoras. Por ejemplo, para la variable grupo de país en los datos de la ONU se requieren 2 variables indicadoras \\((u_{j})\\) como lo muestra la Tabla 1.1. Table 1.1: Variables indicadoras para la variable group de los datos de la ONU Categoría \\(u_{1}\\) \\(u_{2}\\) OECD 0 0 otro 1 0 África 0 1 1.2.1 Modelos con covariables categóricas Suponga que se quiere ajustar un modelo para una variable respuesta \\(Y\\) en función de dos covariables: una continua \\(X\\) y una indicadora \\(Z\\) (es decir una variable categórica con dos 2 categorías). El modelo propuesto es el siguiente: \\[\\begin{equation} y_{i} = \\beta_{0} + x_{i}\\beta_{1} + z_{i}\\beta_{2} + x_{i}z_{i}\\beta_{3} + \\varepsilon_{i}, \\tag{1.1} \\end{equation}\\] donde \\(\\varepsilon_{i} \\sim N(0,\\sigma^{2})\\). Tenemos que, si \\(z_i=0\\): \\[ E(Y | X=x_i, Z=0) = \\beta_{0} + x_{i}\\beta_{1}. \\] Mientras que, si \\(z_i=1\\): \\[ E(Y | X=x_i, Z=1) = (\\beta_{0}+\\beta_{2}) + x_{i}(\\beta_{1}+\\beta_{3}). \\] Por lo que el modelo (1.1) genera dos rectas, una para cada categoría. \\(\\beta_2\\) indica la diferencia de intercepto entre las dos categorías y \\(\\beta_3\\) la diferencia entre pendientes. En la Figura 1.3(izquierda) se observan las dos rectas que se obtienen a partir de este modelo. Si se elimina la interacción entre variables, se obtienen dos rectas paralelas (Figura 1.3, derecha). Figure 1.3: Efecto de la interacción entre variable continua e indicadora. Modelo general (izquierda) y modelo de líneas paralelas (derecha). 1.2.2 Modelo para los datos de atletas australianos Para los datos de los atletas, se sugiere el siguiente modelo: \\[\\begin{equation} \\begin{split} \\mbox{Hg}_{i} =&amp; \\beta_{0} + \\mbox{Sex}_{i}\\beta_{1} + \\mbox{BMI}_{i}\\beta_{2} + \\mbox{Sex}_{i}\\mbox{BMI}_{i}\\beta_{3} + \\varepsilon_{i}, \\end{split} \\nonumber \\end{equation}\\] donde: \\[ \\mbox{Sex}_{i} = \\begin{cases} 1 &amp; \\mbox{ si la observación i corresponde a una mujer}, \\\\ 0 &amp; \\mbox{ de otra forma}. \\\\ \\end{cases} \\] Note que en la base de datos ais, la variable sex ya está codificada de esta forma. Si la variable no está codificada de forma numérica, R eligirá la categoría de referencia de forma automática. Para ajustar el modelo utilizamos la función lm (aquí estamos incluyendo los efectos de BMI y Sex, así como la interacción): mod.ais = lm(Hg~Sex*BMI, data=ais) summary(mod.ais) ## ## Call: ## lm(formula = Hg ~ Sex * BMI, data = ais) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9997 -0.6625 -0.0478 0.5784 3.5583 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.21316 0.78662 16.797 &lt;2e-16 *** ## Sex -0.66140 1.09835 -0.602 0.5477 ## BMI 0.09788 0.03269 2.994 0.0031 ** ## Sex:BMI -0.05203 0.04761 -1.093 0.2758 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9092 on 198 degrees of freedom ## Multiple R-squared: 0.5613, Adjusted R-squared: 0.5546 ## F-statistic: 84.44 on 3 and 198 DF, p-value: &lt; 2.2e-16 Tenemos que \\(\\widehat{\\beta}_{2} =0.1\\), lo que nos indica que el valor esperado del nivel de hemoglobina aumenta en \\(0.1\\) g/dl por cada aumento unitario en el índice de masa corporal de los hombres. Para las mujeres, el efecto del índice de masa corporal sobre el nivel de hemoglobina también es positivo, pero con una pendiente menor \\(\\widehat{\\beta}_{2} +\\widehat{\\beta}_{3} = 0.1 - 0.05 = 0.05\\). La representación gráfica del modelo se puede observar en la Figura 1.4. Aquí vemos que la pendiente para las mujeres es un poco menor que para los hombres. plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) abline(a=mod.ais$coefficients[1],b=mod.ais$coefficients[3],lwd=2) abline(a=mod.ais$coefficients[1]+mod.ais$coefficients[2],b=mod.ais$coefficients[3]+mod.ais$coefficients[4],col=2,lwd=2) Figure 1.4: Datos de atletas. Ajuste del modelo para la homoglobina en función del índice de masa corporal y sexo. Línea negra para hombres y línea roja para mujeres. Si miramos la significancia del \\(\\widehat{\\beta}_3\\) (valor-\\(p\\) igual a 0.276), podemos concluir que las diferencias en pendiente no son significativas. Por lo que, el efecto del índice de masa corporal sobre los niveles de hemoglobina es el mismo para mujeres que para hombres. Si queremos evaluar si los niveles de hemoglobina son iguales para hombres y mujeres, debemos evaluar la siguiente hipótesis: \\[ H_{0}: \\beta_{1} = \\beta_3 = 0. \\] En R, esto lo podemos realizar usando la función anova() (prueba F) comparando el modelo completo contra el modelo reducido sin la variable Sex: mod.ais.red = lm(Hg~BMI, data=ais) anova(mod.ais.red,mod.ais) ## Analysis of Variance Table ## ## Model 1: Hg ~ BMI ## Model 2: Hg ~ Sex * BMI ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 200 318.52 ## 2 198 163.69 2 154.82 93.637 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Vemos que se rechaza \\(H_0\\), es decir que hay diferencias en los niveles de homoglobina entre hombres y mujeres. \\(\\beta_1\\) o \\(\\beta_3\\) es diferente de cero, pero no ambos (esto por los resultados del modelo completo). Eliminando la interacción obtenemos un modelo con lineas paralelas: \\[ \\mbox{Hg}_{i} = \\beta_{0} + \\mbox{Sex}_{i}\\beta_{1} + \\mbox{BMI}_{i}\\beta_{2} + \\varepsilon_{i}, \\] donde \\(\\varepsilon_{i} \\sim N(0, \\sigma^{2})\\), y se calcula de la siguiente forma: mod.ais.lp = lm(Hg~Sex+BMI, data=ais) summary(mod.ais.lp) ## ## Call: ## lm(formula = Hg ~ Sex + BMI, data = ais) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0131 -0.6530 -0.0263 0.6249 3.5806 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.79954 0.57549 23.979 &lt; 2e-16 *** ## Sex -1.85251 0.13587 -13.634 &lt; 2e-16 *** ## BMI 0.07335 0.02378 3.085 0.00233 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9097 on 199 degrees of freedom ## Multiple R-squared: 0.5586, Adjusted R-squared: 0.5542 ## F-statistic: 125.9 on 2 and 199 DF, p-value: &lt; 2.2e-16 La estimación del efecto asociado al sexo indica la diferencia media del nivel de hemoglobina entre hombres y mujeres. En la Figura 1.5 vemos que el modelo de líneas paralelas tiene casi el mismo ajuste que el modelo general. plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) abline(a=mod.ais$coefficients[1],b=mod.ais$coefficients[3],lwd=2) abline(a=mod.ais$coefficients[1]+mod.ais$coefficients[2],b=mod.ais$coefficients[3]+mod.ais$coefficients[4],col=2,lwd=2) abline(a=mod.ais.lp$coefficients[1],b=mod.ais.lp$coefficients[3],lwd=2,lty=2) abline(a=mod.ais.lp$coefficients[1]+mod.ais.lp$coefficients[2],b=mod.ais.lp$coefficients[3],col=2,lwd=2,lty=2) Figure 1.5: Datos de atletas. Ajuste del modelo general (línea solida) y el modelo líneas paralelas (línea discontinua) para la homoglobina en función del índice de masa corporal y sexo. Líneas negra para hombres y líneas roja para mujeres. En conclusión, hay diferencia en los niveles de hemoglobina entre hombres y mujeres (es, en promedio, mayor para lo hombres en 1.85g/dl). Sin embargo, el efecto del índice de masa corporal es el mismo para ambos sexos. 1.2.3 Modelo para los datos de la ONU El modelo propuesto es el siguiente: \\[\\begin{equation} \\begin{split} \\log\\mbox{fertility}_{i} =&amp; \\beta_{0} + \\mbox{u}_{1i}\\beta_{1}+\\mbox{u}_{2i}\\beta_{2} + \\log\\mbox{ppgdp}_{i}\\beta_{3} + \\\\ &amp; \\mbox{u}_{1i}\\log\\mbox{ppgdp}_{i}\\beta_{4} + \\mbox{u}_{2i}\\log\\mbox{ppgdp}_{i}\\beta_{5} + \\varepsilon_{i}, \\end{split} \\nonumber \\end{equation}\\] donde: \\[ \\mbox{u}_{1i} = \\begin{cases} 1 &amp; \\mbox{ si el país i pertenece a la categoría otro}, \\\\ 0 &amp; \\mbox{ de otra forma}, \\\\ \\end{cases} \\mbox{ y } \\mbox{u}_{2i} = \\begin{cases} 1 &amp; \\mbox{ si el país i pertenece a África}, \\\\ 0 &amp; \\mbox{ de otra forma}. \\\\ \\end{cases} \\] Por lo tanto, OECD es la categoría de referencia. El modelo ajustado es: mod.UN11 = lm(log(fertility)~group*log(ppgdp), data=UN11) summary(mod.UN11) ## ## Call: ## lm(formula = log(fertility) ~ group * log(ppgdp), data = UN11) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69358 -0.16963 0.02005 0.16838 0.73633 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.21836 0.81290 0.269 0.78851 ## groupother 1.87888 0.83290 2.256 0.02520 * ## groupafrica 2.54072 0.84299 3.014 0.00293 ** ## log(ppgdp) 0.03217 0.07832 0.411 0.68170 ## groupother:log(ppgdp) -0.18418 0.08106 -2.272 0.02417 * ## groupafrica:log(ppgdp) -0.22637 0.08430 -2.685 0.00788 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2739 on 193 degrees of freedom ## Multiple R-squared: 0.6305, Adjusted R-squared: 0.6209 ## F-statistic: 65.86 on 5 and 193 DF, p-value: &lt; 2.2e-16 A partir de estos resultados obtenemos tres rectas que describen el valor esperado del logarítmo de la fertilidad en función del logarítmo del PNB per cápita, una para cada tipo de país. Para los países de la OCDE, tenemos que: \\[ E(\\log\\mbox{fertility}) = 0.218 + 0.032\\log \\mbox{ppgdp}. \\] Para los países Africanos: \\[ E(\\log\\mbox{fertility}) = 2.759 -0.194\\log \\mbox{ppgdp}. \\] Finalmente, para los otros países: \\[ E(\\log\\mbox{fertility}) = 2.097 -0.152\\log \\mbox{ppgdp}. \\] Aquí vemos que para los países que no son de la OCDE, el PNB tiene un efecto negativo significativo sobre la tasa de fertilidad. Mientras que para los países de la OCDE, este efecto es positivo, aunque no es significativo. Esto mismo lo podemos ver gráficamente en la Figura 1.6. Beta.UN11 = mod.UN11$coefficients plot(log(fertility)~log(ppgdp),data=UN11,col=UN11$group,xlab=&#39;log PNB per cápita (dólares)&#39;, ylab=&#39;log # esperado de nacidos vivos por mujer&#39;) abline(a=Beta.UN11[1],b=Beta.UN11[4],lwd=2) abline(a=Beta.UN11[1]+Beta.UN11[2],b=Beta.UN11[4]+Beta.UN11[5],col=2,lwd=2) abline(a=Beta.UN11[1]+Beta.UN11[3],b=Beta.UN11[4]+Beta.UN11[6],col=3,lwd=2) Figure 1.6: Datos de la ONU. Ajuste del modelo para la fertilidad en función del PNB y tipo de país. Países de OCDE (línea negra), países africanos (línea verde) y los otros países (línea roja) Usando la función relevel() se puede cambiar la categoría de referencia. Por ejemplo, si queremos que la categoría de referencia sea other: UN11.alt = UN11 UN11.alt$group = relevel(UN11.alt$group,ref =&#39;other&#39;) mod.UN11.alt = lm(log(fertility)~group*log(ppgdp), data=UN11.alt) summary(mod.UN11.alt) Note que cambiar la categoría de referencia no altera en nada los resultados del ajuste. Solo cambian la interpretación de los coeficientes. Se podría hacer la siguiente pregunta, ¿el efecto del PNB sobre la fertilidad es el mismo para cada tipo de país?. Para resolver esta pregunta, se plantea la siguiente hipótesis: \\[ H_0: \\beta_4 = \\beta_5 = 0. \\] El estadístico de prueba lo podemos obtener usando la función anova() (prueba F) en R: mod.UN11.red = lm(log(fertility)~group+log(ppgdp), data=UN11) anova(mod.UN11.red,mod.UN11) ## Analysis of Variance Table ## ## Model 1: log(fertility) ~ group + log(ppgdp) ## Model 2: log(fertility) ~ group * log(ppgdp) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 195 15.033 ## 2 193 14.484 2 0.54848 3.6542 0.02769 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Este resultado indica que hay evidencia suficiente para concluir que el efecto del PNB sobre la fertilidad no es el mismo para cada tipo de país. Ahora, podríamos preguntarnos: ¿las pendientes son las mismas para las categorías de otro y África?. Para esto, se plantea la siguiente hipótesis: \\[ H_0: \\beta_4 = \\beta_5. \\] También se puede expresar de la siguiente forma: \\[ H_{0}: \\boldsymbol L\\boldsymbol \\beta= \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{pmatrix} = 0 \\] Esto, en R es: L = matrix(c(0,0,0,0,1,-1),1,6,byrow = T) linearHypothesis(mod.UN11, hypothesis.matrix=L) ## Linear hypothesis test ## ## Hypothesis: ## groupother:log(ppgdp) - groupafrica:log(ppgdp) = 0 ## ## Model 1: restricted model ## Model 2: log(fertility) ~ group * log(ppgdp) ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 194 14.579 ## 2 193 14.484 1 0.094808 1.2633 0.2624 Dado que no se rechaza \\(H_0\\), el efecto del PNB sobre la fertilidad es el mismo para los países Africanos y los de la categoría de otros. En resumen, a partir del análisis de regresión: El efecto del PNB per cápita sobre la fertilidad depende del tipo de país. Para los países miembros de la OECD, no hay un efecto significativo. Sin embaro, para los demás países, el efecto es negativo (y no es significativamente diferente para las categorías de África y otros). "],["modelos-polinomiales.html", "Capítulo 2 Modelos polinomiales 2.1 Ejemplos 2.2 Modelos polinomiales 2.3 Regresión por segmentos", " Capítulo 2 Modelos polinomiales 2.1 Ejemplos 2.1.1 Pasteles Con los datos cakes de la libreria alr4 se tienen dos objetivos. Primero, evaluar el efecto de la temperatura y tiempo de horneado sobre la palatabilidad de mazclas de pasteles para hornear. Segundo, encontrar la combinación de estos factores que maximizan la palatabilidad. Como variable respuesta (Y) se tiene el promedio de calificación de la palatabilidad de cuatro pasteles horneados. Mientras que las covariables son: el tiempo de horneado (X1, en minutos) y la temperatura (X2, en grados Fahrenheit) library(alr4) data(cakes) plot(cakes[,-1]) Figure 2.1: Datos de pasteles. Diagrama de dispersión. 2.1.2 Datos de Boston La base de datos Boston de la libreria MASS contiene información sobre 506 suburbios del area metropolitana de Boston. El objetivo del estudio es evaluar la relación del precio de las viviendas y la concentración de contaminación ambiental. En esta sección evaluaremos la relación entre la concentración anual de óxido de nitrógeno (\\(y\\), en partes por diez millones) y la distancia a cinco centros de empleo. library(MASS) data(Boston) plot(nox~dis,data=Boston,ylab=&#39;NOx&#39;,xlab=&#39;distancia a centros de empleo&#39;) Figure 2.2: Datos de Boston. Relación entre el óxido de nitrógeno y la distancia a centros de empleo. 2.2 Modelos polinomiales El modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_i, \\] describe la relación lineal entre \\(y\\) y \\(x_1\\). Si las relación entre las variables presentan curvaturas, se puede considerar un modelo polinómico de la forma: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + \\ldots + + \\beta_{k}x_{i}^{k} + \\varepsilon_i. \\] Este modelo se sigue considerando como un modelo lineal, dado que es lineal en los parámetros \\(\\boldsymbol \\beta\\). En la Figura 2.3 se puede observar diferentes curvas para modelos lineales (orden 1), cuadráticos (orden 2) y cúbicos (orden 3). Aquí vemos que este tipo de modelos son muy versatiles. Cualquier función suave se puede ajustar meidante un polinomio de grado suficientemente alto. Por esta razón, los modelos polinomicos son usados en casos donde las relaciones entre las variables son no-lineales y se pueden aproximar por un polinomio. Figure 2.3: Polinomio de grado 1 (linea negra), grado 2 (linea roja) y grado 3 (linea verde). En el caso que se tengan dos covariables, un modelo de orden 2 se expresa de la forma: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}^2 + \\beta_{4}x_{2i}^2 + \\beta_{5}x_{1i}x_{2i} + \\varepsilon_i. \\] En las Figuras 2.4-2.6 muestran el valor esperado de \\(Y\\) en un modelo lineal (asumiendo \\(\\beta_3=\\beta_4=\\beta_5=0\\)), con interacción (\\(\\beta_3=\\beta_4=0\\)) y cuadrático. Figure 2.4: Valor esperado de \\(Y\\) en un modelo lineal (izquierda) y gráfico de contorno (derecha). El número de parámetros incrementa rápidamente con el número de covariables. Con \\(k\\) covariables, tenemos: un intercepto, \\(k\\) términos lineales, \\(k\\) términos cuadráticos, y \\(k(k - 1)/2\\) interacciones. Por esta razón, en muchos casos, no se toman en cuentas las interacción cuando el número de covariables es grande. Figure 2.5: Valor esperado de \\(Y\\) en un modelo cuadrático (izquierda) y gráfico de contorno (derecha). Figure 2.6: Valor esperado de \\(Y\\) en un modelo lineal con interacción (izquierda) y gráfico de contorno (derecha). Hay aspectos que se deben tener en la práctica cuando se implementa un modelo polinomial: Selección del orden: La idea es mantener el orden del polinomio bajo. Si embargo, si es muy bajo no logra capturar la curvatura presente en los datos. En caso que el orden sea grande, el modelo es innecesariamente más complejo y puede haber problemas de multicolinealidad. Si los datos exige un modelo de orden alto \\((k &gt; 3)\\), se pueden hacer transformación sobre las variables, y así, poder ajustar un modelo polinomial de orden bajo (por ejemplo cuadrático). La selección del orden puede hacerse de dos formas. (1) Hacia delante: empezar con un modelo de orden \\(1\\) e incrementar el orden uno a uno hasta que un término mayor ya no sea significativo. Hacia atrás: empezar con el modelo más complejo y eliminar los términos mayores uno a uno hasta que todos sean significativos. Extrapolación: La extrapolación con modelos polinomiales puede ser muy peligrosa. Por ejemplo en la Figura 2.7 podemos ajustar un modelo de orden dos a los datos (linea negra). Si hacemos una predicción fuera del rango de los datos, el valor esperado predicho sigue el comportamiento cuadrático propuesto. Sin embargo, el valor esperado de \\(Y\\) puede seguir un comportamiento diferente (linea roja discontinua). Lo que lleva a tener una predicción sesgada. Figure 2.7: Problema de extrapolación. Multicolinealidad: Al aumentar el polinomio, la matriz \\(\\boldsymbol X&#39;\\boldsymbol X\\) se vuelve mal acondicionada. Es decir, las estimaciones pueden ser inestables y los errores estándar se inflan. Este problema se puede solucionar centrando las covariables. Por ejemplo en un modelo de orden 2: \\[ E(Y | X=x) = \\beta_{0} + \\beta_{1}(x-\\bar{x}) + \\beta_{2}(x-\\bar{x})^{2}. \\] Otra solución es usando polinomios ortogonales. 2.2.1 Interpretación de los coeficientes Considere un modelo de orden 2. El valor esperado de \\(Y\\) está dado por: \\[ E(Y| X_{1}=x_1,X_{2}=x_2) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}^{2} + \\beta_{4}x_{2}^{2} + \\beta_{5}x_{1}x_{2}. \\] Si \\(x_{1}\\) cambia en \\(\\delta\\) unidades \\((x_{1} + \\delta)\\), tenemos que: \\[ E(Y| X_{1}=x_1+\\delta,X_{2}=x_2) = \\beta_{0} + \\beta_{1}(x_{1}+\\delta) + \\beta_{2}x_{2} + \\beta_{3}(x_{1} + \\delta)^{2} + \\beta_{4}x_{2}^{2} + \\beta_{5}(x_{1}+\\delta)x_{2}. \\] Ahora calculando la diferencia: \\[ E(Y| X_{1}=x_1+\\delta,X_{2}=x_2) - E(Y| X_{1}=x_1,X_{2}=x_2) = (\\beta_{1}\\delta + \\beta_{3}\\delta^{2}) + 2\\beta_{3}\\delta x_{1} + \\beta_{5}\\delta x_{2}. \\] Note que el efecto del cambio \\(\\delta\\) en \\(X_1\\) depende de ambas covariables y del valor de \\(\\delta\\). Por esta razón, la interpretación de los coeficientes es complicada. 2.2.2 Pasteles Para los datos de los pasteles, se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}^2 + \\beta_{4}x_{2i}^{2} + \\beta_{5}x_{1i}x_{2i} + \\varepsilon_{i}. \\] El ajuste del modelo es: mod.cakes = lm(Y ~ X1*X2 + I(X1^2)+I(X2^2),data=cakes) summary(mod.cakes) ## ## Call: ## lm(formula = Y ~ X1 * X2 + I(X1^2) + I(X2^2), data = cakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4912 -0.3080 0.0200 0.2658 0.5454 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.204e+03 2.416e+02 -9.125 1.67e-05 *** ## X1 2.592e+01 4.659e+00 5.563 0.000533 *** ## X2 9.918e+00 1.167e+00 8.502 2.81e-05 *** ## I(X1^2) -1.569e-01 3.945e-02 -3.977 0.004079 ** ## I(X2^2) -1.195e-02 1.578e-03 -7.574 6.46e-05 *** ## X1:X2 -4.163e-02 1.072e-02 -3.883 0.004654 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4288 on 8 degrees of freedom ## Multiple R-squared: 0.9487, Adjusted R-squared: 0.9167 ## F-statistic: 29.6 on 5 and 8 DF, p-value: 5.864e-05 Además, los factores de inflación de varianza son los siguientes: car::vif(mod.cakes) ## X1 X2 I(X1^2) I(X2^2) X1:X2 ## 3778.083 5921.833 1328.089 5309.339 3063.500 Aquí vemos que los VIF presentan valores muy altos, producto de ajustar un modelo cuadrático. Ahora consideremos el modelo con las covariables centradas: cakes$X1c = cakes$X1 - mean(cakes$X1) cakes$X2c = cakes$X2 - mean(cakes$X2) modc.cakes = lm(Y ~ X1c*X2c + I(X1c^2)+I(X2c^2),data=cakes) summary(modc.cakes) ## ## Call: ## lm(formula = Y ~ X1c * X2c + I(X1c^2) + I(X2c^2), data = cakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4912 -0.3080 0.0200 0.2658 0.5454 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.070000 0.175044 46.103 5.41e-11 *** ## X1c 0.367558 0.075796 4.849 0.001273 ** ## X2c 0.096392 0.015159 6.359 0.000219 *** ## I(X1c^2) -0.156875 0.039446 -3.977 0.004079 ** ## I(X2c^2) -0.011950 0.001578 -7.574 6.46e-05 *** ## X1c:X2c -0.041625 0.010719 -3.883 0.004654 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4288 on 8 degrees of freedom ## Multiple R-squared: 0.9487, Adjusted R-squared: 0.9167 ## F-statistic: 29.6 on 5 and 8 DF, p-value: 5.864e-05 Los VIFs del modelo con las covariables centradas son: car::vif(modc.cakes) ## X1c X2c I(X1c^2) I(X2c^2) X1c:X2c ## 1.000000 1.000000 1.005952 1.005952 1.000000 En este caso, los VIF decrecieron considerablemente con respecto al modelo con las covariables originales. En la figura 2.8 podemos observar el valor esperado estimado de la palatabilidad para diferentes valores de tiempo y temperatura de horneado. Figure 2.8: Datos de pasteles. Valor esperado de la palatabilidad para diferentes valores de tiempo y temperatura de horneado. En la Figura 2.8 (izquierda) vemos que, cuando la temperatura es de 350 grados Fahrenheit, el máximo de palatabilidad que se obtiene en un tiempo entre 36 y 37 minutos. Sin embargo, cuando la temperatura se incrementa a 360, la máxima palatibilidad que se puede lograr es menor. Además, se obtiene en un tiempo también menor. El efecto de la interacción también se puede observar en la Figura 2.9 (derecha), pero ahora fijando el tiempo de horneado y variando la temperatura. El gráfico de contornos (Figura 2.9) muestra que la máxima palatabilidad se observa cuando la temperatura está alrededor de 355 y el tiempo de horneado está entre 35 y 36 minutos. X1 = seq(32, 38, length.out = 50) X2 = seq(335, 365, length= 50) y &lt;- outer(X= X1, Y = X2, FUN = function(x, y) { predict(modc.cakes, newdata = data.frame(X1c = x-mean(cakes$X1), X2c = y-mean(cakes$X2))) }) contour(X1, X2, y,xlab=&#39;tiempo de horneado (minutos)&#39;, ylab=&#39;temperatura de horneado (Fahrenheit)&#39;) Figure 2.9: Datos de pasteles. Gráfica de contornos. Para determinar en que combinación de tiempo (X1) y temperatura de horneado (X2) se obtiene la máxima palatabilidad, debemos resolver la siguiente ecuación: \\[ \\frac{\\partial E(Y)}{\\partial \\boldsymbol x} = \\frac{\\partial}{\\partial \\boldsymbol x} \\left( \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}^2 + \\beta_{4}x_{2}^{2} + \\beta_{5}x_{1}x_{2} \\right) = \\boldsymbol 0, \\] verificando que se obtiene un máximo. Resolviendo la ecuación anterior, el máximo valor esperado de palatabilidad \\((8.3)\\) se obtiene en un tiempo de horneado de 35.8 minutos y a una temperatura de F. El intervalo del 95% de confianza en este punto es: \\((7.9, 8.7)\\). 2.2.3 Datos de Boston Para los datos de contaminación se puede proponer el siguiente modelo: \\[ \\mbox{NOx}_{i}^{-1.5} = \\beta_{0} + \\beta_{1}\\mbox{dis}_{i} + \\beta_{2}\\mbox{dis}_{i}^{2} + \\beta_{3}\\mbox{dis}_{i}^{3} + \\varepsilon_{i}. \\] La potencia en la variable respuesta se seleccionó usando el método de Box-Cox. El ajuste del modelo es el siguiente: Boston$disc = Boston$dis - mean(Boston$dis) mod3.Boston = lm(I(nox)^(-1)~disc+I(disc^2)+I(disc^3),data=Boston) summary(mod3.Boston) ## ## Call: ## lm(formula = I(nox)^(-1) ~ disc + I(disc^2) + I(disc^3), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45677 -0.10233 0.00922 0.13091 0.32643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9785763 0.0121059 163.440 &lt; 2e-16 *** ## disc 0.1788433 0.0051507 34.722 &lt; 2e-16 *** ## I(disc^2) -0.0257929 0.0029431 -8.764 &lt; 2e-16 *** ## I(disc^3) 0.0013534 0.0004814 2.811 0.00513 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.173 on 502 degrees of freedom ## Multiple R-squared: 0.7765, Adjusted R-squared: 0.7752 ## F-statistic: 581.5 on 3 and 502 DF, p-value: &lt; 2.2e-16 La Figura ?? muestra el ajuste del modelo cúbico. Para hacer comparaciones, se muestra también el ajuste lineal y cuadrático. Se observa que el modelo cúbico presenta un buen ajuste. El modelo explica alrededor del \\(78.3\\)% de la variabilidad de la concentración anual de óxido de nitrógeno. Con el modelo cuadrático, el coeficiente de determinación es de \\(0.781\\). Sin embargo, se puede observar que este ajuste es un poco deficiente cuando las distancias son muy grandes (mayores a 11). Figure 2.10: Datos de Boston. Valor esperado de la concentración anual de óxido de nitrógeno (en partes por diez millones) en función de las distancias a cinco centros de empleo (media ponderada). Modelo lineal (negro), cuadrático (rojo) y cúbico (verde). 2.3 Regresión por segmentos El modelo por segmentos se puede expresar como: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(x_{i}-t)^{0}_{+} + \\beta_{3}(x_{i}-t)^{1}_{+} + \\varepsilon_{i}. \\] donde: \\[ (x_{i}-t)_{+}^{r} = \\begin{cases} 0 &amp; \\mbox{ si } x_{i} + t \\leq 0, \\\\ (x_{i}-t)^{r} &amp; \\mbox{ si } x_{i} + t &gt; 0. \\\\ \\end{cases} \\nonumber \\] Por lo tanto, si \\(x_{i} \\leq t\\), \\(E(y_{i}| x_{i}) = \\beta_{0} + \\beta_{1}x_{i}\\). Mientras que, si \\(x_{i} &gt; t\\), tenemos que \\(E(y_{i}| x_{i}) = (\\beta_{0} + \\beta_{2} - \\beta_{1}t) + (\\beta_{1} + \\beta_{3})x_{i}\\). Este modelo está representado graficamente en la Figura 2.11. Cuando \\(\\beta_2 \\neq 0\\), el modelo presenta una discontinuidad en \\(t\\). Mientras que, cuando \\(\\beta_2 = 0\\), el modelo presenta un cambio de pendiente en el punto \\(t\\). Además, \\(\\beta_3\\) indica el cambio de pendiente. Figure 2.11: Modelo de regresión por segmentos. Modelo con discontinuidad en \\(t\\) (izquierda). Modelo con cambio de pendiente en \\(t\\) (derecha). Aquí asumimos que \\(t\\) es conocido. Si este valor se asume como desconocido, debe estimarse a partir de los datos como un parámetro adicional. Sin embargo, se tendría que recurrir a un método de estimación para modelos no-lineales. 2.3.1 Ejemplo library(MPV) data(p7.11) plot(p7.11,ylab=&#39;costo de producción por unidad (USD)&#39;,xlab=&#39;Unidades por lote&#39;) Figure 2.12: Datos de costos por lote. Diagram de dispersión de el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). Considere la base de datos p7.11 de la librería MPV. Aquí se quiere modelar la relación entre el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). Este relación se puede observar en la Figura 2.12. Se puede observar que la relación entre las variables es lineal. Sin embargo, se aprecia un posible cambio de pendiente en el punto \\(x=200\\). Por esta razón se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(x_{i}-200)_{+}^{1} + \\varepsilon_{i}. \\] Note que no se asume ninguna discontinuidad. El ajuste del modelo es: p7.11$x2 = p7.11$x - 200 p7.11$x2[p7.11$x &lt; 200] = 0 mod.lotes = lm(y~.,data=p7.11) summary(mod.lotes) ## ## Call: ## lm(formula = y ~ ., data = p7.11) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.37596 -0.16641 -0.09677 0.20363 0.51734 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.116481 0.535383 28.235 2.67e-09 *** ## x -0.050199 0.003332 -15.065 3.73e-07 *** ## x2 0.038852 0.005946 6.534 0.000181 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3157 on 8 degrees of freedom ## Multiple R-squared: 0.9829, Adjusted R-squared: 0.9787 ## F-statistic: 230.4 on 2 and 8 DF, p-value: 8.474e-08 A partir del ajuste podemos concluir que, por cada unidad que incrementa el lote, el costo de producción disminuye en \\(0.05\\)USD. Si embargo, si el tamaño del lote es mayor a \\(200\\), el costo de producción disminuye solamente \\(0.01\\)USD cuando el lote aumenta en un artículo. Gráficamente, el ajuste se puede observar en la Figura 2.13. b.lotes = mod.lotes$coefficients plot(p7.11$x,p7.11$y,ylab=&#39;costo de producción por unidad (USD)&#39;,xlab=&#39;unidades por lote&#39;) x = c(100,200) lines(x,b.lotes[1]+x*b.lotes[2],lwd=2) x = c(200,300) lines(x,b.lotes[1]-200*b.lotes[3]+x*(b.lotes[2]+b.lotes[3]),lwd=2) abline(v=200,lty=2) Figure 2.13: Datos de costos por lote. Diagram de dispersión de el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). "],["multicolinealidad.html", "Capítulo 3 Multicolinealidad 3.1 Ejemplos 3.2 Multicolinealidad 3.3 Detección de multicolinealidad 3.4 Datos de cemento 3.5 Datos de grasa corporal 3.6 Solución al problema de multicolinealidad 3.7 Estimador de ridge 3.8 Estimador por componentes principales", " Capítulo 3 Multicolinealidad 3.1 Ejemplos 3.1.1 Cemento Los datos data(cement) en la librería MASS corresponden a un experimento sobre calor emanado en el fraguado de diferentes combinaciones químicas de cemento. Se tiene una muestra de \\(13\\) fraguados de cemento Portland. En cada muestra, se midió con precisión los porcentajes de los cuatro ingredientes químicos principales (covariables). Mientras el cemento fraguaba, también se midió la cantidad de calor desprendido (cals/gm, variable respuesta). Los cuatro ingredientes químicos son: x1 aluminato tricálcico (%). x2 silicato tricálcico (%). x3 tetra-aluminio ferrita de calcio (%). x4 silicato dicálcico (%). En la Figura 3.1 podemos observar que hay relaciones lineales positivas entre la variable respuesta, y las covariables aluminato tricálcico y silicato tricálcico. Mientras que la relación con la variable silicato dicálcico es negativa. También podemos notar que hay una relación negativa fuerte en las covariables aluminato tricálcico y tetra-aluminio ferrita de calcio, y entre silicato tricálcico y silicato dicálcico. data(cement,package = &#39;MASS&#39;) plot(cement[,c(5,1:4)]) Figure 3.1: Datos de cemento. Diagrama de dispersión. Para estos datos, se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + x_{1i}\\beta_{1} + x_{2i}\\beta_{2} + x_{3i}\\beta_{3} + x_{4i}\\beta_{4} + \\varepsilon_{i}. \\] Los resultados del ajuste son: mod.cement = lm(y ~ x1+x2+x3+x4,data=cement) summary(mod.cement) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3 + x4, data = cement) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1750 -1.6709 0.2508 1.3783 3.9254 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.4054 70.0710 0.891 0.3991 ## x1 1.5511 0.7448 2.083 0.0708 . ## x2 0.5102 0.7238 0.705 0.5009 ## x3 0.1019 0.7547 0.135 0.8959 ## x4 -0.1441 0.7091 -0.203 0.8441 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.446 on 8 degrees of freedom ## Multiple R-squared: 0.9824, Adjusted R-squared: 0.9736 ## F-statistic: 111.5 on 4 and 8 DF, p-value: 4.756e-07 Note que el ajuste es bueno, el \\(98.2\\)% de la variabilidad de la cantidad de calor desprendido durante la fraguado es explicada por el modelo. Sin embargo, los resultados de las pruebas de hipótesis individuales sobre los coeficientes muestran que no son significantes. 3.1.2 Grasa corporal Se tiene una muestra de \\(20\\) mujeres saludables con edades entre \\(25\\) y \\(34\\) años (data(bodyfat) en la librería isdals). La medición del porcentaje de grasa corporal es caro y engorroso, por lo tanto se quiere buscar un modelo que proporcione predicciones fiables. Como variables de este modelo se utiliza: Triceps: pliegue cutáneo del tríceps (cm). Thigh: circunferencia del muslo (cm). Midarm: circunferencia del brazo medio (cm). La Figura 3.2 muestra la relación entre variables. Podemos observar que hay una relación fuerte del % de masa corporal con las covariables Triceps y Thigh. Además hay una relación lineal fuerte entre esas dos covariables. library(isdals) data(bodyfat) plot(bodyfat) Figure 3.2: Datos de grasa corporal. Diagrama de dispersión. El modelo propuesto es: \\[ \\mbox{Fat}_{i} = \\beta_{0} + \\mbox{Triceps}_{i}\\beta_{1} + \\mbox{Thigh}_{i}\\beta_{2} + \\mbox{Midarm}_{i}\\beta_{3} + \\varepsilon_{i}. \\] El ajuste del modelo es el siguiente: mod.bodyfat = lm(Fat ~ Triceps+Thigh+Midarm,data=bodyfat) summary(mod.bodyfat) ## ## Call: ## lm(formula = Fat ~ Triceps + Thigh + Midarm, data = bodyfat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7263 -1.6111 0.3923 1.4656 4.1277 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 117.085 99.782 1.173 0.258 ## Triceps 4.334 3.016 1.437 0.170 ## Thigh -2.857 2.582 -1.106 0.285 ## Midarm -2.186 1.595 -1.370 0.190 ## ## Residual standard error: 2.48 on 16 degrees of freedom ## Multiple R-squared: 0.8014, Adjusted R-squared: 0.7641 ## F-statistic: 21.52 on 3 and 16 DF, p-value: 7.343e-06 Asi como en el caso anterior, observamos que el modelo explica gran parte de la variabilidad del % de grasa corporal. Sin embargo, los valores \\(p\\) de las pruebas individuales sobre los coeficientes son altos. 3.2 Multicolinealidad El estimador de \\(\\boldsymbol \\beta\\) es \\(\\widehat{\\boldsymbol \\beta}= (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol y\\). Además, \\(V(\\widehat{\\boldsymbol \\beta}) = \\sigma^{2}(\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\). Por lo tanto requiere que \\(\\boldsymbol X\\) sea de rango completo. Es decir, que hayan columnas que sean linealmente dependientes. En algunos casos, las columnas de \\(\\boldsymbol X\\) son casi linealmente dependientes o colineales,lo que lleva a que \\(\\boldsymbol X&#39;\\boldsymbol X\\) se casi singular, lo que provoca problemas a la hora de hacer inferencias. Sea \\(\\boldsymbol x_j\\) la \\(j\\)-ésima columna de la matriz \\(\\boldsymbol X\\), por lo tanto \\(\\boldsymbol X= (\\boldsymbol 1,\\boldsymbol x_{1},\\ldots,\\boldsymbol x_{p-1})\\). Los vectores \\(\\boldsymbol x_{1},\\boldsymbol x_{2},\\ldots,\\boldsymbol x_{p-1}\\) son linealmente dependientes si hay conjunto de constantes \\(a_{1},a_{2},\\ldots, a_{p-1}\\) no todas igual a cero, tal que: \\[ \\sum_{j=1}^{p-1}a_{j}\\boldsymbol x_{j} = c, \\mbox{ donde }c\\mbox{ es una constante.} \\] Si esto se cumple para un subconjunto de \\(\\boldsymbol X\\), el rango de la matriz \\(\\boldsymbol X&#39;\\boldsymbol X\\) es menor que \\(p\\), y por lo tanto, \\((\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\) no existe. Si la relación es aproximada \\((\\sum_{j=1}^{p-1}a_{j}\\boldsymbol x_{j} \\approx c)\\), existe el problema de multicolinealidad. Vamos a ilustrar el efecto de la multicolinealidad con un ejemplo sencillo considerando un modelo lineal con dos covariables: \\[ y_{i}^{*} = z_{i1}\\beta_1 +z_{i2}\\beta_2 +\\varepsilon_i, \\] donde las covariables están escaladas con longitud unitaria. Esto es: \\[ y_{i}^{*} = \\frac{y_{i}-\\bar{y}}{\\sqrt{SST}}, \\mbox{ y } z_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{\\sqrt{s_{jj}}}, \\] donde \\(s_{jj} = \\sum_{i=1}^{n}(x_{ij}-\\bar{x}_{j})^{2}\\). Además, sea \\(\\boldsymbol Z\\) la matriz de covariables. Con esta transformación, tenemos que \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) es la matriz de correlación de las covariables, \\[ \\boldsymbol Z&#39;\\boldsymbol Z= \\begin{pmatrix} 1 &amp; r_{12} \\\\ r_{12} &amp; 1 \\end{pmatrix}, \\] y \\(\\boldsymbol Z&#39;\\boldsymbol y\\) es el vector de correlaciones entre \\(y\\) y las dos covariables: \\[ \\boldsymbol Z&#39;\\boldsymbol y^{*} = \\begin{pmatrix} r_{y1} \\\\ r_{y2} \\end{pmatrix}. \\] Por lo que el estimador por MCO de \\(\\boldsymbol b\\) es: \\[ \\widehat{\\boldsymbol b}= \\begin{pmatrix} 1 &amp; r_{12} \\\\ r_{12} &amp; 1 \\end{pmatrix}^{-1}\\begin{pmatrix} r_{y1} \\\\ r_{y2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1-r_{12}^{2}} &amp; \\frac{-r_{12}}{1-r_{12}^{2}} \\\\ \\frac{-r_{12}}{1-r_{12}^{2}} &amp; \\frac{1}{1-r_{12}^{2}} \\end{pmatrix} \\begin{pmatrix} r_{1y} \\\\ r_{2y} \\end{pmatrix}. \\] Particularmente, tenemos: \\[ \\widehat{b}_{1} = \\frac{r_{1y}-r_{12}r_{2y}}{1-r_{12}^{2}} \\mbox{ y } \\widehat{b}_{2} = \\frac{r_{2y}-r_{12}r_{1y}}{1-r_{12}^{2}}. \\] Además, la matriz de varianzas-covarianzas de \\(\\widehat{\\boldsymbol b}\\) es: \\[\\begin{equation} V(\\widehat{\\boldsymbol b}) = \\sigma^{2} \\begin{pmatrix} \\frac{1}{1-r_{12}^{2}} &amp; \\frac{-r_{12}}{1-r_{12}^{2}} \\\\ \\frac{-r_{12}}{1-r_{12}^{2}} &amp; \\frac{1}{1-r_{12}^{2}} \\end{pmatrix}. \\tag{3.1} \\end{equation}\\] En (3.1) podemos observar que la varianza de los coeficientes \\(\\widehat{\\boldsymbol b}\\) tienden a infinito cuando \\(|r_{12}|\\rightarrow 1\\). Mientras que se hace mínima cuando las covariables están incorrelacionadas \\((r_{12}=0)\\). Es decir, una fuerte correlación entre \\(x_1\\) y \\(x_2\\) da como resultado grandes varianzas y covarianzas para \\(\\widehat{\\boldsymbol b}\\). Por esta razón es preferible tener covariables que sean ortogonales. Puesto que así se garantiza la menor varianza para \\(\\widehat{\\boldsymbol \\beta}\\). Sin embargo, las covariables son díficiles de controlar en estudios observacionales. Ahora consideremos un modelo con \\(p-1\\) covariables, \\[ y_{i}^{*} = b_{1}z_{i1} + b_{2}z_{i2} + \\ldots + b_{p-1}z_{i,p-1} + \\varepsilon_{i}. \\] Las ecuaciones normales son: \\[ \\begin{pmatrix} 1 &amp; r_{12} &amp; r_{13} &amp; \\ldots &amp; r_{1,p-1} \\\\ r_{12} &amp; 1 &amp; r_{23} &amp; \\ldots &amp; r_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{1,p-1} &amp; r_{2,p-1} &amp; r_{3,p-1} &amp; \\ldots &amp; 1 \\end{pmatrix} \\begin{pmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{p-1} \\end{pmatrix} = \\begin{pmatrix} r_{y1} \\\\ r_{y2} \\\\ \\vdots \\\\ r_{y,p-1} \\end{pmatrix}. \\] Por lo que el estimador de \\(\\boldsymbol b\\) es: \\[ \\widehat{\\boldsymbol b}= \\boldsymbol R^{-1}\\boldsymbol Z&#39;\\boldsymbol y^{*}, \\mbox{donde }\\boldsymbol R= \\boldsymbol Z&#39;\\boldsymbol Z. \\] Se puede demostrar que los elementos de la diagonal de la matriz \\(\\boldsymbol R^{-1}\\) son: \\[ \\{\\boldsymbol R^{-1}\\}_{jj} = \\frac{1}{1-R^{2}_{j}}, j=1,\\ldots,p-1, \\] donde \\(R^{2}_{j}\\) es el coeficiente de determinación de la regresión de \\(z_{j}\\) en función de las \\((p-2)\\) covariables restantes. Es decir, coeficiente de determinacón del siguiente ajuste: \\[ z_{ij} = z_{i1}\\alpha_1 + z_{i2}\\alpha_2 + \\ldots + z_{i,j-1}\\alpha_{j-1} + z_{i,j+1}\\alpha_{j+1} + \\ldots + z_{i,p-1}\\alpha_{p-1} + \\varepsilon_i. \\] Por lo que la varianza del estimador de \\(\\boldsymbol b\\) es: \\[ V(\\widehat{b}_{j}) = \\frac{\\sigma^{2}}{1-R^{2}_{j}}. \\] Por lo que, si \\(z_j\\) se puede expresar como una combinación lineal aproximada de las demás covariables, la varianza del estimador de \\(b_j\\) Si \\(R^{2}_{j} \\rightarrow 1\\), entonces \\(V(\\widehat{b}_{j})\\rightarrow \\infty\\). La multicolinealidad también produce que los estimadores \\(\\widehat{b}_{j}\\), para \\(j=1,\\ldots,p-1\\), sean muy grandes en valor absoluto. Considere: \\[ L_{1}^{2} = (\\widehat{\\boldsymbol b}- \\boldsymbol b)&#39;(\\widehat{\\boldsymbol b}- \\boldsymbol b) = \\sum_{j=1}^{p}(\\widehat{b}_{j}-b_{j})^{2}. \\] El valor esperado de \\(L_{1}^{2}\\) es: \\[ E(L_{1}^{2}) = \\sum_{j=1}^{p-1}E(\\widehat{b}_{j}-b_{j})^{2} = \\sum_{j=1}^{p-1}\\sigma^{2}\\mbox{tr}\\boldsymbol R^{-1} \\] Dado que la traza de una matriz es igual a la suma de sus valores propios, tenemos que: \\[ E(L_{1}^{2}) = \\sigma^{2}\\sum_{j=1}^{p-1}1/\\lambda_{j}, \\] donde \\(\\lambda_{j} &gt; 0\\), para \\(j=1,\\ldots,p-1\\), son los valores propios de \\(\\boldsymbol R\\). Por lo tanto, si \\(\\boldsymbol R\\) está mal condicionada debido a la multicolinealidad, al menos un \\(\\lambda_{j}\\) será muy pequeño. Cabe notar que, la multicolinealidad no viola ningún supuesto sobre los errores. Por lo tanto los estimadores por MCO siguen siendo BLUE. Pero, en presencia de multicolinealidad: Las varianzas de los coeficientes estimados \\((\\widehat{\\beta}_{j})\\) se incrementan. Lo que reduce los valores \\(t\\) asociados. Los \\(\\widehat{\\beta}_{j}\\) son sensibles a las especificaciones. Estos pueden cambiar drásticamente cuando se agregan o eliminan covariables. El ajuste general del modelo, y por lo tanto los pronósticos y las predicciones, no se verá afectado en gran medida. Pero los intervalos de confianza que se calculen pueden ser muy amplios. 3.2.1 Causas de la multicolinealidad La multicolinealidad se puede deber a múltiples razones, entre las que están: Con frecuencia se presentan problemas donde intervienen procesos de producción o químicos, donde los regresores son los componentes de un producto y ésos suman una constante. Variables que son componentes de un sistema pueden mostrar dependencias casi lineales debido a las limitaciones biológicas, físicas o químicas del sistema. Variables que tienen una tendencia común y evolucionan de forma muy parecida en el tiempo. Inclusión de variables irrelevantes en el modelo. La información que contienen estas variables ya estarían incluidas en otras y no aportan a la explicación de la variabilidad de la variable respuesta. Por ejemplo, en los datos del cemento, tenemos que el problema de multicolinealidad se presenta porque: \\[ x_{i1} + x_{i2} + x_{i3} + x_{i4} \\approx \\mbox{constante}. \\] En los datos de grasa corporal, el problema es causado por la alta correlación entre el pliegue cutáneo del triceps \\((X_{1})\\) y la circunferencia del muslo \\((X_{2})\\). 3.3 Detección de multicolinealidad Dado que la multicolinealidad provoca una inflación de la varianza, un indicio de este problema esta en que aunque el modelo presenta un buen ajuste (un \\(R^2\\) por ejemplo), las estimaciones de coeficientes asociados a covariables releventas tienen valores \\(t\\) pequeños. Además, al eliminar covariables, las estimacioes cambian considerablemente. Los métodos mas usados para detectar multicolinealidad son: la matriz de correlación de las covariables. los factores de inflación de varianza. los índices de condiciones de \\(\\boldsymbol R\\) o \\(\\boldsymbol Z\\). 3.3.1 Factores de inflación de varianza Dado que \\(V(\\widehat{\\boldsymbol b}) = \\sigma^2\\boldsymbol R^{-1}\\), la diagonal de \\(\\boldsymbol R^{-1}\\) es un buen indicador de multicolinealidad: \\[ \\{\\boldsymbol R^{-1}\\}_{jj} = (1-R_{j}^{2})^{-1}, \\] donde \\(R_{j}^{2}\\) es el coeficiente de determinación de ajustar un modelo de \\(x_{j}\\) en función de las demás covariables. Si \\(x_{j}\\) es casi ortogonal a las demás covariables, \\(R^{2}_{j}\\) es pequeño, y por lo tanto, \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) cercano a \\(1\\). Mientras que, si \\(x_{j}\\) es casi linealmente dependiente a las demás covariables, entonces \\(R^{2}_{j}\\) es cercano a \\(1\\) y \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) grande. Es decir, \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) puede ser visto como un factor de cuanto se incrementa \\(V(\\widehat{b}_{j})\\) debido a la colinealidad entre las covariables. De aquí el nombre de factor de inflación de varianza (VIF). Generalmente, uno o mas valores grandes de VIF (\\(5\\) o \\(10\\)) es un indicador de problemas de multicolinealidad. 3.3.2 Valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) Dado que \\(\\boldsymbol R= \\boldsymbol Z&#39;\\boldsymbol Z\\) es una matriz simétrica y positiva semi-definida, entonces se puede descomponer: \\[ \\boldsymbol R= \\boldsymbol T\\boldsymbol \\Lambda\\boldsymbol T&#39;, \\] donde \\(\\boldsymbol T= (\\boldsymbol t_{1},\\ldots,\\boldsymbol t_{k})\\) es una matriz ortogonal de vectores propios, y \\(\\boldsymbol \\Lambda\\) una matriz diagonal con los valores propios \\(\\lambda_{j}\\), para \\(j=1,\\ldots,p-1\\), en la diagonal principal. Por lo tanto, \\[ V(\\widehat{\\boldsymbol \\beta}) = \\sigma^{2}\\boldsymbol T\\boldsymbol \\Lambda^{-1}\\boldsymbol T&#39;. \\] Entonces, \\[ V(\\widehat{b}_{j}) = \\sigma^{2}\\sum_{k=1}^{p-1}\\frac{t_{jk}^{2}}{\\lambda_{k}}. \\] De la expresión anterior, \\[ VIF_{j} = \\sum_{j=1}^{p-1}\\frac{t_{ij}^{2}}{\\lambda_{j}}, \\] Por lo tanto, uno o más valores propios pequeños pueden inflar la varianza de \\(\\widehat{b}_{j}\\). De aquí salen los indicadores llamados índices de condición y número de condición. El índice de condición está definido como: \\[ \\eta_{j} = \\frac{\\lambda_{\\mbox{max}}}{\\lambda_{j}}. \\] El número de condición está definido como el máximo índice de condición. Un número de condición mayor de 100 es un indicador de multicolinealidad. 3.3.3 Valores singulares de \\(\\boldsymbol Z\\) El número de condición también se puede calcular a partir de la descomposición en valores singulares (SVD) de la matriz de covariables \\(\\boldsymbol Z\\). Esto es: \\[ \\boldsymbol Z=\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39;, \\] donde \\(\\boldsymbol U\\) y \\(\\boldsymbol T\\) son matrices \\(n\\times (p-1)\\) y \\((p-1)\\times (p-1)\\), respectivamente, \\(\\boldsymbol U\\boldsymbol U= \\boldsymbol I\\) y \\(\\boldsymbol T&#39;\\boldsymbol T= \\boldsymbol I\\), y \\(\\boldsymbol D\\) es una matriz diagonal con elementos \\(\\mu_{j}\\), \\(j=1,\\ldots,p-1\\), llamados valores singulares. Hay una relación entre los valoes singulares de \\(\\boldsymbol Z\\) y los valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\): \\[ \\boldsymbol Z&#39;\\boldsymbol Z= (\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39;)&#39;\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39; = \\boldsymbol T\\boldsymbol D^2\\boldsymbol T&#39; = \\boldsymbol T\\boldsymbol \\Lambda\\boldsymbol T&#39;. \\] El índice de condición de \\(\\boldsymbol Z\\) está definido como: \\[ \\kappa_{j} = \\frac{\\mu_{\\mbox{max}}}{\\mu_{j}}. \\] El número de condición de \\(\\boldsymbol Z\\) está definido como el máximo \\(\\kappa_{j}\\). Un número de condición mayor de 10,15 o 30 es un indicador de problemas de multicolinealidad. Note que \\(\\kappa_{j} = \\sqrt{\\eta_{j}}\\). Por lo que el punto de corte de \\(10\\) para el número de condición de \\(\\boldsymbol Z\\) es equivalente al número de condición de \\(100\\) para el número de condición de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\). 3.3.4 Proporciones de descomposición de varianza Hay una relación entre el VIF, los valores propios y los valores singulares. Esta es: \\[ VIF_{j} = \\sum_{j=1}^{p-1}\\frac{t_{ij}^{2}}{\\lambda_{j}} = \\sum_{j=1}^{p-1}\\frac{t_{ij}^{2}}{\\mu_{j}^2}. \\] De aquí salen los indicadores llamados proporciones de descomposición de varianza. Estos se calculan de la siguiente forma: \\[ \\pi_{ij} = \\frac{t_{ij}^2 / \\mu_{i}^2 }{VIF_{j}}, j = 1,\\ldots, p-1. \\] Si se agrupan los \\(\\pi_{ij}\\) en una matriz \\((p-1) \\times (p-1)\\) \\(\\boldsymbol \\pi\\), la columna \\(j\\) son proporciones de la varianza de \\(\\widehat{\\boldsymbol b}\\). Valores \\(\\pi_{ij}\\) mayores de 0.5 indican problemas de multicolinealidad. 3.4 Datos de cemento Los VIF para el modelo ajustado para los datos de cemento son los siguientes: car::vif(mod.cement) ## x1 x2 x3 x4 ## 38.49621 254.42317 46.86839 282.51286 Aquí vemos que todos los VIFs son muy altos, mostrando que hay problemas graves de multicolinealidad. Para calcular los índices de condición (a partir de los valores singulares de \\(\\boldsymbol Z\\)) y las proporciones de descomposición de varianza se utiliza la función colldiag() de la librería perturb: library(perturb) colldiag(mod.cement,scale = TRUE,center = TRUE,add.intercept = FALSE) ## Condition ## Index Variance Decomposition Proportions ## x1 x2 x3 x4 ## 1 1.000 0.003 0.001 0.001 0.000 ## 2 1.191 0.004 0.000 0.005 0.000 ## 3 3.461 0.064 0.002 0.046 0.001 ## 4 37.106 0.930 0.997 0.947 0.998 El número de condición \\((37.106)\\) es muy alto reafirmando el problema de multicolinealidad. Además, también se puede observar que varias de las proporciones de descomposición de varianza son mayores de \\(0.5\\). 3.5 Datos de grasa corporal Para el modelo ajustado a los datos de grasa corporal, los VIFs son: car::vif(mod.bodyfat) ## Triceps Thigh Midarm ## 708.8429 564.3434 104.6060 Además, los índices de condición y las proporciones de descomposición de varianza son: library(perturb) colldiag(mod.bodyfat,scale = TRUE,center = TRUE,add.intercept = FALSE) ## Condition ## Index Variance Decomposition Proportions ## Triceps Thigh Midarm ## 1 1.000 0.000 0.000 0.001 ## 2 1.488 0.000 0.000 0.008 ## 3 53.329 1.000 0.999 0.991 Todos estos indicadores muestran que hay problemas de multicolinealidad. 3.6 Solución al problema de multicolinealidad Una forma sencilla de resolver los problemas de multicolinealidad son: recolección de datos adicionales: si se tiene control sobre las covariables, es posible tomar más observaciones para romper con la casi dependencia en \\(\\boldsymbol X\\). Pero esto en muchas casos es díficl por la naturaleza de las covariables. Por ejemplo, sería imposible buscar personas con circunferencia del muslo grande y pliegue cutáneo del tríceps bajo (o viceversa). re-especificación del modelo: se pueden eliminar del modelo las covariables que me generan el problema de multicolinealidad y que pueden tener poco aporte explicativo dentro del modelo. Por ejemplo, en los datos de la grasa corporal podemos eliminar la variable asociada al muslo (o al tríceps, pues ambas proporcionan la misma información). Si quitamos esta covariable: mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat) car::vif(mod.bodyfat0) ## Triceps Midarm ## 1.265118 1.265118 los VIFs disminuyen considerablemente. Además, no hay una reducción notable del \\(R^2\\). Con las tres covariables temenos que \\(R^{2}=0.801\\). Mientras que al remover Thigh, tenemos que \\(R^{2} = 0.786\\) Considerando que el estimador por MCO es el mejor estimador lineal insesgado de \\(\\boldsymbol \\beta\\), podemos relajar la condición de insesgamiento y buscar estimadores que aunque sean sesgados tengan menor varianza que los estimadores por MCO. Dos de estas alternativas son: Estimador de ridge. Estimadores por componentes principales. 3.7 Estimador de ridge El estimador de ridge tiene como objetivo minimizar la siguiente suma de cuadrados penalizada: \\[\\begin{equation} \\begin{split} S_{R}(\\boldsymbol b) &amp;= \\sum_{i=1}^{n}(y_{i} - \\boldsymbol z_{i}&#39;\\boldsymbol b)^{2} + k \\sum_{j=1}^{p-1}\\boldsymbol b_{j}^2 \\\\ &amp;= (\\boldsymbol y- \\boldsymbol Z\\boldsymbol b)&#39;(\\boldsymbol y- \\boldsymbol Z\\boldsymbol b) + k ||\\boldsymbol b||_{2}^{2}, \\end{split} \\nonumber \\end{equation}\\] con \\(k \\geq 0\\) (el cual es seleccionado por el investigador). Lo que lleva a las siguientes ecuaciones normales: \\[ (\\boldsymbol R+ k \\boldsymbol I)\\widehat{\\boldsymbol b}_{R} = \\boldsymbol Z&#39;\\boldsymbol y. \\] La solución de las ecuaciones normales lleva al estimador ridge: \\[ \\widehat{\\boldsymbol b}_{R} = (\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol Z&#39;\\boldsymbol y. \\] Note que, incluso si \\(\\boldsymbol R\\) es no es invertible, un \\(k &gt; 0\\) resuelve el problema. Además, la solución depende de \\(k\\) (por cada \\(k\\), hay una estimación diferente). \\(k\\) es un parámetro de contracción: si \\(k \\rightarrow 0\\), encontramos que \\(\\widehat{\\boldsymbol b}_{R} \\rightarrow \\widehat{\\boldsymbol b}\\). si \\(k \\rightarrow \\infty\\), encontramos que \\(\\widehat{\\boldsymbol b}_{R} \\rightarrow \\boldsymbol 0\\) (excepto el intercepto). Ahora veamos las propiedades del estimador de ridge. El valor esperado de \\(\\widehat{\\boldsymbol b}_{R}\\) es: \\[ E(\\widehat{\\boldsymbol b}_{R}) = (\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol R\\boldsymbol b. \\] De aquí vemos que \\(\\widehat{\\boldsymbol b}_{R}\\) es sesgado, y aumenta con \\(k\\) (por lo que este parámetro también es llamado de sesgo). La varianza de \\(\\widehat{\\boldsymbol b}_{R}\\) es: \\[ V(\\widehat{\\boldsymbol b}_{R}) = \\sigma^{2}(\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol R(\\boldsymbol R+ k \\boldsymbol I)^{-1}. \\] A partir de las cantidades anteriores se puede determinar el error cuadrático medio (ECM) de \\(\\widehat{\\boldsymbol b}_{R}\\): \\[\\begin{equation} \\begin{split} \\mbox{ECM}(\\widehat{\\boldsymbol b}_{R}) &amp;= \\sigma^{2}\\mbox{tr}[(\\boldsymbol R+ k\\boldsymbol I)^{-1}\\boldsymbol R(\\boldsymbol R+ k\\boldsymbol I)^{-1}] + k^{2}\\boldsymbol b&#39;(\\boldsymbol R+ k\\boldsymbol I)^{-2}\\boldsymbol b\\\\ &amp;=\\sigma^{2}\\sum_{i=1}^{p-1}\\frac{\\lambda_{j}}{(\\lambda_{j}+ k)^2} + k^{2} \\boldsymbol b&#39;(\\boldsymbol R+ k\\boldsymbol I)^{-2}\\boldsymbol b, \\end{split} \\nonumber \\end{equation}\\] donde \\(\\lambda_{1},\\ldots,\\lambda_{k}\\) son los valores propios de \\(\\boldsymbol R\\). Finalmente, la suma de cuadrados de los residuos usando \\(\\widehat{\\boldsymbol b}_{R}\\) es: \\[ SS_{\\mbox{res}}= (\\boldsymbol y- \\boldsymbol X\\widehat{\\boldsymbol b})&#39;(\\boldsymbol y- \\boldsymbol X\\widehat{\\boldsymbol b}) + (\\widehat{\\boldsymbol b}_{R}-\\widehat{\\boldsymbol b})&#39;\\boldsymbol R(\\widehat{\\boldsymbol b}_{R}-\\widehat{\\boldsymbol b}). \\] A partir de estos resultados, vemos que: (1) Si \\(k\\) crece, disminuye la varianza, pero aumenta el sesgo. (2) Si \\(k\\) disminuye, aumenta la varianza, pero disminuye el sesgo. (3) el estimador ridge proporciona un \\(R^{2}\\) mas pequeño que el estimador por MCO. Sin embargo, la regresión ridge proporciona estimaciones de \\(\\boldsymbol b\\) más estables. La idea de la regresión de ridge es encontrar un valor de \\(k\\) tal que \\(\\mbox{ECM}(\\widehat{\\boldsymbol b}_{R}) &lt; V(\\widehat{\\boldsymbol b})\\). En la Figura 3.3 podemos ver la representación del ECM del estimador de ridge. Aquí vemos que a medida que aumenta \\(k\\), el sesgo incrementa y la varianza disminuye. Además, hay una región de \\(k\\) donde se puede obtener un ECM del estimador de ridge menor que el que se obtiene por medio de MCO. La idea es encontrar el valor de \\(k\\) que minimiza el ECM, o por lo menos algún valor en la región donde \\(\\mbox{ECM}(\\widehat{\\boldsymbol b}_{R}) &lt; V(\\widehat{\\boldsymbol b})\\). x= seq(from=0,to=0.3,length.out = 100) sesgo = 0.25/(1+exp(-20*(x-0.001))) sesgo = sesgo - min(sesgo) vari = 0.06*exp(-70*x)+ 0.0025 ecm =sesgo+vari plot(x,sesgo,type = &#39;l&#39;,xlim=c(0,0.2),ylim=range(c(sesgo,vari,ecm)),lty=2, xaxt=&#39;n&#39;,yaxt=&#39;n&#39;, xlab=&#39;k&#39;,ylab=&#39;Sesgo, varianza y MSE&#39;) lines(x,vari,lty=3) lines(x,ecm,lwd=2) abline(h=max(vari),col=2) lines(c(x[ecm==min(ecm)],x[ecm==min(ecm)]),c(-1,min(ecm)),col=4) lines(c(-1,x[ecm==min(ecm)]),c(min(ecm),min(ecm)),col=4) axis(1,x[ecm==min(ecm)],&#39;k óptimo&#39;) Figure 3.3: Representación del sesgo al cuadrado (linea cortada), varianza (linea punteada) y error cuadrático medio (linea solida) del estimador de ridge. La linea roja representa el error cuadrático medio del estimador por MCO. Algunos métodos de selección de \\(k\\) son: Traza de ridge \\(k\\): el efecto de \\(k\\) sobre las estimaciones de \\(\\widehat{\\boldsymbol b}_{R}\\) es mas fuerte para valores bajos. De igual forma, si \\(k\\) es muy grande introducimos mucho sesgo. Por lo que se puede hacer es incrementar \\(k\\) hasta que parezca que su influencia sobre \\(\\widehat{\\boldsymbol b}_{R}\\) se atenúe. Validación cruzada (CV): sea \\(\\widehat{y}_{(i),k}\\) la estimación de \\(E(y_i)\\) por medio del estimador de ridge con el parámetro \\(k\\) y usando una muestra excluyendo la i-ésima observación. La validación cruzada está definida como: \\[ CV(k) = \\sum_{i=1}^{n} (y_i - \\widehat{y}_{(i),k})^2. \\] Por lo que la selección de \\(k\\) es: \\[ k_{CV} = \\arg \\min_{k} CV(k). \\] 3.7.1 Datos de cemento Para ajustar el modelo usando el estimador de ridge podemos usar la función lmridge del paquete lmridge. Primero, ajustamos el modelo usando diferentes valores de \\(k\\): library(lmridge) K = seq(from=0,to=0.3,length.out = 100) ridge.cement = lmridge(y~., data=cement,K=K,scaling=&#39;sc&#39;) En el objeto ridge.cement tenemos las estimaciones por el estimador de ridge para \\(100\\) valores de \\(k\\) entre \\(0\\) y \\(0.3\\). Para observar como cambian las estimaciones para los diferentes valores de \\(k\\) podemos graficar la traza de ridge así (en términos de las covariables en su escala orginal): EstRidge.cement = coef(ridge.cement) plot(K,EstRidge.cement[,2],type=&#39;l&#39;,ylim=range(EstRidge.cement[,-1]),lwd=2, ylab=&#39;Estimaciones de los coeficientes&#39;,xlab=&#39;k&#39;) lines(K,EstRidge.cement[,3],col=2,lwd=2) lines(K,EstRidge.cement[,4],col=3,lwd=2) lines(K,EstRidge.cement[,5],col=4,lwd=2) abline(h=0,lty=2) Figure 3.4: Datos de cemento. Traza de ridge. Coeficiente asociado a X1 (negro), coeficiente asociado a X2 (rojo), coeficiente asociado a X3 (verde) y coeficiente asociado a X4 (azul) En la Figura 3.4 podemos observar que, cuando incrementamos \\(k\\), las estimaciones cambian rápidamente y luego parecen estabilizarse cuando \\(k\\) es grande. Además hay un cambio de signo para el coeficiente asociado a X3. También puede usarse plot(mod.r) (traza de ridge para las estimaciones de los coeficientes asociados a las covariables escaladas). La selección del \\(k\\) óptimo por medio de validación cruzada (CV) se hace de la siguiente manera: Criterios.cement = kest(ridge.cement) plot(K,Criterios.cement$CV,type=&#39;l&#39;,xlab=&#39;K&#39;,ylab=&#39;validación cruzada&#39;) Figure 3.5: Datos de cemento. Validación cruzada. K[Criterios.cement$CV==min(Criterios.cement$CV)] ## [1] 0.009090909 Aquí podemos ver que el valorde \\(k\\) que minimiza la validación cruzada es \\(0.009\\). El valor óptimo de \\(k\\) por medio de otros criterios son: Criterios.cement ## Ridge k from different Authors ## ## k values ## Minimum CV at K 0.00909 ## Minimum GCV at K 0.02424 ## Thisted (1976): 0.00581 ## LW (lm.ridge) 0.05183 ## LW (1976) 0.00797 ## HKB (1975) 0.01162 ## Dwividi &amp; Srivastava (1978): 0.00291 ## Kibria (2003) (AM) 0.28218 ## Kibria 2003 (GM): 0.07733 ## Kibria 2003 (MED): 0.01718 ## Muniz et al. 2009 (KM2): 14.84574 ## Muniz et al. 2009 (KM3): 5.32606 ## Muniz et al. 2009 (KM4): 3.59606 ## Muniz et al. 2009 (KM5): 0.27808 ## Muniz et al. 2009 (KM6): 7.80532 ## Mansson et al. 2012 (KMN8): 14.98071 ## Mansson et al. 2012 (KMN9): 0.49624 ## Mansson et al. 2012 (KMN10): 6.63342 ## Mansson et al. 2012 (KMN11): 0.15075 ## Mansson et al. 2012 (KMN12): 8.06268 ## Dorugade et al. 2010: 0.00000 ## Dorugade et al. 2014: 101.64433 La estimación con \\(K=0.0101\\) se puede obtener usando la función lmridge usando \\(K=0.0101\\) como argumento: ridge.cement2 = lmridge(y~., data=cement,K=0.0101,scaling=&#39;sc&#39;) summary(ridge.cement2) ## ## Call: ## lmridge.default(formula = y ~ ., data = cement, K = 0.0101, scaling = &quot;sc&quot;) ## ## ## Coefficients: for Ridge parameter K= 0.0101 ## Estimate Estimate (Sc) StdErr (Sc) t-value (Sc) Pr(&gt;|t|) ## Intercept 82.7052 -267.6034 306.3344 -0.8736 0.4052 ## x1 1.3146 26.7886 3.9606 6.7637 0.0001 *** ## x2 0.3059 16.4871 5.2766 3.1246 0.0124 * ## x3 -0.1295 -2.8734 3.9360 -0.7300 0.4841 ## x4 -0.3432 -19.8985 5.4000 -3.6849 0.0051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Ridge Summary ## R2 adj-R2 DF ridge F AIC BIC ## 0.97180 0.96240 3.07629 133.92403 23.27712 58.35941 ## Ridge minimum MSE= 392.3519 at K= 0.0101 ## P-value for F-test ( 3.07629 , 9.74488 ) = 3.013147e-08 ## ------------------------------------------------------------------- Vemos algunas diferencias con las estimaciones por MCO. Hay un cambio de signo en la estimación del coeficiente asociado a x3. Si embargo, no hay mucha diferencia en las estimaciones de los otros coeficientes. También podemos observar que las covariables x1, x2 y x3 ahora tienen un aporte significativo. Como era de esperarse, hay una disminución del \\(R^{2}\\), sin embargo es muy leve. 3.8 Estimador por componentes principales Considere el modelo en su forma canónica: \\[ \\boldsymbol y^{*} = \\boldsymbol Z\\boldsymbol T\\boldsymbol \\alpha+ \\boldsymbol \\varepsilon, \\] donde \\(\\boldsymbol \\alpha= \\boldsymbol T&#39;\\boldsymbol b\\), \\((\\boldsymbol Z\\boldsymbol T)&#39;\\boldsymbol Z\\boldsymbol T=\\boldsymbol \\Lambda\\), \\(\\boldsymbol \\Lambda\\) es una matriz diagonal de valores propios \\((\\lambda_1,\\ldots,\\lambda_p)\\) de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\), y \\(\\boldsymbol T= (\\boldsymbol t_{1},\\ldots,\\boldsymbol t_{p-1})\\) es la matriz ortogonal de vectores propios asociados \\(\\boldsymbol \\Lambda\\). Las columnas de \\(\\boldsymbol P= \\boldsymbol Z\\boldsymbol T= (\\boldsymbol p_{1},\\ldots,\\boldsymbol p_{p-1})\\) son un conjunto de regresores ortogonales(llamados componentes principales): \\[ \\boldsymbol p_{k} = \\sum_{j=1}^{p-1} t_{kj}\\boldsymbol z_{j}. \\] El estimador de \\(\\boldsymbol \\alpha\\) por MCO es: \\[ \\widehat{\\boldsymbol \\alpha}= (\\boldsymbol P&#39;\\boldsymbol P)^{-1}\\boldsymbol P&#39;\\boldsymbol y^{*} = \\boldsymbol \\Lambda^{-1}\\boldsymbol P&#39;\\boldsymbol y^{*}, \\] y la varianza de \\(\\widehat{\\boldsymbol \\alpha}\\) es: \\[ V(\\widehat{\\boldsymbol \\alpha}) = \\sigma^{2}(\\boldsymbol P&#39;\\boldsymbol P)^{-1} = \\sigma^{2}\\boldsymbol \\Lambda^{-1}. \\] De aquí podemos ver que valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) están asociados con la varianza de los coeficientes de regresión. Si \\(\\lambda_j=1\\) (para \\(j=1,\\ldots,p\\)), las covariables originales son ortogonales. Mientras que valores propios cercanos a cero indican problemas de multicolinealidad dado que inflan la varianza de \\(\\widehat{\\boldsymbol \\alpha}\\). Además, Note que para \\(\\widehat{\\boldsymbol b}= \\boldsymbol T\\widehat{\\boldsymbol \\alpha}\\), tenemos: \\[ V(\\widehat{\\boldsymbol b}) = V(\\boldsymbol T\\widehat{\\boldsymbol \\alpha}) =\\sigma^{2}\\boldsymbol T\\boldsymbol \\Lambda^{-1}\\boldsymbol T&#39; = \\sigma^{2} \\sum_{j=1}^{p-1}\\lambda_{j}^{-1}\\boldsymbol t_{j}\\boldsymbol t_{j}&#39;, \\] lo que implica que \\(V(\\widehat{b}_{k}) = \\sigma^{2} \\sum_{j=1}^{p-1}t_{kj}^{2}/\\lambda_{j}\\); la varianza de \\(\\widehat{b}_j\\) es una combinación lineal de los valores propios. Para combatir el problema de multicolinealidad, la idea de la regresión por componentes principales es usar un subconjunto de \\(\\boldsymbol P\\) como regresores (en vez de todos los compontes). Para esto, eliminamos los componentes principales \\((\\boldsymbol p_{r+1},\\ldots,\\boldsymbol p_{p-1})\\) asociados a los valores propios cercanos a cero \\((\\lambda_{r+1},\\ldots,\\lambda_{p-1})\\). Aquí estamos asumiendo que los valores propios están ordenados de mayor a menor, \\(\\lambda_1 \\leq \\lambda_2 \\leq \\ldots \\leq \\lambda_p\\) (como lo hace R). Esto es, \\[ \\widehat{\\boldsymbol \\alpha}_{PC} = \\boldsymbol L\\widehat{\\boldsymbol \\alpha}= \\begin{pmatrix} \\boldsymbol I_{r} &amp; \\boldsymbol 0\\\\ \\boldsymbol 0&amp; \\boldsymbol 0\\end{pmatrix} \\widehat{\\boldsymbol \\alpha}. \\] Por lo tanto \\(\\widehat{\\boldsymbol \\alpha}_{PC} = (\\overbrace{\\widehat{\\alpha}_{1},\\widehat{\\alpha}_{2},\\ldots, \\widehat{\\alpha}_{r}}^{r},\\overbrace{0, \\ldots,0}^{p-1-r})&#39;\\). En términos de \\(\\boldsymbol b\\), \\[ \\widehat{\\boldsymbol b}_{PC} = \\boldsymbol T\\widehat{\\boldsymbol \\alpha}_{PC} = \\boldsymbol T\\boldsymbol L\\boldsymbol \\Lambda^{-1}\\boldsymbol T&#39;\\boldsymbol Z&#39;\\boldsymbol y^{*}. \\] Además, \\[ V(\\widehat{\\boldsymbol b}_{PC}) = \\sigma^{2} \\boldsymbol T\\boldsymbol L\\boldsymbol \\Lambda^{-1}\\boldsymbol L&#39;\\boldsymbol T&#39; = \\sigma^{2}\\sum_{j=1}^{r} \\lambda_{j}^{-1}\\boldsymbol t_{j}\\boldsymbol t_{j}&#39;. \\] lo que implica que \\(V(\\widehat{b}_{PC,j}) = \\sigma^{2} \\sum_{k=1}^{r}t_{jk}^{2}/\\lambda_{k}\\). El sesgo de \\(\\widehat{\\boldsymbol b}_{PC}\\) está definido como: \\[ E(\\boldsymbol T\\widehat{\\boldsymbol \\alpha}_{PC}) - \\boldsymbol T\\boldsymbol \\alpha= -\\sum_{k=r+1}^{p-1}\\alpha_k\\boldsymbol t_{k}. \\] Mientras que la varianza de \\(\\widehat{\\boldsymbol b}_{PC}\\) es: \\[\\begin{equation} V(\\boldsymbol T\\widehat{\\boldsymbol \\alpha}_{PC}) = \\boldsymbol T\\left[ \\sigma^{2}\\boldsymbol L\\boldsymbol \\Lambda^{-1} \\boldsymbol L\\right] \\boldsymbol T^{-1} = \\sigma^{2} \\sum_{k=1}^{r}\\lambda_k^{-1}\\boldsymbol t_{k}\\boldsymbol t_{k}&#39; \\leq \\sigma^{2} \\sum_{k=1}^{p-1}\\lambda_k^{-1}\\boldsymbol t_{k}\\boldsymbol t_{k}&#39; = \\sigma^{2}(\\boldsymbol Z&#39;\\boldsymbol Z)^{-1}. \\tag{3.2} \\end{equation}\\] Por lo tanto, al eliminar componentes principales se aumenta el sesgo, pero se disminuye la varianza de \\(\\widehat{\\boldsymbol b}_{PC}\\). Note en (3.2) que la diferencia en varianzas con respecto al estimador de MCO de \\(\\boldsymbol b\\): \\[ \\sigma^{2}\\sum_{k=r+1}^{p-1}\\lambda_k^{-1} \\boldsymbol t_{k}\\boldsymbol t_{k}&#39;, \\] será mayor si las componentes principales excluidas están asociadas a valores propios pequeños. 3.8.1 Datos de cemento Antes de ajustar el modelo vamos a escalar las variables escalar &lt;- function(x) {(x-mean(x)) / sqrt(sum((x-mean(x))^2))} X = as.matrix(cement[,1:4]) y.e = escalar(cement$y) Z = apply(cement[,1:4],2,escalar) A partir de las variables escaladas podemos calcular los vectores y valores propios: T.mat = eigen(t(Z)%*%Z)$vectors lambda = eigen(t(Z)%*%Z)$values lambda ## [1] 2.235704035 1.576066070 0.186606149 0.001623746 Aquí podemos observar que un valor propio es cercano a cero, por lo que lo podemos eliminarlo para remediar el problema de multicolinealidad: P = Z%*%T.mat PCR.cement = lm(y.e~P[,-4]-1) summary(PCR.cement) ## ## Call: ## lm(formula = y.e ~ P[, -4] - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.058393 -0.036851 0.006961 0.022836 0.073963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## P[, -4]1 -0.656958 0.028271 -23.238 4.93e-10 *** ## P[, -4]2 -0.008309 0.033671 -0.247 0.8101 ## P[, -4]3 0.302770 0.097856 3.094 0.0114 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.04227 on 10 degrees of freedom ## Multiple R-squared: 0.9821, Adjusted R-squared: 0.9768 ## F-statistic: 183.2 on 3 and 10 DF, p-value: 4.895e-09 Las estimaciones de los coeficientes \\((\\boldsymbol b)\\) para las variables escaladas: beta.CP =T.mat%*%c(PCR.cement$coefficients,0) beta.CP ## [,1] ## [1,] 0.51297502 ## [2,] 0.27868114 ## [3,] -0.06078483 ## [4,] -0.42288461 La matriz de varianza de \\(\\boldsymbol b_{PC}\\) es: sigma2.pc = sum(PCR.cement$residuals^2)/(13-4) Var.b = sigma2.pc*T.mat%*%diag(c(1/lambda[1:3],0))%*%t(T.mat) Var.b ## [,1] [,2] [,3] [,4] ## [1,] 0.005382414 -0.0022868445 0.004028698 -0.0013467849 ## [2,] -0.002286844 0.0015500408 -0.002015159 0.0001440783 ## [3,] 0.004028698 -0.0020151593 0.004925579 -0.0014780352 ## [4,] -0.001346785 0.0001440783 -0.001478035 0.0009294415 "],["selección-de-variables.html", "Capítulo 4 Selección de variables 4.1 Ejemplos 4.2 Problema de selección de variables 4.3 Métodos para la selección de variables 4.4 Comparación de los modelos", " Capítulo 4 Selección de variables 4.1 Ejemplos 4.1.1 Unidad quirúrgica Una unidad quirúrgica de un hospital está interesada en predecir la supervivencia de los pacientes sometidos a un tipo particular de operación hepática. Se dispuso de una selección aleatoria de \\(108\\) pacientes para el análisis. De cada registro del paciente, se extrajo la siguiente información de la evaluación preoperatoria: bcs: coagulación sanguínea. pindex: índice de pronóstico. enzyme: función enzimática. liver_test: función hepática. age: edad. gender: genero (0 = masculino, 1 = femenino). alc_mod: historial de consumo de alcohol (0 = Ninguno, 1 = Moderado). alc_heavy: &amp; historial de consumo de alcohol (0 = Ninguno, 1 = Fuerte). y: tiempo de supervivencia. El objetivo del estudio es determinar los factores que influyen sobre el tiempo de supervivencia (que se determinó posteriormente) en función de las demás variables. El modelo propuesto es el siguiente: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0}+\\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2}+ \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{liver}_{i}\\beta_{4} + \\mbox{age}_{i}\\beta_{5} + \\mbox{gender}_{i}\\beta_{6}+ \\\\ &amp; \\mbox{alc_mod}_{i}\\beta_{7} + \\mbox{alc_heavy}_{i}\\beta_{8} + \\varepsilon_{i} \\end{split} \\nonumber \\end{equation}\\] El ajuste del modelo es: library(olsrr) data(surgical) mod.surgical.completo = lm(log(y)~.,data=surgical) summary(mod.surgical.completo) ## ## Call: ## lm(formula = log(y) ~ ., data = surgical) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35555 -0.13849 -0.05179 0.14912 0.46349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.050949 0.251741 16.092 &lt; 2e-16 *** ## bcs 0.068551 0.025420 2.697 0.00982 ** ## pindex 0.013459 0.001947 6.913 1.37e-08 *** ## enzyme_test 0.014948 0.001809 8.261 1.44e-10 *** ## liver_test 0.007931 0.046706 0.170 0.86592 ## age -0.003567 0.002751 -1.296 0.20145 ## gender 0.084151 0.060746 1.385 0.17279 ## alc_mod 0.057313 0.067480 0.849 0.40019 ## alc_heavy 0.388190 0.088374 4.393 6.73e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2093 on 45 degrees of freedom ## Multiple R-squared: 0.8461, Adjusted R-squared: 0.8187 ## F-statistic: 30.93 on 8 and 45 DF, p-value: 7.823e-16 4.1.2 Grasa corporal La medición de la grasa corporal es un proceso complejo. Dado que los músculos y los huesos son más densos, el calculo de % de grasa corporal se basa, entre otros aspectos, en la medición de la densidad corporal la cuál requiere sumergir a las personas en el agua. Por esta razón se quiere buscar un método más sencillo para determinar el % de grasa corporal. Para esto, se registraron la edad, el peso, la altura y \\(10\\) medidas de la circunferencia corporal de \\(252\\) hombres. De igual forma, a cada uno de estos hombres se les midió el % de grasa corporal de forma precisa (usando la ecuación de Brozek, medición a partir de la densidad). Cómo variable respuesta se utiliza la medición por el método de Brozek, y las posibles covariables son: age: edad (en años). weight: peso (en libras). height: altura (en pulgadas). neck: circunferencia del cuello (en centímetros). chest: circunferencia del pecho (en centímetros). abdom: circunferencia del abdomen (en centímetros). hip: circunferencia de la cadera (en centímetros). thigh:circunferencia del muslo (en centímetros). knee:circunferencia de la rodilla (en centímetros). ankle:circunferencia del tobillo (en centímetros). biceps: circunferencia del bíceps extendido (en centímetros). forearm: circunferencia del antebrazo (en centímetros). wrist: circunferencia de la muñeca (en centímetros). El modelo propuesto es el siguiente: \\[ \\mbox{brozek}_i = \\beta_{0} + \\mbox{age}_i\\beta_1+ \\mbox{weight}_i\\beta_2 + \\ldots + \\mbox{wrist}_i\\beta_{13} + \\varepsilon_i. \\] El ajuste del modelo es: library(faraway) data(fat) mod.fat &lt;- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat) summary(mod.fat) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + chest + ## abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, ## data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.264 -2.572 -0.097 2.898 9.327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -15.29255 16.06992 -0.952 0.34225 ## age 0.05679 0.02996 1.895 0.05929 . ## weight -0.08031 0.04958 -1.620 0.10660 ## height -0.06460 0.08893 -0.726 0.46830 ## neck -0.43754 0.21533 -2.032 0.04327 * ## chest -0.02360 0.09184 -0.257 0.79740 ## abdom 0.88543 0.08008 11.057 &lt; 2e-16 *** ## hip -0.19842 0.13516 -1.468 0.14341 ## thigh 0.23190 0.13372 1.734 0.08418 . ## knee -0.01168 0.22414 -0.052 0.95850 ## ankle 0.16354 0.20514 0.797 0.42614 ## biceps 0.15280 0.15851 0.964 0.33605 ## forearm 0.43049 0.18445 2.334 0.02044 * ## wrist -1.47654 0.49552 -2.980 0.00318 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.988 on 238 degrees of freedom ## Multiple R-squared: 0.749, Adjusted R-squared: 0.7353 ## F-statistic: 54.63 on 13 and 238 DF, p-value: &lt; 2.2e-16 4.2 Problema de selección de variables En problemas de regresión se tiene un conjunto grande de potenciales covariables. Si ajustamos un modelo considerandolas todas podemos estar incluyendo covariables que son irrelevante. Por el otro lado, si no las incluimos todas es posible que estemos omitiendo covariables importantes. En ambos casos hay consecuencias negativas. Para illustrar esto, considere el siguiente modelo: \\[\\begin{equation} \\begin{split} y_{i} &amp;= \\beta_{0} + \\sum_{j=1}^{p-1}\\beta_{j}x_{ij} + \\varepsilon_{i} \\\\ &amp;= \\beta_{0} + \\sum_{j=1}^{r}\\beta_{j}x_{ij} + \\sum_{j=r+1}^{p-1}\\beta_{j}x_{ij} + \\varepsilon_{i} \\\\ &amp;= \\boldsymbol x_{1i}&#39;\\boldsymbol \\beta_1 + \\boldsymbol x_{2i}&#39;\\boldsymbol \\beta_2 + \\varepsilon_i, \\end{split} \\tag{4.1} \\end{equation}\\] donde \\(\\boldsymbol x_{1i} = (1,x_{1i},x_{2i},\\ldots,x_{ri})\\), \\(\\boldsymbol x_{2i} = (x_{r+1,i},x_{r+2,i},\\ldots,x_{p-1,i})\\), \\(\\boldsymbol \\beta_1 = (\\beta_0,\\beta_1,\\beta_2,\\ldots,\\beta_r)&#39;\\), \\(\\boldsymbol \\beta_2 = (\\beta_{r+1},\\beta_{r+2},\\ldots,\\beta_{p-1})&#39;\\), y \\(\\varepsilon_i \\sim N(0,\\sigma^{2})\\). Es decir, se hace una partición de las covariables y los coeficientes de regressión en dos componentes. En forma matricial, el modelo es: \\[ \\boldsymbol y= \\boldsymbol X_{1}\\boldsymbol \\beta_{1} + \\boldsymbol X_{2}\\boldsymbol \\beta_{2}+ \\boldsymbol \\varepsilon, \\] donde \\(\\boldsymbol X_{1}\\) es una matriz \\(n \\times r\\) con la \\(i\\)-ésima fila igual a \\(\\boldsymbol x_{1i}\\) y \\(\\boldsymbol X_{2}\\) es una matriz \\(n \\times (p-r-1)\\) con la \\(i\\)-ésima fila igual a \\(\\boldsymbol x_{2i}\\). 4.2.1 ¿Qué pasa si ignoramos covariables importantes? Ahora, considere que el modelo de regresión real es (4.1), pero decidimos estimar: \\[ y_{i} = \\boldsymbol x_{1i}&#39;\\boldsymbol \\beta_1 + \\varepsilon_i. \\] Por lo tanto, estamos omitiendo las covariables \\(\\boldsymbol x_{2i}\\) del modelo (puesto que \\(\\boldsymbol \\beta_2 \\neq 0\\)). El estimador por MCO de \\(\\boldsymbol \\beta_1\\) es: \\[ \\widehat{\\boldsymbol \\beta}_{1} = (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol y. \\] De aquí tenemos que \\(E(\\widehat{\\boldsymbol \\beta}_{1}) = \\boldsymbol \\beta_{1} + (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\boldsymbol \\beta_{2}\\). Es decir que \\(\\widehat{\\boldsymbol \\beta}_{1}\\) es un estimador sesgado, a menos que \\(\\boldsymbol X_{1}&#39;\\boldsymbol X_{2} = \\boldsymbol 0\\) (las columnas de \\(X_{1}\\) son ortogonales a las columnas de \\(X_{2}\\)). De igual forma, las predicciones también serán sesgadas. La predicción en el punto \\(\\boldsymbol x_{01}\\) es: \\[ \\widehat{y}_{0} = \\boldsymbol x_{01}&#39;\\widehat{\\boldsymbol \\beta}_{1}. \\] Su valor esperado es: \\[ E(\\widehat{y}_{0}) = \\boldsymbol x_{01}&#39;\\boldsymbol \\beta_{1} + \\boldsymbol x_{01}&#39;(\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\boldsymbol \\beta_{2} \\neq \\boldsymbol x_{01}&#39;\\beta_{1} + \\boldsymbol x_{02}&#39;\\beta_{2}. \\] Por lo tanto, si omitimos variables relevantes obtenemos sesgo en las estimaciones. 4.2.2 ¿Que pasa si incluimos covariables irrelevantes? Ahora, consideremos el caso en que \\(\\boldsymbol \\beta_2=0\\), es decir, las covariables \\(\\boldsymbol x_{2}\\) no tienen un aporte significativo en el modelo. Pero decidimos estimar el modelo completo. En este caso, el estimador de \\(\\boldsymbol \\beta\\) es: \\[ \\widehat{\\boldsymbol \\beta}= (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol y= \\begin{pmatrix} \\boldsymbol X_{1}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{1}\\boldsymbol X_{2} \\\\ \\boldsymbol X_{2}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{2}&#39;\\boldsymbol X_{2} \\end{pmatrix}^{-1} \\begin{pmatrix} \\boldsymbol X_{1}&#39; \\\\ \\boldsymbol X_{2}&#39; \\end{pmatrix}\\boldsymbol y. \\] El Valor esperado de \\(\\widehat{\\boldsymbol \\beta}\\) es: \\[\\begin{equation} \\begin{split} E(\\widehat{\\boldsymbol \\beta}) =&amp; (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;E(\\boldsymbol y) = (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol X_{1}\\boldsymbol \\beta_1 \\\\ = &amp; (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;(\\boldsymbol X_{1} \\ \\boldsymbol X_{2}) \\begin{pmatrix} \\boldsymbol \\beta_1 \\\\ \\boldsymbol 0 \\end{pmatrix} = \\begin{pmatrix} \\boldsymbol \\beta_1 \\\\ \\boldsymbol 0 \\end{pmatrix}. \\end{split} \\nonumber \\end{equation}\\] Es decir que \\(\\widehat{\\boldsymbol \\beta}\\) es un estimador insesgado. La varianza de \\(\\widehat{\\boldsymbol \\beta}\\) es: \\[\\begin{equation} \\begin{split} V(\\widehat{\\boldsymbol \\beta}) =&amp; \\sigma^{2}(\\boldsymbol X&#39;\\boldsymbol X)^{-1} = \\sigma^{2}\\begin{pmatrix} \\boldsymbol X_{1}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{1}\\boldsymbol X_{2} \\\\ \\boldsymbol X_{2}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{2}&#39;\\boldsymbol X_{2} \\end{pmatrix}^{-1} \\\\ =&amp; \\sigma^{2} \\begin{pmatrix} (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1} + \\boldsymbol L\\boldsymbol M\\boldsymbol L&amp; - \\boldsymbol L\\boldsymbol M\\\\ -\\boldsymbol M\\boldsymbol L&#39; &amp; \\boldsymbol M \\end{pmatrix}, \\end{split} \\nonumber \\end{equation}\\] donde \\(\\boldsymbol L= (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\) y \\(\\boldsymbol M= \\boldsymbol X_{2}&#39;(\\boldsymbol I- \\boldsymbol H_{1})\\boldsymbol X_{2}\\). Particularmente, para \\(\\widehat{\\boldsymbol \\beta}_1\\) tenemos que: \\[ V(\\widehat{\\boldsymbol \\beta}_{1}) = \\sigma^{2} \\left[ (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1} + \\boldsymbol L\\boldsymbol M\\boldsymbol L\\right]. \\] Dado que \\(\\boldsymbol M\\) (y por lo tanto \\(\\boldsymbol L\\boldsymbol M\\boldsymbol L\\)) es positiva-definida, la varianza de \\(\\widehat{\\boldsymbol \\beta}_{1}\\) se infla al incluir las covariables irrelevantes al modelo. La única excepción es cuando \\(\\boldsymbol X_{1}\\) y \\(\\boldsymbol X_{2}\\) son ortogonales (\\(\\boldsymbol X_{1}&#39;\\boldsymbol X_{2} = \\boldsymbol 0\\)). De igual forma, las predicciones en el punto \\(\\boldsymbol x_{0}&#39; = (\\boldsymbol x_{01}&#39; \\ \\boldsymbol x_{02}&#39;)\\) son insesgadas: \\[ E(\\widehat{y}_{0}) = E(\\boldsymbol x_{0}&#39;\\widehat{\\boldsymbol \\beta}) = (\\boldsymbol x_{01}&#39; \\ \\boldsymbol x_{02}&#39;)\\begin{pmatrix} \\boldsymbol \\beta_{1} \\\\ \\boldsymbol 0 \\end{pmatrix} = \\boldsymbol x_{01}&#39;\\boldsymbol \\beta_{1}. \\] Pero su varianza también se infla debido a incluir las covariables irrelevantes: \\[ V(\\widehat{y}_{0}) = \\sigma^{2} \\boldsymbol x_{0}&#39;(\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol x_{0}. \\] En conclusión: Cuando omitimos covariables relevantes, obtenemos sesgos en las estimaciones. Cuando incluimos covariables irrelevantes, se inflan las varianzas de las estimaciones. Adicionalmente, incluir más covariables puede llevar a problemas de multicolinealidad. 4.3 Métodos para la selección de variables Si tenemos \\((p-1)\\) covariables, entonces tenemos \\((p-1)^2\\) potenciales modelos. Por lo que podemos ajustar todos los posibles modelos y hacer una comparación entre ellos usando algunos criterios de decisión. Existen varios criterios para determinar que modelo es “mejor” que otro y este debe escogerse teniendo en cuenta cuál es el objetivo que se tiene al ajustar el modelo (descripción de la relación, predicción, control, etc.). Algunos de estos críterios son: Coeficiente de determinación (\\(R^{2}\\) y \\(R^{2}_{adj}\\)). Estadístico \\(C_{p}\\) de Mallows. Estadístico PRESS y el \\(R^{2}\\) de predicción. Criterios de información (AIC y BIC). 4.3.1 Coeficiente de determinación Esté indicador está definido como: \\[ R^{2} = \\frac{SS_{\\mbox{reg}}}{SS_{\\mbox{T}}} = 1 - \\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}}. \\] El \\(R^{2}\\) cuantifica la cantidad de variabilidad de la variable respuesta que es explicada por el modelo. Se tiene que \\(0 \\leq R^{2} \\leq 1\\). Valores más cercanos a \\(1\\) implican que el modelo explica gran parte de la variabilidad de \\(y\\). Hay que tener en cuenta que el \\(R^{2}\\) siempre crece a medida que se adicionan más covariables al modelo. Por lo tanto, se puede puede agregar regresores hasta el punto en que una covariable adicional no propociona un aumento considerable en el \\(R^{2}\\). 4.3.2 Coeficiente de determinación ajustado Para evitar el incoviente del \\(R^{2}\\), se puede utlizar el el coeficiente de determinación ajustado definido como: \\[ R^{2}_{adj} = 1 - \\frac{n-1}{n-p}\\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}} = 1- \\frac{MS_{\\mbox{res}}}{SS_{\\mbox{T}}/(n-1)} = 1- \\frac{n-1}{n-p}(1-R^{2}). \\] El \\(R^{2}_{adj}\\) no necesariamente aumenta al adicionar nuevos términos al modelo. Este solo aumenta si hay una disminución del \\(MS_{\\mbox{res}}\\). 4.3.3 C\\(_p\\) de Mallows Mallows propone un criterio basado en el error cuadrático medio (ECM) de \\(\\widehat{y}_i\\), esto es: \\[ E[\\widehat{y}_{i}- E(y_{i})]^2 = [E(y_{i}) - E(\\widehat{y}_{i})]^2 + V(\\widehat{y}_{i}), \\] donde \\(E(y_{i})\\) es el valor esperado de la respuesta (‘modelo real’), y \\(E(\\widehat{y}_{i})\\) es el valor esperado de la respuesta basado en el modelo propuesto (basado en \\(p\\) covariables). El ECM total estandarizado está definido como: \\[\\begin{equation} \\begin{split} \\Gamma_{p} =&amp; \\frac{1}{\\sigma^{2}}\\left\\{\\sum_{i=1}^{n}[E(y_{i}) - E(\\widehat{y}_{i})]^2 + \\sum_{i=1}^{n} V(\\widehat{y}_{i}) \\right\\} \\\\ =&amp; \\frac{1}{\\sigma^{2}}\\left\\{SS_{B}(p) + \\sum_{i=1}^{n} V(\\widehat{y}_{i}) \\right\\} = \\frac{1}{\\sigma^{2}}\\left\\{SS_{B}(p) + p\\sigma^{2} \\right\\} \\\\ =&amp; \\frac{1}{\\sigma^{2}}\\left\\{ E[SS_{\\mbox{res}}(p)] - (n-p)\\sigma^{2} + p\\sigma^{2} \\right\\} \\\\ =&amp; \\frac{E[SS_{\\mbox{res}}(p)]}{\\sigma^{2}} - n + 2p. \\end{split} \\nonumber \\end{equation}\\] Reemplazando \\(E[SS_{\\mbox{res}}(p)]\\) por \\(SS_{\\mbox{res}}(p)\\), y asumiendo que \\(MS_{\\mbox{res}}(p^{*})\\) (calculado usando el modelo completo) es un buen estimador de \\(\\sigma^{2}\\): \\[ C_{p} = \\frac{SS_{\\mbox{res}}(p)}{MS_{\\mbox{res}}(p^{*})} - n + 2p. \\] Por lo tanto, para el modelo completo \\(C_{p} = p^{*}\\). Si \\(E[SS_{\\mbox{res}}(p)] = (n-p)\\sigma^{2}\\) (asumiendo que \\(SS_{B}(p)=0\\)), tenemos que: \\[ E[C_{p}| \\mbox{Sesgo}=0] = \\frac{(n-p)\\sigma^{2}}{\\sigma^{2}} - n +2p = p. \\] Si el modelo propuesto es insesgado se espera que el \\(C_p\\) esté cercano a \\(p\\). Aunque se espera que el \\(C_p=p\\), es deseable que \\(C_p &lt; p\\). Por lo tanto, modelos con valores pequeños de \\(C_p\\) son mejores. 4.3.4 Estadístico PRESS El estadístico PRESS (prediction error sum of squares) está definido como: \\[ \\mbox{PRESS} = \\sum_{i=1}^{n} (y_{i} - \\widehat{y}_{(i)})^{2} = \\sum_{i=1}^{n} \\left( \\frac{e_{i}}{1-h_{ii}} \\right)^{2}. \\] Para comparar modelos, menor valor del PRESS indica que el modelo es mejor para hacer predicciones. A partir del PRESS se puede calcular el \\(R^{2}\\) de predicción: \\[ R^{2}_{pred} = 1 - \\frac{PRESS}{SST}. \\] Basado en este criterio, mayor es el valor de \\(R^{2}_{pred}\\) mejor es el modelo para hacer predicciones. La ventaja del PRESS y \\(R_{pred}^{2}\\) es que evitan el sobreajuste dado que se calculan utilizando observaciones no incluidas en la estimación del modelo. 4.3.5 Criterios de información La idea es comparar modelos estimados teniendo en cuenta la bondad de ajuste del modelo (verosimilitud, \\(L\\)) y su complejidad (número de parámetros). El criterio de información de Akaike está definido como: \\[ \\mbox{AIC} = -2\\log (L) + 2p. \\] El criterio de información bayesiano (o de Schwarz - SBC): \\[ \\mbox{BIC} = -2\\log (L) + p\\log n. \\] Es preferible modelos con valores menores de AIC o BIC. Dado que la penalización del BIC es mayor (si \\(n &gt; 7\\)), este indicador tiende a preferir modelos con menor número de covariables. Recordemos que la log-verosimilitud es: \\[ \\log L(\\boldsymbol \\beta,\\sigma^{2}) = - \\frac{n}{2}\\log (2\\pi) - n\\log(\\sigma) - \\frac{1}{2\\sigma^{2}}(\\boldsymbol y- \\boldsymbol X\\boldsymbol \\beta)&#39;(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta). \\] El estimador por máxima verosimilitud de \\(\\sigma^{2}\\) es \\(\\widehat{\\sigma}=SS_{\\mbox{res}}/n\\). Por lo tanto, el máximo valor de la log-verosimilitud es: \\[ \\log L(\\widehat{\\boldsymbol \\beta},\\widehat{\\sigma}^{2}) = -\\frac{n}{2}\\log (2\\pi) - \\frac{n}{2}\\log\\widehat{\\sigma}^{2} - \\frac{1}{2\\widehat{\\sigma}^{2}}SS_{\\mbox{res}}= -\\frac{n}{2}\\log (SS_{\\mbox{res}}/n) + \\mbox{constante}. \\] Por lo tanto: \\[ AIC \\propto n\\log(SS_{\\mbox{res}}/n) + 2p \\mbox{ y } BIC \\propto n\\log(SS_{\\mbox{res}}/n) + p\\log n. \\] Hay varias adaptaciones de estos criterios de información definiendo diferentes penalizaciones. 4.4 Comparación de los modelos 4.4.1 Todos los posibles modelos Con la función ols_step_all_possible() de la librería olsrr es posible ajustar todos posibles modelos y determinar el mejor bajo diferentes criterios. Otra alternativa es la función regsubsets() de la librería leaps. Está función es más rápida (se basa en un algoritmo más eficiente), pero no es user-friendly. 4.4.1.1 Datos de unidad quirúrgica A través de la función ols_step_all_possible podemos ajustar los \\(255\\) modelos que se pueden ajustar usando las ocho posibles covariables: surgical.all.mods=ols_step_all_possible(mod.surgical.completo) Además de ajustar los modelos, se calculan varios criterios (\\(R^{2}\\),\\(R^{2}_{adj}\\), \\(R^{2}_{pred}\\),AIC,BIC,…) para cada uno de ellos. Puesto que son muchos modelos, podemos organizar los resultados de tal forma que obtengamos los mejores modelos basándonos en cada uno de los criterios. Por ejemplo, los 5 mejores ajustes según el \\(R^{2}_{adj}\\) son: R2adj.order = order(surgical.all.mods$adjr,decreasing = T) as.data.frame(surgical.all.mods)[R2adj.order[1:5],c(2:8,10)] ## n predictors rsquare adjr predrsq cp aic sbc ## 226 6 bcs pindex enzyme_test age gender alc_heavy 0.8434664 0.8234834 0.7836037 5.772458 -8.612898 7.298974 ## 251 7 bcs pindex enzyme_test age gender alc_mod alc_heavy 0.8460095 0.8225761 0.7806940 7.028837 -7.497389 10.403468 ## 171 5 bcs pindex enzyme_test gender alc_heavy 0.8374622 0.8205312 0.7827597 5.528174 -8.580332 5.342556 ## 248 7 bcs pindex enzyme_test liver_test age gender alc_heavy 0.8436412 0.8198474 0.7749807 7.721367 -6.673207 11.227649 ## 169 5 bcs pindex enzyme_test age alc_heavy 0.8358522 0.8187535 0.7862369 5.998959 -8.048073 5.874815 Basándonos en este criterio el mejor ajuste se obtiene considerando las covariables bcs, pindex, enzyme_test, age, gender, y alc_heavy. Es decir, eliminando las covariables función hepática y consumo de alcohol moderado. Note que no todos los demás críterios sugieren el mismo modelo. Si eliminamos las covariables gender obtenemos un modelo con un \\(R^{2}_{pred}\\) más alto. Ahora, si nos apoyamos en el AIC, los mejores 5 ajustes son: AIC.order = order(surgical.all.mods$aic,decreasing = F) as.data.frame(surgical.all.mods)[AIC.order[1:5],c(2:8,10)] ## n predictors rsquare adjr predrsq cp aic sbc ## 226 6 bcs pindex enzyme_test age gender alc_heavy 0.8434664 0.8234834 0.7836037 5.772458 -8.612898 7.298974 ## 171 5 bcs pindex enzyme_test gender alc_heavy 0.8374622 0.8205312 0.7827597 5.528174 -8.580332 5.342556 ## 97 4 bcs pindex enzyme_test alc_heavy 0.8299187 0.8160345 0.7862922 5.733992 -8.130569 3.803335 ## 169 5 bcs pindex enzyme_test age alc_heavy 0.8358522 0.8187535 0.7862369 5.998959 -8.048073 5.874815 ## 251 7 bcs pindex enzyme_test age gender alc_mod alc_heavy 0.8460095 0.8225761 0.7806940 7.028837 -7.497389 10.403468 Con este críterio se escoge el mismo modelo que con el \\(R^{2}_{adj}\\). Sin embargo, podemos observar que el BIC sugiere eliminar la covariable asociada a la edad. En la Figura ?? muestra los \\(R^{2}\\),\\(R^{2}_{adj}\\), \\(R^{2}_{pred}\\),C\\(_p\\), AIC y BIC (SBC) para todos los posibles ajustes. Note que, dentro de cada subgrupo de modelos (determinado por el número de covariables), los criterios eligen los modelos en el mismo orden. La diferencia está en el número de covariables a elegir. Generalmente, el BIC prefiere modelos más parsimoniosos. Esto no ocurre con criterios de validación cruzada, como el PRESS o \\(R^{2}_{pred}\\). plot(surgical.all.mods) Figure 4.1: Valores de los criterios de selección calculados para cada uno de todos los posibles modelos. Figure 4.2: Valores de los criterios de selección calculados para cada uno de todos los posibles modelos. Teniendo en cuenta esto, con la función ols_step_best_subset() selecciona el mejor modelo para cada subconjunto de número de covariables basándose en los diferentes criterios: ols_step_best_subset(mod.surgical.completo) ## Best Subsets Regression ## ----------------------------------------------------------------------------- ## Model Index Predictors ## ----------------------------------------------------------------------------- ## 1 enzyme_test ## 2 pindex enzyme_test ## 3 pindex enzyme_test alc_heavy ## 4 bcs pindex enzyme_test alc_heavy ## 5 bcs pindex enzyme_test gender alc_heavy ## 6 bcs pindex enzyme_test age gender alc_heavy ## 7 bcs pindex enzyme_test age gender alc_mod alc_heavy ## 8 bcs pindex enzyme_test liver_test age gender alc_mod alc_heavy ## ----------------------------------------------------------------------------- ## ## Subsets Regression Summary ## -------------------------------------------------------------------------------------------------------------------------------- ## Adj. Pred ## Model R-Square R-Square R-Square C(p) AIC SBIC SBC MSEP FPE HSP APC ## -------------------------------------------------------------------------------------------------------------------------------- ## 1 0.4273 0.4162 0.3496 117.4783 51.4343 -105.4395 57.4013 7.6160 0.1463 0.0028 0.6168 ## 2 0.6632 0.6500 0.6044 50.4918 24.7668 -131.5971 32.7228 4.5684 0.0893 0.0017 0.3765 ## 3 0.7780 0.7647 0.7291 18.9015 4.2432 -150.4023 14.1881 3.0718 0.0610 0.0012 0.2575 ## 4 0.8299 0.8160 0.7863 5.7340 -8.1306 -160.5329 3.8033 2.4030 0.0486 9e-04 0.2048 ## 5 0.8375 0.8205 0.7828 5.5282 -8.5803 -160.2288 5.3426 2.3453 0.0482 9e-04 0.2032 ## 6 0.8435 0.8235 0.7836 5.7725 -8.6129 -159.4064 7.2990 2.3077 0.0482 9e-04 0.2032 ## 7 0.8460 0.8226 0.7807 7.0288 -7.4974 -157.6344 10.4035 2.3207 0.0492 0.0010 0.2076 ## 8 0.8461 0.8187 0.7711 9.0000 -5.5320 -155.2573 14.3579 2.3719 0.0511 0.0010 0.2154 ## -------------------------------------------------------------------------------------------------------------------------------- ## AIC: Akaike Information Criteria ## SBIC: Sawa&#39;s Bayesian Information Criteria ## SBC: Schwarz Bayesian Criteria ## MSEP: Estimated error of prediction, assuming multivariate normality ## FPE: Final Prediction Error ## HSP: Hocking&#39;s Sp ## APC: Amemiya Prediction Criteria A partir de estos resultados, y con la ayuda de expertos en el tema, se puede hacer una selección del mejor modelo para hacer las predicciónes. 4.4.2 Algorítmos de selección Para el proceso de selección, la mejor opción es evaluar todos los posibles modelos. Sin embargo, en la presencia de muchas posibles covariables este proceso puede requerir una carga computacional muy alta. Por esta razón, se han desarrollado varios algoritmos para evaluar solo un subconjunto de modelos agregando o eliminando covariables una a la vez. 4.4.2.1 Selección hacia delante (forward selection) Este algoritmo parte del modelo sin ninguna covariable (es decir, solo el intercepto) y el ajuste ``óptimo’’ se encuentra ingresando covariables una a la vez basándose en algún criterio (por ejemplo AIC). La primera covariable se escoge luego de ajustar los \\((p-1)\\) modelos simples con cada uno de los regresores. Por ejemplo, seleccionado la covariable que proporciona el mejor AIC. Luego se ajustan los modelos combinando la covariable previamente seleccionada con cada una de los restantes \\((p-2)\\) regresores. Si el mejor ajuste con dos covariables proporcina un menor AIC que en el paso anterior, continuamos seleccionando la tercer covariable de la misma forma. El algoritmo continua seleccionando covariables hasta que se satisface un criterio de parada (por ejemplo, hasta que el AIC aumente). 4.4.2.2 Selección hacia atrás (backward selection) Aquí se empieza evaluando el modelo con todas las covariables candidatas y se van eliminando covariables una a una hasta que un criterio de parada se satisface (por ejemplo, hasta que el AIC aumenta). 4.4.2.3 Selección por segmentos (stepwise selection) Aquí se siguen los mismos pasos que la selección hacia delante. Pero en cada paso se evalúan de nuevo los candidatos que ya habían ingresado en el modelo. Por lo tanto, una covariable que ya esté en el modelo puede ser eliminada en algún paso posterior. 4.4.2.4 Unidad quirúrgica Consideremos el modelo anterior adicionando las interacciones de las covariables continuas con las categóricas: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0}+\\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2}+ \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{liver}_{i}\\beta_{4} + \\mbox{age}_{i}\\beta_{5} + \\mbox{gender}_{i}\\beta_{6}+ \\mbox{alc_mod}_{i}\\beta_{7} + \\\\ &amp; \\mbox{alc_heavy}_{i}\\beta_{8} \\mbox{bcs}_{i}\\mbox{gender}_{i}\\beta_{9} + \\mbox{pindex}_{i}\\mbox{gender}_{i}\\beta_{10} + \\mbox{enzyme}_{i}\\mbox{gender}_{i}\\beta_{11} + \\mbox{liver}_{i}\\mbox{gender}_{i}\\beta_{12} + \\\\ &amp; \\mbox{age}_{i}\\mbox{gender}_{i}\\beta_{13} + \\mbox{bcs}_{i}\\mbox{alc_mod}_{i}\\beta_{14} + \\mbox{pindex}_{i}\\mbox{alc_mod}_{i}\\beta_{15} + \\mbox{enzyme}_{i}\\mbox{alc_mod}_{i}\\beta_{16} + \\\\ &amp; \\mbox{liver}_{i}\\mbox{alc_mod}_{i}\\beta_{17} + \\mbox{age}_{i}\\mbox{alc_mod}_{i}\\beta_{18} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{19} + \\mbox{pindex}_{i}\\mbox{alc_heavy}_{i}\\beta_{20} + \\\\ &amp; \\mbox{enzyme}_{i}\\mbox{alc_heavy}_{i}\\beta_{21} + \\mbox{liver}_{i}\\mbox{alc_heavy}_{i}\\beta_{22} + \\mbox{age}_{i}\\mbox{alc_heavy}_{i}\\beta_{23} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] En este caso tenemos \\(2^{23}=8&#39;388,608\\) posibles modelos. Lo que hace que sea difícil ajustarlos todos (aunque es posible usando la librería leaps). Por lo tanto, vamos a utilizar los algortimos de selección. Selección hacia delante. Podemos utilizar la función ols_step_forward_aic de la librería olsrr: mod.surgical.completo2 = lm(log(y)~bcs*gender+pindex*gender+enzyme_test*gender+liver_test*gender+age*gender+ bcs*alc_mod+pindex*alc_mod+enzyme_test*alc_mod+liver_test*alc_mod+age*alc_mod+bcs*alc_heavy+pindex*alc_heavy+enzyme_test*alc_heavy+liver_test*alc_heavy+age*alc_heavy,data=surgical) ols_step_forward_aic(mod.surgical.completo2,details = F) ## ## Selection Summary ## ------------------------------------------------------------------------ ## Variable AIC Sum Sq RSS R-Sq Adj. R-Sq ## ------------------------------------------------------------------------ ## enzyme_test 51.434 5.471 7.334 0.42725 0.41624 ## pindex 24.767 8.492 4.313 0.66318 0.64997 ## bcs:alc_heavy -3.014 10.320 2.485 0.80596 0.79432 ## bcs -9.430 10.678 2.126 0.83396 0.82041 ## gender:pindex -10.781 10.806 1.998 0.84395 0.82770 ## gender:enzyme_test -19.676 11.171 1.633 0.87246 0.85618 ## gender -23.040 11.326 1.479 0.88452 0.86695 ## age -24.081 11.407 1.398 0.89085 0.87144 ## ------------------------------------------------------------------------ Con el argumento details = T se puede ver la selección con más detalle. Usando este algoritmo el modelo óptimo es: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{age}_{i}\\beta_{4} + \\mbox{gender}_{i}\\beta_{5} + \\\\ &amp;\\mbox{gender}_{i}\\mbox{pindex}_{i}\\beta_{6} + \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{7} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{8} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Selección hacia atrás. Podemos utilizar la función ols_step_backward_aic de la librería olsrr: ols_step_backward_aic(mod.surgical.completo2,details = F) ## ## ## Backward Elimination Summary ## --------------------------------------------------------------------------- ## Variable AIC RSS Sum Sq R-Sq Adj. R-Sq ## --------------------------------------------------------------------------- ## Full Model -9.135 1.058 11.747 0.91740 0.85408 ## age:alc_heavy -11.134 1.058 11.747 0.91740 0.85878 ## alc_heavy -13.133 1.058 11.747 0.91740 0.86319 ## bcs:alc_mod -15.057 1.059 11.745 0.91728 0.86715 ## age:alc_mod -16.852 1.063 11.741 0.91697 0.87057 ## gender:pindex -18.639 1.067 11.737 0.91664 0.87377 ## alc_mod -19.980 1.080 11.724 0.91562 0.87577 ## enzyme_test:alc_heavy -20.698 1.106 11.698 0.91359 0.87622 ## liver_test:alc_heavy -20.988 1.142 11.662 0.91081 0.87560 ## pindex:alc_heavy -22.223 1.158 11.646 0.90954 0.87706 ## pindex:alc_mod -22.957 1.186 11.619 0.90739 0.87729 ## bcs:gender -22.981 1.230 11.575 0.90394 0.87583 ## gender:age -23.038 1.275 11.529 0.90042 0.87434 ## gender:liver_test -23.549 1.311 11.494 0.89764 0.87383 ## age -23.745 1.355 11.449 0.89416 0.87251 ## --------------------------------------------------------------------------- Por lo tanto, el modelo seleccionado es: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{gender}_{i}\\beta_{4} + \\mbox{liver}_{i}\\beta_{5} + \\\\ &amp; \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{6} + \\mbox{enzyme}_{i}\\mbox{alc_mod}_{i}\\beta_{7} + \\mbox{liver}_{i}\\mbox{alc_mod}_{i}\\beta_{8} + \\mbox{liver}_{i}\\mbox{gender}_{i}\\beta_{9} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Con este algoritmo no se considera la edad del paciente pero si la función hepática y otras interacciones. Selección por segmentos. Aquí tenemos la función ols_step_both_aic de la librería olsrr: ols_step_both_aic(mod.surgical.completo2,details = F) ## ## ## Stepwise Summary ## ------------------------------------------------------------------------------------ ## Variable Method AIC RSS Sum Sq R-Sq Adj. R-Sq ## ------------------------------------------------------------------------------------ ## enzyme_test addition 51.434 7.334 5.471 0.42725 0.41624 ## pindex addition 24.767 4.313 8.492 0.66318 0.64997 ## bcs:alc_heavy addition -3.014 2.485 10.320 0.80596 0.79432 ## bcs addition -9.430 2.126 10.678 0.83396 0.82041 ## gender:pindex addition -10.781 1.998 10.806 0.84395 0.82770 ## gender:enzyme_test addition -19.676 1.633 11.171 0.87246 0.85618 ## gender addition -23.040 1.479 11.326 0.88452 0.86695 ## gender:pindex removal -23.631 1.518 11.287 0.88147 0.86634 ## age addition -25.088 1.424 11.381 0.88882 0.87190 ## ------------------------------------------------------------------------------------ Note que este método sigue los mismos pasos que la selección hacia delante hasta el paso 8 donde se elimina la interacción entre el índice de pronostico y el genero. Por lo que aquí obtenemos el siguiente modelo: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{age}_{i}\\beta_{4} + \\mbox{gender}_{i}\\beta_{5} + \\\\ &amp; \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{6} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{7} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Dado que los algoritmos hacen la busqueda del modelo “óptimo” evaluando diferentes subconjuntos de covariables, se obtuvieron diferentes ajustes. Si observamos el AIC de las tres opciones, el modelo obtenido con el algoritmo stepwise presenta el mejor resultado. 4.4.3 Regresión de LASSO Otra alternativa para encontrar las variables relevantes es a través del estimador de LASSO (Least Absolute Selection and Shrinkage Operator). Este es un método de regularización que se implementa se tiene cientos de covariables disponibles y se cree que pocas tienen un aporte relevante (sparsity principle). Por ejemplo, en estudios de genetica se tienen miles de genes disponibles pero solo unos pocos están activos para una mutación de interés. El estimador de LASSO se obtiene minimizando la siguiente expresión: \\[\\begin{equation} \\begin{split} S_{lasso}(\\beta)=&amp; \\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\\beta)^{2}+ \\lambda\\sum_{j=1}^{p-1}|\\beta_{j}| \\end{split} \\nonumber \\end{equation}\\] De forma de equivalente, el estimador de LASSO minimiza: \\[ \\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\\beta)^{2} \\quad \\mbox{ sujeto a } \\quad \\sum_{j=1}^{p-1}|\\beta_{j} | \\leq t. \\] Mientras que, en la regresión de ridge,\\(\\widehat{\\boldsymbol \\beta}_{ridge}\\) minimiza: \\[ \\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\\beta)^{2} \\quad \\mbox{ sujeto a } \\quad \\sum_{j=1}^{p-1}\\beta_{j}^2\\leq t. \\] Aquí vemos que la penalización tiene como efecto forzar a que los coeficientes del ajuste, especialmente los de poco aporte, tiendan a cero. A medida que se incrementa mas coeficientes tomarán el valor de cero (con el objetivo de excluir covariables que no son relevantes). Para eliminar el efecto de las unidades de medida, se recomienda escalar las covariables. También se puede probar que, cuando \\(\\lambda \\rightarrow \\infty\\), la varianza de \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\) disminuye, pero el sesgo aumenta. No hay una estimación analítica para \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\), pero hay algoritmos eficientes para su estimación. De igual forma, se recomienda escalar las variables para remover el efecto de las unidades de medida. Resultados usando los datos de % de grasa …. #se debe especificar alpha=1 X = model.matrix(mod.fat)[,-1] lasso.mod &lt;- glmnet(X, fat$brozek, alpha = 1,nlambda = 100) plot(lasso.mod,xvar=&#39;lambda&#39;,label=T,lwd=2) abline(h=0,lty=2) Figure 4.3: Estimación de los coeficientes vs log(lambda) …. selección de \\(\\lambda\\) …. 4.4.3.1 Validación cruzada La validación cruzada se utiliza para evaluar y comparar modelos. Consiste en dividir la muestra en dos grupos: Entrenamiento: se usa para ajusta el modelo. Validación: se utiliza para validar el modelo. Para no perder información, en la validación cruzada se divide la muestra en dos (o más) partes y se hace la validación en cada una. División de la muestra de forma aleatoria en \\(k = 5\\) grupos \\((k-fold)\\) Para cada división,\\(k = 1, . . . , K\\) , y para cada valor de \\(\\lambda\\), se estima el modelo basado en la muestra de entrenamiento. Mientras que con cada muestra de validación, y para cada valor de \\(\\lambda\\), se utiliza para calcular el error cuadrático medio: \\[ EMC_{k}(\\lambda) = \\sum_{i=1}^{n_k} \\frac{[y_{i}^{(k)}-x_{i}^{(k)}\\widehat{\\boldsymbol \\beta}_{lasso}^{(k)}(\\lambda)]^2}{n} \\] donde \\(y_{i}^{(k)}\\) son las observaciones de la muestra de validación \\(k\\), y \\(\\widehat{\\boldsymbol \\beta}_{lasso}^{(k)}(\\lambda)\\) es la estimación utilizando la muestra de entrenamiento \\(k\\). Para cada \\(\\lambda\\), se calcula la validación cruzada como: \\[ CV(\\lambda) = \\frac{1}{K}\\sum_{i=1}^{K}EMC_{k}(\\lambda) \\] y la desviación estándar: \\[ SD(\\lambda) = \\sqrt{\\sum_{i=1}^{K} \\frac{[EMC_{k}(\\lambda)-CV(\\lambda)]^2}{K-1}} \\] Luego, selección el \\(\\lambda\\) que minimiza \\(CV(\\lambda)\\): \\[ \\hat{\\lambda}_{cv}=arg\\quad mín_{\\lambda}-CV(\\lambda) \\] También, se puede aplicar la regla de una desviación estánda: \\[ \\hat{\\lambda}_{cv1sd}=máx \\{\\lambda:CV(\\hat{\\lambda})&lt;CV(\\hat{\\lambda}_{cv})+SD(\\hat{\\lambda}_{cv})\\} \\] 4.4.3.2 Ejemplo grasa corporal Validación cruzada con \\(k=10\\). lasso.cv &lt;-cv.glmnet(X, fat$brozek, nfolds = 252, alpha = 1,nlambda = 100) plot(lasso.cv) Figure 4.4: Validación cruzada grasa corporal Las covariables seleccionadas al estimar el lambda óptimo (regla una desviación estándar): est = glmnet(X, fat$brozek, alpha = 1,lambda = lasso.cv$lambda.1se) est$beta ## 13 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## age 0.04227147 ## weight . ## height -0.14779997 ## neck . ## chest . ## abdom 0.60302949 ## hip . ## thigh . ## knee . ## ankle . ## biceps . ## forearm . ## wrist -1.02250135 Observamos que las variables seleccionadas son age, height,abdom y wrist: mod.lasso = lm(brozek ~ age+height+abdom+wrist,data=fat) summary(mod.lasso) ## ## Call: ## lm(formula = brozek ~ age + height + abdom + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.2571 -2.8824 -0.2919 3.0419 9.3464 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.24853 6.27849 0.040 0.9685 ## age 0.06707 0.02196 3.055 0.0025 ** ## height -0.16461 0.07821 -2.105 0.0363 * ## abdom 0.67569 0.03120 21.654 &lt; 2e-16 *** ## wrist -1.93709 0.38343 -5.052 8.51e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.118 on 247 degrees of freedom ## Multiple R-squared: 0.7223, Adjusted R-squared: 0.7178 ## F-statistic: 160.6 on 4 and 247 DF, p-value: &lt; 2.2e-16 Las covariables selecionas en este modelo son todas significativas, el \\(R^2=0.7223\\) y el \\(R^2_{adj}=0.7178\\) han disminuido ligeramente respecto al modelo con todas las covariables. Método de selección AIC y PRESS BIC LASSO Est er. std. valor-p Est er. std. valor-p Est er. std. valor-p int. -20.062 10.8465 0.0656 -31.297 6.7089 0 0.249 6.2785 0.9685 age 0.059 0.0285 0.0388 0.067 0.022 0.0025 weight -0.084 0.037 0.0237 0.126 0.0229 0 height -0.165 0.0782 0.0363 neck -0.432 0.208 0.0389 abdom 0.877 0.0666 0 0.921 0.0519 0 0.676 0.0312 0 hip -0.186 0.1282 0.1473 thigh 0.286 0.1195 0.1473 forearm 0.4825 0.1725 0.0056 0.446 0.1682 0.0085 wrist -1.4049 0.4717 0.0032 -1.392 0.4099 8e-04 -1.937 0.3834 0 Modelo \\(R^2\\) PRESS AIC BIC AIC y PRESS 0.7467 4139.682 1420.225 1455.520 BIC 0.7351 4209.140 1423.471 1444.647 LASSO 0.7223 4447.052 1435.389 1456.566 "],["modelos-no-lineales.html", "Capítulo 5 Modelos no lineales 5.1 Ejemplos 5.2 Modelos no lineales 5.3 Método de Bootstrap", " Capítulo 5 Modelos no lineales 5.1 Ejemplos 5.1.1 Crecimiento de pavos Datos: turk0 de la librería alr4 Objetivo: evaluar la metionina como suplemento alimenticio para pavos. Se alimentó a 60 corrales de pavos con una dieta similar, complementada con una dosis de metionina diferente. Luego de un tiempo, se observó el peso ganado por corral. Las variables son: A: Cantidad de suplemento de metionina ( % de la dieta). Gain: Peso medio ganado por corral (gramos) después de 3 semanas. plot(Gain~A,data=turk0,xlab=&#39;cantidad de metionina (% dieta)&#39;,ylab=&#39;Peso ganado (gramos)&#39;) Figure 5.1: Datos pavos. Diagrama de dispersión. Para estos datos se puede proponer el siguiente modelo: \\[ E(Gain|A)=\\theta_{1}+\\theta_{2}[1-exp(-\\theta_{3}A)] \\] Si \\(A=0\\), entonces \\(E(Gain|A)=\\theta_{1}\\) (peso ganado sin suplemento). Si \\(\\theta_{3}&gt;0\\), \\(\\theta_{1}+\\theta_{2}\\) es la asíntota (máximo peso que se puede ganar). \\(\\theta_{2}\\) es el máximo crecimiento adicional debido al suplemento. \\(\\theta_{3}\\) representa la tasa de crecimiento. A valores de \\(\\theta_3\\) mas grandes, el crecimiento esperado se acerca a su máximo más rápidamente. 5.1.2 Puromicina Datos: Puromycin. Objetivo: evaluar la velocidad de una reacción enzimática de células tratadas con Puromicina. Se midió la reacción enzimática (qué tan rápido ésta cataliza la reacción que convierte un sustrato en producto) de 23 encimas (12 tratadas con Puromicina). Las variables son: conc :concentración de sustrato (ppm). rate:velocidad de reacción instantáneas (conteo/min2) state:tratado y no tratado. plot(Puromycin$conc,Puromycin$rate,col=Puromycin$state, xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) Figure 5.2: Daros Puromicina. Diagrama de dispersión:Encimas tratadas(puntos negros), Encimas no tratadas(puntos rojos) Modelo Michaelis-Menten(bioquímica): \\[ y_{i}=\\frac{x_{1}\\theta_{1}}{\\theta_{2}+x_{1}}+\\epsilon_{i}. \\] Figure 5.3: Modelo Michaelis-Menten Por lo que para estos datos se puede proponer el modelo: \\[ rate_{i}=\\frac{conc_{i}\\theta_{1}+state_{i}conc_{i}\\theta_{3}} {\\theta_{2}+state_{i}\\theta_{3}+conc_{i}}+\\epsilon_{i}. \\] Por lo que se tiene una curva diferente para las enzimas tratadas y no tratadas: Para enzimas no tratadas: \\[ E(rate_{i}|state=0)=\\frac{conc_{i}\\theta_{1}}{\\theta_{2}+conc_{i}}. \\] Para enzimas tratadas: \\[ E(rate_{i}|state=1)=\\frac{conc_{i}(\\theta_{1}+\\theta_{3})}{(\\theta_{2}+\\theta_{4})+conc_{i}}. \\] 5.2 Modelos no lineales En modelos de regresión asumimos que: \\[ y_{i}=m(x_{i},\\theta)+\\epsilon_{i}. \\] En el caso de modelos lineales: \\[ m(x_{i},\\theta)=\\Psi_{j}(x_{i},\\psi)^{&#39;}\\beta. \\] donde \\(\\Psi(\\cdot)\\) es una función de unos parámetros constantes \\(\\psi\\) (es decir, puedo asumir transformaciones sobre las covariables). \\(\\Psi_{j}(x_{i},\\psi)=x_{i}\\), tenemos: \\[ m(x_{i},\\theta)=\\beta_{0}+x_{1i}\\beta_{1}+x_{2i}\\beta_{2}+...+x_{p-1,i}\\beta_{p-1}. \\] Los modelos: \\[ y_{i}=m(x_{i},\\theta)+\\epsilon_{i}=\\theta_{1}+\\theta_{1}[1+exp(-\\theta_{3}x_{i})]+\\epsilon_{i}, \\] y \\[ y_{i}=m(x_{i},\\theta)+\\epsilon_{i}=\\frac{x_{i}\\theta_{i}}{\\theta_{2}+x_{i}}+\\epsilon_{i}, \\] son un modelo no-lineal (no es una combinación lineal de los parámetros). Si se hacen los mismos supuestos sobre los errores, esto es: \\[ \\epsilon\\sim N(\\boldsymbol 0,\\sigma^2 \\boldsymbol I) \\] se tiene que: \\(E(y|x_{i})=m(\\boldsymbol x_{i},\\boldsymbol \\theta)\\). \\(E(y|x_{i})=\\sigma^2\\). 5.2.1 Modelos no-lineales linealizables Modelo de regresión exponencial: \\[ E(y)=\\theta_{0}+\\theta_{1}exp(\\theta_{2}+x_{i}\\theta_{3}) \\] Si \\(\\theta_{0}=0\\), el modelo es linealizable: \\[ logy_{i}=(log\\theta_{1}+\\theta_{2})+x_{i}\\theta_{3}+\\epsilon_{i}^* \\] Sin embargo, hay que tener cuidado con el efecto de las transformaciones sobre los residuos. 5.2.2 Estimación de los parámetros La estimación de \\(\\boldsymbol \\theta\\) se hace minimizando la suma de cuadrados de los residuos: \\[ S(\\boldsymbol \\theta)=\\sum_{i=1}^n[y_{i}-m(\\boldsymbol x_{i},\\boldsymbol \\theta)]^2 \\] Para encontrar el mínimo, \\((1)\\) calculamos la derivada de \\(S(\\boldsymbol \\theta)\\) con respecto a \\(\\boldsymbol \\theta\\): \\[ \\frac{\\partial}{\\partial\\boldsymbol \\theta}S(\\boldsymbol \\theta)=-2\\sum_{i=1}^n[y_{i}-m(\\boldsymbol x_{i},\\boldsymbol \\theta)] \\begin{bmatrix}\\frac{\\partial }{\\partial\\boldsymbol \\theta}m(\\boldsymbol x_{i},\\boldsymbol \\theta)\\end{bmatrix} \\] \\((2)\\) igualamos a \\(0\\), y \\((3)\\) resolvemos la ecuación para \\(\\boldsymbol \\theta\\). En la mayoría de los casos, \\(\\frac{\\partial}{\\partial\\boldsymbol \\theta}S(\\boldsymbol \\theta)\\) es una función no-lineal \\(\\boldsymbol \\theta\\). Por lo tanto, no es posible encontrar una solución analítica y necesitamos encontrar la solución iterativamente. 5.2.2.1 Notación \\(\\theta_{j}^{(t)}\\): estimación de \\(\\theta_{j}\\) en la iteración \\(t\\) (\\(\\theta_{j}^{(0)}\\) valores iniciales). Vector score (gradiente): \\[ u(\\boldsymbol \\theta)=\\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta}=\\begin{pmatrix} \\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta} \\\\ \\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta} \\\\ \\vdots \\\\ \\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta} \\\\ \\end{pmatrix} \\quad u(\\hat{\\boldsymbol \\theta})=u(\\boldsymbol \\theta)|_{\\boldsymbol \\theta=\\hat{\\boldsymbol \\theta}} \\] Matriz Hessiana (Jacobiano): \\[ \\boldsymbol H(\\boldsymbol \\theta)= \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta^{&#39;} \\partial\\boldsymbol \\theta}=\\begin{pmatrix} \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial^2 \\boldsymbol \\theta_{1}} &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta_{1} \\partial \\boldsymbol \\theta_{2} } &amp; \\dots &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta_{1} \\partial \\boldsymbol \\theta_{p}} \\\\ \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta_{2} \\partial \\boldsymbol \\theta_{1}} &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial^2 \\boldsymbol \\theta_{2}} &amp; \\dots &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta_{2} \\partial \\boldsymbol \\theta_{p}}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta_{p} \\partial \\boldsymbol \\theta_{1}} &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\boldsymbol \\theta_{p} \\partial \\boldsymbol \\theta_{2}} &amp; \\dots &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial^2 \\boldsymbol \\theta_{p}} \\end{pmatrix} \\quad \\quad \\quad \\boldsymbol H(\\hat{\\boldsymbol \\theta})=\\boldsymbol H(\\boldsymbol \\theta)|_{\\boldsymbol \\theta=\\hat{\\boldsymbol \\theta}} \\] 5.2.2.2 Expansión de series de Tylor Una función \\(f(\\theta)\\) puede expandirse como una serie de Taylor: \\[ f(\\theta)=\\sum_{n=0}^\\infty \\frac{1}{n!}(\\theta-\\theta^*)^n \\frac{\\partial^nf(\\theta^*)}{\\partial\\theta^n} \\] Una aproximación de la función \\(f(\\theta)\\) en los valores alrededor del punto \\(\\theta^*\\) se puede hacer usando la series de Taylor con solo las dos primeras derivadas (orden 2): \\[ f(\\theta)\\approx f(\\theta^*)+(\\theta-\\theta^*) \\frac{\\partial f(\\theta^*)}{\\partial\\theta}+\\frac{1}{2}(\\theta-\\theta^*)^2\\frac{\\partial^2 f(\\theta^*)}{\\partial\\theta^2} \\] En caso de \\(\\boldsymbol \\theta\\) (vector), entonces: \\[ f(\\boldsymbol \\theta)\\approx f(\\boldsymbol \\theta^*)+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u(\\boldsymbol \\theta^*)+\\frac{1}{2}(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}H(\\boldsymbol \\theta^*)(\\boldsymbol \\theta-\\boldsymbol \\theta^*), \\] 5.2.3 Métodos iterativos de estimación Algunos métodos iterativos de estimación son: Gauss-Newton (approx. series de Taylor a la función de la media). Newton-Raphson (approx. series de Taylor a la función score). 5.2.3.1 Algoritmo de Gauss-Newton La idea es aproximar \\(m(x_{i},\\boldsymbol \\theta^*)\\) usando series de Taylor de orden 1 alrededor de \\(\\boldsymbol \\theta^*\\): \\[ m(x_{i},\\boldsymbol \\theta)\\approx m(x_{i},\\boldsymbol \\theta^*)+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u_{i}(\\boldsymbol \\theta^*) \\] Lo que lleva a una aproximación de la suma de cuadrados de los residuos: \\[\\begin{equation} \\begin{split} S(\\boldsymbol \\theta)&amp;=\\sum_{i=1}^n[y_{i}-m(x_{i},\\boldsymbol \\theta)]^2 \\approx \\sum_{i=1}^n[y_{i}-m(x_{i},\\boldsymbol \\theta^*)+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u_{i}(\\boldsymbol \\theta^*)]^2 \\\\ &amp;= \\sum_{i=1}^n[\\hat e_{i}^*+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u_{i}(\\boldsymbol \\theta^*)]^2, \\end{split} \\end{equation}\\] Esta aproximación de \\(S(\\boldsymbol \\theta)\\) es equivalente a una suma de cuadrados de un modelo lineal con \\(\\hat e_{i}^*\\) (residuos de trabajo) como variable respuesta y \\(u_{i}(\\boldsymbol \\theta^*)\\) como covariables. Por lo tanto, la solución es: \\[ (\\hat{\\boldsymbol \\theta}-\\boldsymbol \\theta^*)=[U(\\boldsymbol \\theta^*)^{&#39;}U(\\boldsymbol \\theta^*)]^{-1} U(\\boldsymbol \\theta^*)^{&#39;} \\hat{e^*} \\] \\[\\begin{equation} \\hat{\\boldsymbol \\theta}=\\boldsymbol \\theta^*+[U(\\boldsymbol \\theta^*)^{&#39;}U(\\boldsymbol \\theta^*)]^{-1}U(\\boldsymbol \\theta^*)^{&#39;} \\hat{e^*} \\tag{5.1} \\end{equation}\\] donde: \\(\\hat{e_{i}^*}=(\\hat{e_{1}^*},...,\\hat{e_{n}^*})^{&#39;}\\) \\(U(\\boldsymbol \\theta^*)\\) es una matriz con la fila \\(i\\) igual a \\(u_{i}(\\boldsymbol \\theta^*)\\). A partir de estas ecuaciones se propone el algoritmo de Gauss-Newton. Seleccione unos valores iniciales \\(\\boldsymbol \\theta_{0}\\) y calcule \\(S(\\boldsymbol \\theta_{0})\\). Establezca el contador de iteraciones en \\(k=0\\) Calcule \\(U(\\boldsymbol \\theta^{(j)})\\) y \\(\\hat{e}^{(j)}\\) y encuentre \\(\\boldsymbol \\theta^{(j+1)}\\) usando (5.1): \\[\\begin{equation} \\begin{split} &amp; \\boldsymbol \\theta^{(j+1)}=\\boldsymbol \\theta^{(j)}+[U(\\boldsymbol \\theta^{(j)})^{&#39;}U(\\boldsymbol \\theta^{(j)})]^{-1}U(\\boldsymbol \\theta^{(j)})^{&#39;} \\hat{e}^{(j)}. \\end{split} \\end{equation}\\] y calcule \\(S(\\boldsymbol \\theta^{(j+1)})\\). Pare si \\(\\delta=|S(\\boldsymbol \\theta^{(j)})-S(\\boldsymbol \\theta^{(j+1)})|\\) es suficientemente pequeño (convergente). De otra forma, \\(j=j+1\\) y vaya al paso 3. Si \\(j\\) es muy grande (muchas iteraciones), se dice que hay divergencia. El algoritmo de Gauss-Newton estima los parámetros de un problema de regresión no lineal mediante una secuencia de cálculos de mínimos cuadrados lineales aproximados. La estimación de \\(\\sigma^2\\) es: \\[ \\hat{\\sigma}^2=\\frac{1}{n-p}\\sum_{i=1}^n[y_{i}-m(x_{i},\\hat{\\boldsymbol \\theta)}]^2, \\] donde \\(p\\) es la dimensión de \\(\\boldsymbol \\theta\\). 5.2.4 Estimación de los parámetros ¿Cómo selecciono los valores iniciales? El ajuste de un modelo no-lineal requiere buenos valores iniciales (cercanos a los valores de parámetros). los valores iniciales se pueden obtener a través: Conocimiento previo. Significado físico de los coeficientes. Evaluación gráfica. Linearización de los datos. Dado que la solución del algoritmo puede caer en un máximo local, es recomendable ejecutar el algoritmo para diferentes valores iniciales. 5.2.4.1 Crecimiento de pavos - Valores iniciales En el caso del peso de los pavos: \\(\\theta_{1}^0=620\\) (pesi ganado sin suplemento) \\(\\theta_{2}^0+\\theta_{1}^0=800\\) (Asíntota). Por lo tanto \\(\\theta_{2}^0=180\\) \\(\\theta_{3}^0\\) se puede obtener a partir de resolver la ecuación para un punto posible: \\[ 750=620+180[1-exp(-\\theta_{3}^00.16)] \\] Resolviendo la ecuación \\(\\theta_{3}^0\\approx8\\). mod = nls(Gain ~ th1 + th2*(1-exp(-th3*A)),data = turk0, start = list(th1=620,th2=180,th3=8),trace=F) plot(Gain~A,data=turk0,xlab=&#39;cantidad de metionina (% dieta)&#39;,ylab=&#39;Peso ganado (gramos)&#39;) th = coef(mod) x = seq(0,0.5,length.out=100) lines(x, th[1]+th[2]*(1-exp(-th[3]*x)),col=2, lwd = 2) Figure 5.4: Datos crecimiento de pavos. Ajuste del modelo para el peso medio ganado por corral en función de la cantidad de suplemento de metionina. \\[ E(Gain|A)=622.958 + 178.252 [1 - exp(-7.122A)] \\] 5.2.5 Inferencia sobre los parámetros si \\(\\epsilon \\sim N(\\boldsymbol 0,\\sigma^2 \\boldsymbol I)\\), tenemos que asintóticamente \\((n \\to \\infty)\\): \\[ \\hat{\\boldsymbol \\theta}\\sim N(\\boldsymbol \\theta^*,\\sigma^2[U(\\boldsymbol \\theta^*)U(\\boldsymbol \\theta^*)^{&#39;}]^{-1}). \\] Dado que automáticamente \\(\\boldsymbol \\theta^* \\to \\boldsymbol \\theta\\), \\(\\hat{\\boldsymbol \\theta}\\) es un estimador insesgado de \\(\\boldsymbol \\theta\\). Hay que hacer énfasis que estas son propiedades para muestras grandes, para muestras pequeñas, estas propiedades pueden ser inadecuadas. A partir de las propiedades de muestras grandes se pueden hacer inferencias sobre los coeficientes \\(\\boldsymbol \\theta\\). Por ejemplo: Pruebas de hipótesis: \\(\\qquad H_{0}:\\theta_{j}=\\theta_{0}, \\qquad H_{1}:\\theta_{j}\\ne\\theta_{0}\\) \\[ t_{0}:\\frac{\\hat{\\theta_{j}}-\\theta_{0}}{\\sqrt{V(\\hat{\\theta_{j}})}}, \\quad \\text{donde} \\quad t_{0} \\sim t_{n-p} \\] Intervalos de confianza:IC del \\((1-\\alpha)\\)% para \\(\\theta_{j}\\): \\[ \\hat{\\theta_{j}}\\pm t_{1-\\alpha/2,n-p}\\sqrt{V(\\hat{\\theta_{j}})} \\] Intervalos de confianza: IC del \\((1-\\alpha)\\)% para \\(E(Y|x_{0})\\): Sea \\(\\boldsymbol \\theta=(\\boldsymbol \\theta_{1}^{&#39;},\\boldsymbol \\theta_{2}^{&#39;})^{&#39;}.\\) Hipótesis: \\[ H_{0}:\\boldsymbol \\theta_{2}=\\boldsymbol 0\\qquad H_{1}:\\boldsymbol \\theta_{2}\\ne\\boldsymbol 0 \\] Estadístico de prueba: \\[ F_{0}=\\frac{[SS_{res}(\\boldsymbol \\theta)-SS_{res}(\\boldsymbol \\theta_{1})]/r}{MS_{res}(\\boldsymbol \\theta)}\\sim F_{r,n-p}. \\] 5.2.5.1 Crecimiento de pavos-Estimación Modelo estimado: summary(mod) ## ## Formula: Gain ~ th1 + th2 * (1 - exp(-th3 * A)) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## th1 622.958 5.901 105.57 &lt; 2e-16 *** ## th2 178.252 11.636 15.32 2.74e-16 *** ## th3 7.122 1.205 5.91 1.41e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 32 degrees of freedom ## ## Number of iterations to convergence: 4 ## Achieved convergence tolerance: 6.736e-06 Es posible considerar otros modelos de crecimiento. Por ejemplo: Modelo 2 (modelo logístico): \\[ y_{i}=\\frac{\\theta_{1}}{1+\\theta_{2}exp(-\\theta_{3}x_{i})}+\\epsilon_{i} \\] Modelo 3 modelo Weibull: \\[ y_{i}=\\theta_{1}+\\theta_{2}[1-exp(-\\theta_{3}x_{i}^{\\theta_{4}})]+\\epsilon_{i} \\] La comparación se puede hacer por criterios de información (AIC o BIC) plot(Gain~A,data=turk0,xlab=&#39;cantidad de metionina (% dieta)&#39;,ylab=&#39;Peso ganado (gramos)&#39;) mod2 = nls(Gain ~ th1/(1+th2*exp(-th3*A)),data = turk0, start = list(th1=620,th2=0.25,th3=8) ) mod3 = nls(Gain ~ th1 + th2*(1-exp(-th3*A^th4)),data = turk0, start = list(th1=800,th2=180,th3=8,th4=1) ) th2 = coef(mod2) th3 = coef(mod3) lines(x, th[1]+th[2]*(1-exp(-th[3]*x)),col=2, lwd = 2) lines(x, th2[1]/(1+th2[2]*exp(-th2[3]*x)),col=3, lwd = 2) lines(x, th3[1]+th3[2]*(1-exp(-th3[3]*x^th3[4])),col=4, lwd = 2) Figure 5.5: Datos Pavos.Modelo 1(Rojo), Modelo logístico(verde) y Modelo Weibull(Azul) AIC(mod) ## [1] 312.6872 AIC(mod2) ## [1] 312.3979 AIC(mod3) ## [1] 314.5287 5.2.5.2 Puromicina-Estimación Los valores iniciales del modelo Michaelis-Menten se puede hacer por linealización. par(mfrow=c(1,2)) plot(Puromycin$conc,Puromycin$rate,col=Puromycin$state, xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) plot(1/Puromycin$conc,1/Puromycin$rate,col=Puromycin$state, xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) Figure 5.6: Datos puromicina. Diagrama de dispersión. Datos linealizados(derecha). A través del ajuste por MCO: \\[ 1/y_{i}=\\beta_{0}+1/conc_{i}\\beta_{1}+\\epsilon_{i} \\] Se puede obtener valores iniciales. Luego de ajustar el modelo: \\[ \\theta_{1}^{(0)}=\\frac{1}{\\hat{\\beta}_{0}}=167.408 \\qquad \\theta_{2}^{(0)}=\\frac{\\hat{\\beta}_{1}}{\\hat{\\beta}_{0}}=0.039 \\] Inicialmente, se podría asumir \\(\\beta_{3}=\\beta_{4}=0.\\) Modelo estimado: Puromycin$state2 = as.double(Puromycin$state == &#39;treated&#39;) mod.puromicyn = nls(rate ~ (th1*conc + th3*conc*state2)/(th2+th4*state2+conc),data = Puromycin, start = list(th1=168,th2=0.4,th3=0,th4=0),trace=F ) summary(mod.puromicyn ) ## ## Formula: rate ~ (th1 * conc + th3 * conc * state2)/(th2 + th4 * state2 + ## conc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## th1 1.603e+02 6.896e+00 23.242 2.04e-15 *** ## th2 4.771e-02 8.281e-03 5.761 1.50e-05 *** ## th3 5.240e+01 9.551e+00 5.487 2.71e-05 *** ## th4 1.641e-02 1.143e-02 1.436 0.167 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.4 on 19 degrees of freedom ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 3.018e-06 La tasa de crecimiento máxima es diferente si la enzima es tratada o no. El punto donde se logra la mitad del máximo es el mismo. x = seq(0,1.5,length.out=200) thP= coef(mod.puromicyn) plot(Puromycin$conc,Puromycin$rate,col=Puromycin$state, xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) lines(x, thP[1]*x/(thP[2]+x),col=2) lines(x, (thP[1]+thP[3])*x/(thP[2]+thP[4]+x)) Figure 5.7: Datos puromicina. Enzimas tratadas(negro), enzimas no tratadas(rojo). 5.3 Método de Bootstrap Las inferencias basadas para muestras grande pueden ser inexactas y/o engañosas en muestras pequeñas. En esos casos, se puede hacer inferencias usando remuestreo (bootstrap). El bootstrap es una técnica para calcular intervalos de confianza and pruebas de hipótesis cuando el cumplimiento de los supuestos asumidos están en duda. Se quiere calcular un intervalo de confianza para la mediana (o cualquier otro parámetro) de \\(Y\\). Para esto tomamos una muestra independiente \\(\\boldsymbol y=(y_{1},...,y_{n})\\). Pasos del boostrap: Obtener una muestra aleatoria con remuestreo de \\(\\boldsymbol y=(y_{1}^{*},...,y_{n}^{*})\\). Calcular la mediana usando la muestra el paso 1 \\((Me_{1})\\). Repita los pasos 1 y 2 un número grande de veces \\((B)\\). un intervalo de confianza del 95% basado en percentiles muestrales 2.5% y 97.5 %. 5.3.1 Bootstrap en regresión Método residual resampling: Con la muestra dada: \\((x_{i},y_{i}),i=1,...,n\\) ajustar el modelo y obtener los resifuos: \\(e_{i}=y_{i}-x_{i}^{&#39;}\\hat{\\beta}.\\) Obtener una muestra aleatoria con reemplazo de los residuos \\(e^*=(e_{1}^*,...,e_{n}^*)\\). 3.Crear una variable respuesta bootstrap con \\(y_{i}^*=x_{i}^{&#39;}\\hat{\\beta}+e_{1}^*\\) y estimar \\(\\hat{\\beta}^*\\). Repetir los pasos 1-3 un número grande de veces \\((B)\\) y obtener el intervalo de confianza para \\(\\beta_{j}\\) usando los percentiles*. \\(*\\)Hay muchas modificaciones y extensiones para calcular los intervalos de confianza. set.seed(310) mod.boot = Boot(mod.puromicyn,method=&#39;residual&#39;,R=1000) ## ## Number of bootstraps was 999 out of 1000 attempted pairs(mod.boot$t,labels=c(expression(hat(theta)[1]),expression(hat(theta)[2]),expression(hat(theta)[3]), expression(hat(theta)[4]))) Figure 5.8: Metodo Bootstrap datos puromicina. Estimaciones por bootstrap par(mfrow=c(1,4)) hist(mod.boot$t[,1],breaks = 20,xlab=expression(hat(theta)[1]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[1],lty=2,lwd=2) abline(v=mean(mod.boot$t[,1]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,1],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) hist(mod.boot$t[,2],breaks = 20,xlab=expression(hat(theta)[2]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[2],lty=2,lwd=2) abline(v=mean(mod.boot$t[,2]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,2],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) hist(mod.boot$t[,3],breaks = 20,xlab=expression(theta[3]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[3],lty=2,lwd=2) abline(v=mean(mod.boot$t[,3]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,3],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) hist(mod.boot$t[,4],breaks = 20,xlab=expression(theta[4]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[4],lty=2,lwd=2) abline(v=mean(mod.boot$t[,4]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,4],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) Figure 5.9: Linea negra: estimación por mínimos cuadrados.Lineas rojas: percentil 2.5, media y percentil 97.5 Intervalos del 95% de confianza usando bootstrap (corrección de sesgo): quantile(mod.boot$t[,1],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## 150.2763 175.1425 quantile(mod.boot$t[,2],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## 0.03538444 0.06329289 quantile(mod.boot$t[,3],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## 34.8151 70.2094 quantile(mod.boot$t[,4],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## -0.00663091 0.03540431 Puromicina - bootstrap fit.rate= function(theta,x){ cbind(theta[1]*x/(theta[2]+x), (theta[1]+theta[3])*x/(theta[2]+theta[4]+x)) } x= seq(from=0,to=1.1,length.out = 100) plot(NULL,NULL,xlim=c(0,1.1),ylim=c(0,210),xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) Fit = mapply(function(i){ pred = fit.rate(mod.boot$t[i,],x) lines(x,pred[,1],col=&#39;lightgray&#39;) lines(x,pred[,2],col=&#39;gray&#39;) },i=1:999) lines(x, thP[1]*x/(thP[2]+x),col=2) lines(x, (thP[1]+thP[3])*x/(thP[2]+thP[4]+x)) points(Puromycin$conc,Puromycin$rate,col=Puromycin$state) Figure 5.10: Todas las curvas estimadas por bootstrap. Linea roja: estimación puntual. 5.3.2 Algunas consideraciones Lo ideal es que el algoritmo llegue a la solución en pocas iteraciones (esto pasa si la aproximación lineal es adecuada). Siempre es bueno evaluar la solución con diferentes puntos iniciales. Es posible que caigamos en un máximo local. Si el tamaño de muestra no es grande, las propiedades asintóticas pueden no ser adecuadas. Por lo tanto, es mas conveniente usar bootstrap para hacer inferencias. "],["modelo-lineal-generalizado.html", "Capítulo 6 Modelo lineal generalizado 6.1 Introducción 6.2 Modelo lineal generalizado (GLM) 6.3 GLM 6.4 intervalos de confianza 6.5 Devianza 6.6 Bondad de ajuste 6.7 Pseudo \\(R^2\\) 6.8 Selección de variables", " Capítulo 6 Modelo lineal generalizado 6.1 Introducción 6.1.1 Modelos lineales Modelo lineal: \\[ y_{i}=\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta+\\epsilon, \\quad \\text{donde}\\quad \\epsilon \\sim N(\\boldsymbol 0,\\sigma^2\\boldsymbol I) \\] Lo que implica que \\(E(y_{i}|\\boldsymbol x_{i})=\\boldsymbol x_{i}\\boldsymbol \\beta\\) y \\(V(y_{i}|\\boldsymbol x_{i})=\\sigma^2\\) En algunos casos es difícil que se cumplan esas propiedades (incluso luego de hacer transformaciones). Si la variable respuesta \\(Y\\) solo puede tomar dos valores \\((y_{i}\\in\\{0,1\\})\\) no podríamos representar \\(E(y|\\boldsymbol x)\\) como una función lineal. Los modelos lineales generalizados (GLM) son una clase de modelos que permite modelar variables aleatorias con distribución de probabilidad diferentes a la normal. 6.1.2 Mortalidad de escarabajos Número de escarabajos muertos después de cinco horas de exposición a disulfuro de carbono gaseoso \\((CS2mgl^{-1})\\) en diversas concentraciones: Table 6.1: Datos mortalidad de escarabajos logdose n dead 1.6907 59 6 1.7242 60 13 1.7552 62 18 1.7842 56 28 1.8113 63 52 1.8369 59 53 1.8610 62 61 1.8839 60 60 ¿Hay una relación entre la dosis y la mortalidad de escarabajos? plot(logdose,dead/n,xlab=&#39;log dosis&#39;,ylab=&#39;proporción de muertos&#39;,ylim=c(0,1),pch=16) Figure 6.1: Datos escarabajos. Proporción de muerte vs log dosis. 6.1.3 Modelo logístico Definiendo \\(y_{i}=\\sum_{i=1}^ny_{ij}/n_{i}\\) (la proporción de éxitos en \\(n_i\\) ensayos independientes), tenemos que: \\[ n_{i}y_{i}\\sim binomial(n_{i},\\pi_{i}), \\quad i=1,...,n, \\quad \\text{donde} \\quad \\pi_{i}=g(\\boldsymbol x_{i},\\boldsymbol \\beta). \\] Entonces: \\(E(y_{i}|\\boldsymbol x_{i})=\\pi_{i}\\) y \\(V(y_{i}|\\boldsymbol x_{i})=\\pi_{i}(1-\\pi_{i})/n_{i}\\). El modelo ligístico es un GLM y asume que: \\[ \\pi_{i}=g^{-1}(\\boldsymbol x_{i},\\boldsymbol \\beta)=\\frac{exp(\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta)}{1+exp(\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta)}=\\frac{1}{1+exp(-\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta)} \\] Por lo que: \\(g(\\pi_{i})=log(\\frac{\\pi_{i}}{1-\\pi_{i}})=\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta\\) (función logit). Figure 6.2: Función logística 6.1.4 ataques de epilepsia Ensayo clínico para evaluar el impacto de progabida sobre las crisis epilépticas (data(epilepsy) de la librería HSAUR2). Datos: age: edad del paciente. base: número de ataques epilépticos (x 8 semanas) antes del ensayo. treatment: tratamiento (placebo, progabida). seizure.rate(variable respuesta): número de ataques epilépticos (x dos semanas) luego de 8 semanas. epilepsy=subset(epilepsy, period==4)#selecionamos la semana 4 par(mfrow=c(1,3)) plot(seizure.rate~age, col=treatment, pch=16, data=epilepsy, xlab=&quot;Edad&quot;, ylab = &quot;Num. ataques epiléptico(4ta semana)&quot;) plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy, xlab=&quot;Num. ataques epiléptico(pre tratamiento)&quot;, ylab = &quot;Num. ataques epiléptico(4ta semana)&quot;) boxplot(seizure.rate~treatment,data=epilepsy, xlab = &quot;Tratamiento&quot;, ylab = &quot;Num. ataques epiléptico(4ta semana)&quot;) Figure 6.3: Datos epilepsia 6.1.4.1 Modelo poisson Aquí podemos suponer que: \\[ y_{i}\\sim Poisson(\\lambda_{i}), \\quad i=1,...,n,\\quad \\text{donde} \\quad \\lambda_{i}=g(\\boldsymbol x_{i},\\boldsymbol \\beta). \\] Entonces:\\(E(y_{i}|\\boldsymbol x_{i})=V(y_{i}|\\boldsymbol x_{i})=\\lambda_{i}).\\) El modelo Pooisson es un GLM y asume que: \\[ \\lambda_{i}=g^{-1}(\\boldsymbol x_{i},\\boldsymbol \\beta)=exp(\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta). \\] Por lo que: \\(g(\\lambda_{i})=log\\lambda_{i}=\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta\\) (función log). 6.2 Modelo lineal generalizado (GLM) Un modelo lineal generalizado (GLM) tiene tres componentes: Componente aleatorio: variable respuesta \\(Y\\) y su distribución de probabilidad. Predictor lineal: \\[ \\eta=\\boldsymbol x^{&#39;}\\boldsymbol \\beta, \\] donde \\(\\boldsymbol \\beta\\) es un vector de parámetros y \\(\\boldsymbol x\\) un vector de covariables. Función de enlace: una función g que conecta \\(E(Y)\\) con el predictor lineal, \\[ g[E(Y|\\boldsymbol x)]=\\boldsymbol x^{&#39;}\\boldsymbol \\beta \\] 6.2.1 Componente aleatorio Observaciones independientes \\((y_{i},...,y_{n})\\) de una variable aleatoria \\(Y\\) cuya distribución de probabilidad pertenece a la familia exponencial. Restringir un GLM a la familia exponencial permite tener expresiones generales para: La función de verosimilitud y funciones score. distribución asintótica de los estimadores de los parámetros del modelo Algoritmo para ajustar el modelo. 6.2.2 Predictor lineal Para cada observación \\(i\\), \\[ \\boldsymbol x_{i}=(1,x_{i1},...,x_{i(p-1)})^{&#39;}, \\] donde \\(x_{ij}\\) es la observación \\(i\\) de la covariable \\(j,\\quad j=1,...,p-1\\). Predictor lineal es una combinación lineal de las covariables: \\[ \\eta_{i}=\\beta_{0}+\\sum_{j=1}^{p-1}\\beta_{j}x_{ij}=\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta \\] Un GLM asume que las covariables no son aleatorias. 6.2.3 Función de enlace Sea: \\[ E(Y|\\boldsymbol x_{i})=\\mu_{i}, \\quad i=1,...,n. \\] En algunas distribuciones,\\(\\mu_{i}\\) está acotada en un intervalo. Por ejemplo, en la distribución binomial, \\(0\\leq \\pi_{i}\\leq 1\\), o en la Poisson, \\(\\lambda_{i}&gt;0\\). El GLM conecta \\(\\mu_{i}\\) con \\(\\eta_{i}\\): \\[ \\mu_{i}=g^{-1}(\\eta_{i})=g^{-1}(\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta) \\] La función \\(g(\\cdot)\\) es monótona y diferenciable. \\(g(\\cdot)\\) esta determinada generalmente por la distribución que se asume para \\(Y\\). 6.2.4 Ejemplos de GLM Algunos ejemplos de GLM: Tipo de respuesta Distribución función de enlace Modelo Continuo Normal Identidad Modelo lineal Binaria Bernoulli Logit Modelo logístico Conteo Poisson Log Modelo Poisson Otros ejemplos son beta , gamma, exponencial, … hay extensiones del GLM para distribuciones: binomial negativa para conteo con sobredispersión. beta-binomial para ensayos Bernoulli correlacionados (sobredispersión). multinomial para variables nominales (ordinales) con más de dos categorías Weibull para tiempos de falla. 6.2.5 Ajuste de un GLM El proceso de ajustar un GLM incluye: Especificación del modelo.Definición del componente aleatorio, predictor lineal y función de enlace. Estimación de los parámetros del modelo. Evaluación del modelo. ¿El modelo describe bien los datos? Inferencia. Intervalos de confianza, pruebas de hipótesis e interpretación de los resultados. 6.2.6 Familia exponencial La distribución de probabilidad de una variable aleatoria Y pertenece a la familia exponencial si la función de densidad (o masa) de Y toma esta forma: \\[ f(y;\\theta,\\phi)exp\\{[y\\theta-b(\\theta)]/a(\\phi)+c(y,\\phi)\\} \\] donde: \\(\\theta\\) es el parámetro natural. \\(\\phi&gt;0\\) parámetro de dispersión. Además se tiene que: \\[ E[Y]=b^{&#39;}(\\theta) \\quad \\text{y} \\quad V[Y]=b^{&#39;&#39;}(\\theta)a(\\phi). \\] 6.2.6.1 Ejemplo La distribución Poisson: \\[ f(y;\\mu)=\\frac{\\mu^yexp(-\\mu)}{y!}, \\quad \\theta&gt;0. \\] se puede re-escribir como: \\[ f(y;\\mu)=exp(-\\mu+ylog\\mu-logy!). \\] Por lo tanto: \\(\\theta=log\\mu, \\quad b(\\theta)=exp(\\theta)\\). \\(a(\\phi)=1\\) y \\(c(y,\\phi)=-lny!\\). La distribución normal: \\[ f(y;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma}}exp\\begin{bmatrix}-\\frac{(y-\\mu )^2}{2\\sigma^2}\\end{bmatrix} \\] Se puede re-escribir como: \\[ f(y;\\mu,\\sigma^2)=exp[\\frac{y\\mu-\\frac{1}{2}\\mu^2}{\\sigma^2}-\\frac{1}{2}log(2\\pi\\sigma^2)-\\frac{y^2}{2\\sigma^2}] \\] donde: \\(\\theta=\\mu, \\quad b(\\theta)=\\frac{1}{2}\\theta^2\\). \\(a(\\phi)=\\sigma^2\\) y \\(c(y,\\phi)=-\\frac{1}{2}log(2\\pi\\sigma^2)-\\frac{y^2}{2\\sigma^2}\\). La distribución binomial: \\[ f(y;\\pi)=\\left(\\begin{array}{c}n\\\\y\\end{array}\\right)\\pi^y(1-\\pi)^{n-y}. \\] se puede re-escribir como: \\[ f(y;\\pi)=exp\\left\\{y[log\\pi-log(1-\\pi)]+nlog(1-\\pi)+log\\left(\\begin{array}{c}n\\\\y\\end{array}\\right)\\right\\} \\] donde: \\(\\theta=log(\\frac{\\pi}{1-\\pi}), \\quad b(\\theta)=-nlog[1+exp(\\theta)]\\). \\(c(\\phi)=1\\) y \\(d(y,\\phi)=log\\left(\\begin{array}{c}n\\\\y\\end{array}\\right)\\). Familia exponencial Distribución \\(\\theta\\) \\(a(\\phi)\\) \\(b(\\theta)\\) \\(c(y,\\phi)\\) Normal \\(\\mu\\) \\(\\sigma^2\\) \\(\\frac{1}{2}\\mu^2\\) \\(-\\frac{1}{2}log(2\\pi\\sigma^2)-\\frac{y^2}{2\\sigma^2}\\) Binomial \\(log(\\frac{\\pi}{1-\\pi})\\) \\(\\frac{1}{n}\\) \\(log(1+exp\\theta)\\) \\(log\\left(\\begin{array}{c}n \\\\ y\\end{array}\\right)\\) Poisson \\(log(\\lambda)\\) \\(1\\) \\(exp\\lambda\\) \\(-log(y!)\\) Relación media-varianza: Distribución \\(v(\\mu)\\) Normal \\(\\sigma^2\\) Binomial \\(\\mu(1-\\mu)/n\\) Poisson \\(\\mu\\) 6.2.6.2 Estimador de máxima verosimilitud Modelo \\(Y\\sim f(y,\\boldsymbol \\theta)\\). Estamos interesados en estimar \\(\\boldsymbol \\theta\\). Para ello tomamos una muestra independiente \\((y_{1},y_{2},...,y_{n}).\\) Si las observaciones son independientes, la función de verosimilitud es: \\[ L(\\boldsymbol \\theta)=\\prod_{i=1}^nf(y_{i};\\boldsymbol \\theta). \\] La función de log-verosimilitud: \\[ ℓ(\\boldsymbol \\theta)=\\sum_{i=1}^nlnf(y_{i},\\boldsymbol \\theta). \\] El objetivo es encontrar el \\(\\hat{\\boldsymbol \\theta}\\) que maximiza \\(L(\\boldsymbol \\theta)\\) (o \\(ℓ(\\boldsymbol \\theta)\\)). \\(\\hat{\\boldsymbol \\theta}\\) se obtiene calculando las derivadas de \\(ℓ(\\boldsymbol \\theta)\\) con respecto a cada elemento de \\(\\boldsymbol \\theta\\) y resolviendo las ecuaciones score: \\[ U(\\boldsymbol \\theta)=\\frac{\\partial ℓ(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta}=\\boldsymbol 0. \\] s necesario verificar si la solución corresponde a un máximo de \\(ℓ(\\boldsymbol \\theta)\\). Se requiere que la matriz de segundas derivadas (matriz Hessiana): \\[ H(\\boldsymbol \\theta)=\\frac{\\partial^2ℓ(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta\\partial\\boldsymbol \\theta^{&#39;}}, \\] evaluada en \\(\\boldsymbol \\theta=\\hat{\\boldsymbol \\theta}\\), sea negativa definida. 6.2.6.3 Propiedades asintóticas de los MLEs \\(\\hat{\\boldsymbol \\theta}\\) es asintóticamente \\((n\\to \\infty)\\) insesgado. Esto es, \\(E(\\hat{\\boldsymbol \\theta})=\\boldsymbol \\theta\\). La varianza asintótica \\((n\\to \\infty)\\) de \\(\\hat{\\boldsymbol \\theta}\\) se calcula como la inversa de la matriz de información: \\[ V(\\hat{\\boldsymbol \\theta})=I(\\boldsymbol \\theta)^{-1},\\quad \\text{donde} \\quad I(\\boldsymbol \\theta)=-E[H(\\boldsymbol \\theta)] \\] Teorema de Cramer-Rao: La varianza de cualquier estimador es insesgado de un parámetro \\(\\boldsymbol \\theta\\) debe: \\[ V(\\hat{\\boldsymbol \\theta})\\geq-E[H(\\boldsymbol \\theta)]^{-1}. \\] Por lo cual, es MLE es eficiente. Otras propiedades importantes: Asintoticamente normal:\\(\\hat{\\boldsymbol \\theta}\\sim N[\\boldsymbol \\theta,I(\\boldsymbol \\theta)^{-1}]\\) Invarianza:si \\(\\hat{\\boldsymbol \\theta}\\) es el MLE de \\(\\boldsymbol \\theta\\), entonces \\(g(\\hat{\\boldsymbol \\theta})\\) es el MLE de \\(g(\\boldsymbol \\theta)\\) 6.2.6.4 Métodos iterativos de maximización Algoritmo de Newton-Raphson: Considere una expansión de series de Taylor de orden 1 para la función score, alrededor de \\(\\boldsymbol \\theta=\\boldsymbol \\theta^{(t)}\\): \\[ U(\\boldsymbol \\theta)\\approx U(\\boldsymbol \\theta^{(t)})+H(\\boldsymbol \\theta^{(t)})+(\\boldsymbol \\theta-\\boldsymbol \\theta^{(t)}) \\] Igualando a cero: \\[ U(\\boldsymbol \\theta^{(t)})+H(\\boldsymbol \\theta^{(t)})+(\\boldsymbol \\theta-\\boldsymbol \\theta^{(t)})=0 \\] Ahora encontramos la solución para \\(\\hat{\\boldsymbol \\theta^{(t+1)}}\\): \\[ \\boldsymbol \\theta^{(t+1)}=\\boldsymbol \\theta^{t}-H(\\boldsymbol \\theta^{t})^{-1}U(\\boldsymbol \\theta^{t}). \\] Podemos reemplazar \\(H(\\boldsymbol \\theta)\\) por \\(I(\\boldsymbol \\theta)\\) (algoritmo Fisher’s scoring). 6.3 GLM El modelo lineal generalizado (GLM) asume que: \\[ f(y_{i};\\theta_{i},\\phi)=exp\\left\\{\\frac{y_{i}\\theta_{i}-b(\\theta_{i})}{a(\\phi)} +c(y_{i},\\phi)\\right\\}, \\] donde \\(\\phi&gt;0\\). Además, \\(E(y_{i})=b&#39;(\\theta_{i})\\) y \\(V(y_{i})=b&#39;&#39;(\\theta_{i})a(\\phi).\\) El predictor lineal \\(\\eta_{i}=\\boldsymbol x&#39;_{i}\\boldsymbol \\beta\\) se relaciona con \\(\\mu_{i}\\) a través la función de enlace: \\[ \\eta_{i}=g(\\mu_{i}). \\] La función de enlace \\(g(\\cdot)\\) que transforma \\(\\mu_{i}\\) en el parámetro natural \\(\\theta_{i}\\) es llamada función de enlace canónica. Distribución normal: \\(\\eta_{i}=\\mu_{i}\\) (identidad), por lo tanto: \\[ \\mu_{i}=\\eta_{i} \\] Distribución Poisson:\\(\\eta_{i}=log\\mu_{i}\\) (logarítmica), por lo tanto: \\[ \\mu_{i}=exp(\\eta_{i}) \\] Distribución binomial:\\(\\eta_{i}=log(\\frac{\\pi_{i}}{1-\\pi_{i}})\\) (logit), por lo tanto: \\[ \\pi_{i}=\\frac{exp(\\eta_{i})}{1+exp(\\eta_{i})} \\] 6.3.1 Mortalidad de escarabajos Variable respuesta \\(y_{i}=\\sum_{j=1}^{n_{i}}y_{ij}/n{i}\\): proporción de escarabajos muertos. Distribución de probabilidad binomial: \\[ f(n_{i}y_{i};\\pi_{i})= \\begin{pmatrix} n_{i}\\\\y_{i} \\end{pmatrix}\\pi_{i}^{y_{i}}(1-\\pi_{i})^{(n_{i}-y_{i})}. \\] Función de enlace: \\[ log\\left(\\frac{\\pi_{i}}{1-\\pi_{i}}\\right)=\\beta_{0}+\\beta_{1}log\\text{dosis}_{i}. \\] 6.3.2 Ataques epilépticos Variable respuesta \\(y_{i}\\): número de ataques epilépticos. Distribución de probabilidad Poisson: \\[ f(y_{i};\\lambda_{i})=\\frac{\\lambda_{i}^{y_{i}}exp(-\\lambda_{i})}{y_{i}!}. \\] Función de enlace: \\[ log\\lambda_{i}=\\beta_{0}+\\beta_{1}\\text{treat}_{i}+\\beta_{2}\\text{base}_{i}+\\beta_{3}\\text{base}_{i}\\text{treat}_{i} \\] 6.3.3 MLE de \\(\\boldsymbol \\beta\\) Función de log-verosimilitud: \\[ ℓ(\\boldsymbol \\beta)=\\sum_{i=1}^{n}ℓ_{i}(\\boldsymbol \\beta)=\\sum_{i=1}^{n}logf(y_{i};\\theta_{i},\\phi)=\\sum_{i=1}^{n}\\{\\frac{y_{i}\\theta_{i}-b(\\theta_{i})}{a(\\phi)}+c(y_{i},\\phi)\\} \\] En un GLM tenemos que \\(\\eta_{i}=\\boldsymbol x_{i}&#39;\\boldsymbol \\beta=g(\\mu_{i})\\). Las funciones score están definidas como: \\[ U(\\beta_{j})=\\sum_{i=1}^{n}\\frac{(y_{i}-\\mu_{i})x_{ij}}{V(y_{i})}\\frac{\\partial\\mu_{i}}{\\partial\\eta_{i}}=0, \\quad \\text{para} \\quad j=0,...,k. \\] Dado que las funciones score son no-lineales, es necesario estimar \\(\\boldsymbol \\beta\\) iterativamente usando el algoritmo de Newton-Raphson. En forma matricial, tenemos: Función score:\\(U(\\boldsymbol \\beta)=\\boldsymbol X&#39;\\boldsymbol D\\boldsymbol V^{-1}(\\boldsymbol y-\\boldsymbol \\mu)\\), Función Hessiana: \\(H(\\boldsymbol \\beta)=\\boldsymbol I(\\boldsymbol \\beta)=\\boldsymbol X&#39;\\boldsymbol W\\boldsymbol X\\) donde: \\(\\boldsymbol V\\) es una matriz diagonal con valores \\(v_{ii}=V(y_{i})\\) en la diagonal. \\(\\boldsymbol D\\) es una matriz diagonal con valores \\(d_{ii}=\\frac{\\partial\\mu_{i}}{\\partial\\eta_{i}}\\). \\(\\boldsymbol W\\) es una matriz diagonal con \\(w_{ii}=\\frac{(\\partial\\mu_{i}/\\partial\\eta_{i})^2}{V(y_{i})}\\). 6.3.3.1 Estimador de máxima verosimilitud Utilizando el algoritmo de Newton-Raphson (Fisher’s scoring): \\[ \\boldsymbol \\beta^{(t+1)}=\\boldsymbol \\beta^{(t)}+[\\boldsymbol I(\\boldsymbol \\beta^{(t)})]^{-1}U(\\boldsymbol \\beta^{(t)}). \\] Equivalentemente, utilizando el método de mínimos cuadrados iterativamente reponderados: \\[ \\boldsymbol \\beta^{(t+1)}=(\\boldsymbol X&#39;\\boldsymbol W^{(t)}\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol W^{(t)}z^{(t)} \\] donde, los elementos de \\(z^{(t)}\\) son: \\[ z_{i}^{(t)}=\\boldsymbol x_{i}&#39;\\boldsymbol \\beta^{(t)}+(y_{i}-\\mu_{i}^{(t)})\\frac{\\partial\\eta_{i}^{(t)}}{\\partial \\mu_{i}^{(t)}}. \\] Distribución asintótica del MLE \\((n \\to \\infty)\\): \\[ \\hat{\\boldsymbol \\beta}\\sim N[\\boldsymbol \\beta,(\\boldsymbol X&#39;\\boldsymbol W\\boldsymbol X)^{-1}], \\] donde \\(\\boldsymbol W\\) es una matriz diagonal con: \\[ w_{ii}=\\frac{(\\partial\\mu_{i}/\\partial\\eta_{i})^2}{V(y_{i})}. \\] Note que los valores de \\(\\boldsymbol W\\) dependen de la función de enlace \\(g\\) \\[ \\partial\\mu_{i}/\\partial\\eta_{i}=g&#39;( \\mu_{i}) \\] 6.3.3.2 Mortalidad de escarabajos Modelo estimado: \\[ \\hat{\\pi}_{i}=\\frac{exp(-60.717+34.27log\\text{dosis})}{1+exp(-60.717+34.27log\\text{dosis})} \\] modBin = glm(cbind(dead,n-dead)~logdose,family=binomial) pred.x = data.frame(logdose = seq(min(logdose),max(logdose),length.out = 50)) pred = predict(modBin,pred.x,type=&#39;response&#39;) plot(logdose,dead/n,xlab=&#39;log dosis&#39;,ylab=&#39;proporción de escarabajos muertos&#39;,ylim=c(0,1)) lines(pred.x$logdose,pred,col=2,lwd=2) 6.3.4 interpretación de parámetros - modelo logístico En el caso de la mortalidad de escarabajos, un odds está definido como: \\[ \\text{odd}(\\text{morir}|x)=\\frac{P(Y=1|x)}{P(Y=0|x_{i})}=exp(\\beta_{0}+x\\beta_{1}) \\] Mientras que un odds ratio está definido como: \\[ \\text{OR}=\\frac{\\text{odd}(\\text{morir}|x=a+1)}{\\text{odd}(\\text{morir}|x=a)}=\\frac{exp(\\beta_{0}+(a+1)\\beta_{1})}{exp(\\beta_0+a\\beta_1)}=exp(\\beta_1). \\] Por lo tanto, por cada cambio unitario en \\(x\\) los odds (`chances’) de morir incrementan por un factor de \\(exp(\\beta_1)\\). Por un cambio en\\(x\\) de \\(a\\) a \\(a+\\delta\\), \\(OR=e^{\\delta\\beta_1}\\). 6.3.4.1 Ataques epilépticos Modelo estimado: \\[ \\hat{\\lambda}_i=exp(1.245-0.363\\text{treat}_i+0.021\\text{base}_i+0.0008\\text{base}_i\\text{treat}_i) \\] modPois=glm(seizure.rate~base+treatment+base:treatment,family=poisson, data =epilepsy) summary(modPois) ## ## Call: ## glm(formula = seizure.rate ~ base + treatment + base:treatment, ## family = poisson, data = epilepsy) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0482 -1.2217 -0.1162 0.5115 4.0478 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.2453159 0.1176157 10.588 &lt;2e-16 *** ## base 0.0209356 0.0019144 10.936 &lt;2e-16 *** ## treatmentProgabide -0.3635857 0.1624995 -2.237 0.0253 * ## base:treatmentProgabide 0.0008523 0.0022778 0.374 0.7083 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 476.25 on 58 degrees of freedom ## Residual deviance: 149.54 on 55 degrees of freedom ## AIC: 345.3 ## ## Number of Fisher Scoring iterations: 5 y = data.frame(base=seq(min(epilepsy$base),max(epilepsy$base),length.out=100), treatment = factor(1:2, levels = 1:2, labels = levels(epilepsy$treatment))) predpois = predict(modPois,y,type=&#39;response&#39;) x=seq(min(epilepsy$base),max(epilepsy$base),length.out=50) plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy, xlab=&quot;Num. ataques epiléptico(pre tratamiento)&quot;, ylab = &quot;Num. ataques epiléptico(4ta semana)&quot;) lines(x,predpois[order(y$treatment, y$base)[1:50]], lwd = 2) lines(x,predpois[order(y$treatment, y$base)[51:100]],col=2, lwd = 2) Figure 6.4: Ajuste del modelo Poisson. Placebo(negro), Progabide(rojo) 6.3.5 Interpretación de parámetros - modelo Poisson En el modelo Poisson, tenemos que: \\(E(Y|x)=exp(\\beta_0+x\\beta_1)\\) \\[ \\beta_1=log\\left[\\frac{E(Y|x=a+1)}{E(Y|x=a)}\\right] \\] Por lo tanto, \\[ exp(\\beta_1)=\\frac{E(Y|x=a+1)}{E(Y|x=a)}. \\] Entonces, \\(exp(\\beta_1)\\) es una tasa de crecimiento del valor esperado de \\(Y\\) por un incremento de \\(x\\) de una unidad. 6.3.5.1 Algunas consideraciones Para GLM con función de enlace canónica: Matriz hessiana = -matriz de información Por lo tanto los algoritmos Newton-Raphson y Fisher’s scoring son equivalentes La función de log-verosimilitud es concava. Por lo tanto, no hay posibilidad de múltiples máximos. 6.3.6 Pruebas de hipótesis Al igual que en los modelos lineales, uno puede estar interesado en realizar pruebas hipótesis sobre los coeficientes del modelo: \\[ \\qquad H_0:\\boldsymbol \\beta_0=\\boldsymbol 0\\qquad H_1:\\boldsymbol \\beta_0 \\ne \\boldsymbol 0 \\] Para esto podemos utilizar: Método del score (multiplicadores de Lagrange). Método de Wald. Método de razón de verosimilitud. Modelo completo:\\(\\boldsymbol\\eta=\\boldsymbol X_1\\boldsymbol \\beta_1+\\boldsymbol X_2\\boldsymbol \\beta_2\\). Modelo restringido:\\(\\boldsymbol\\eta_0=\\boldsymbol X_1\\boldsymbol \\beta_1+\\boldsymbol X_2\\boldsymbol \\beta_2^0.\\) En este caso, quiero probar: \\[ \\qquad H_0:\\boldsymbol \\beta_2=\\boldsymbol \\beta_2^0 \\qquad H_1:\\boldsymbol \\beta_2 \\ne \\boldsymbol \\beta_2^0 \\] donde \\(\\boldsymbol \\beta_2\\) es de dimensión \\(q\\) y \\(\\boldsymbol \\beta_2^0\\) es de dimensión \\(p-q\\) 6.3.6.1 Prueba del score Hipótesis: \\[ \\qquad H_0:\\beta_2=\\beta_2^0 \\qquad \\beta_2\\ne\\beta_2^0 \\] Asintóticamente,\\(U(\\boldsymbol \\beta)\\sim N[\\boldsymbol 0,\\boldsymbol I(\\beta)]\\), entonces: \\[ U(\\boldsymbol \\beta)&#39;\\boldsymbol I(\\beta)^{-1}U(\\boldsymbol \\beta)\\sim \\chi^2. \\] Estadístico de prueba: \\[ U_2(\\hat{\\beta}^0)&#39;\\left\\{V\\left[U_2(\\hat{\\beta}^0)\\right]\\right\\}^{-1}U_2(\\hat{\\beta}^0)\\sim \\chi^2_{p-q}, \\] donde \\(\\hat{\\beta}^0=(\\hat{\\beta}_1,\\beta^0_2)&#39;.\\) 6.3.6.2 Prueba de wald Hipótesis: \\[ \\qquad H_0:\\boldsymbol \\beta_2=\\boldsymbol \\beta_2^0 \\qquad H_1:\\boldsymbol \\beta_2 \\ne \\boldsymbol \\beta_2^0 \\] Asintóticamente, \\(\\hat{\\boldsymbol \\beta}\\sim N[\\boldsymbol \\beta,\\boldsymbol I(\\boldsymbol \\beta)^{-1}]\\). por lo tanto: \\[ (\\hat{\\boldsymbol \\beta}-\\boldsymbol \\beta)&#39;\\boldsymbol I(\\boldsymbol \\beta)(\\hat{\\boldsymbol \\beta}-\\boldsymbol \\beta)\\sim\\chi^2. \\] Estadístico de prueba: \\[ \\left(\\hat{\\boldsymbol \\beta}_2-\\boldsymbol \\beta_2^0\\right)&#39;\\left[V\\left(\\hat{\\boldsymbol \\beta}_2\\right)\\right]^{-1}\\left(\\hat{\\boldsymbol \\beta}_2-\\boldsymbol \\beta_2^0\\right)\\sim \\chi^2_{p-q}. \\] 6.3.6.3 Prueba de razón de verosimilitud Hipótesis: \\[ \\quad H_0:\\boldsymbol \\beta_2=\\boldsymbol \\beta_2^0 \\qquad H_1:\\boldsymbol \\beta_2\\ne\\boldsymbol \\beta_2^0 \\] Asintóticamente (y usando aproximación por series Taylor de orden 1): \\[ 2\\left[ℓ(\\hat{\\boldsymbol \\beta})-ℓ(\\hat{\\boldsymbol \\beta}^0)\\right]\\sim\\chi^2. \\] Estadístico de prueba: \\[ 2\\left[ℓ(\\hat{\\boldsymbol \\beta})-ℓ(\\hat{\\boldsymbol \\beta}^0)\\right] \\] donde \\(\\hat{\\boldsymbol \\beta}^0=(\\hat{\\boldsymbol \\beta}_1,\\boldsymbol \\beta_2^0)&#39;.\\) 6.3.6.4 Ejemplo ataques epilépticos Efecto Parm. est. err. std. Valor-z Valor-p Intercepto \\(\\beta_0\\) 1.2453 0.1176 10.59 0.0000 tratamiento \\(\\beta_1\\) -0.3636 0.1625 -2.24 0.0253 base \\(\\beta_2\\) 0.0209 0.0019 10.94 0.0000 tratamientoxbase \\(\\beta_3\\) 0.0009 0.0023 0.37 0.7083 Prueba de hipótesis: \\[ \\qquad H_0:\\beta_1=\\beta_3=0 \\] Prueba Estadístico g.l. Valor-p Razón de verosimilitud 10.405 2 0.005502 Score 10.418 2 0.005466 Wald 10.325 2 0.005726 6.4 intervalos de confianza A partir de los estadísticos de prueba anteriores se pueden encontrar intervalos de confianza para \\(\\hat{\\boldsymbol \\beta}\\). Por ejemplo usando el estadístico de Wald: \\[ \\hat{\\beta}\\pm z_{\\alpha/2}SE(\\hat{\\beta}) \\] Otra alternativa es a partir de los perfiles de log-verosimilitud, el IC está definido a partir de los valores de \\(\\beta_0\\) que satisfacen: \\[ -2[ℓ(\\beta_0,\\hat{\\psi}(\\beta_0))-ℓ(\\hat{\\boldsymbol \\beta,\\hat{\\boldsymbol \\psi}})]&lt;\\chi^2_1, \\] donde \\(\\psi\\) son los demás parámetros del modelo. 6.4.1 Intervalo de confianza para la media Dado que \\(\\hat{\\eta_0}=\\boldsymbol x_0&#39;\\hat{\\boldsymbol \\beta}\\) ,por lo tanto (asintóticamente): \\[ \\hat{\\eta_0}\\sim N[\\boldsymbol x_0&#39;\\boldsymbol \\beta,\\boldsymbol x_0&#39;\\boldsymbol I(\\boldsymbol \\beta)^{-1}\\boldsymbol x_0]. \\] Entonces un intervalo de confianza para \\(\\hat{\\eta_0}\\) es: \\[ \\hat{\\eta_0}\\pm z_{\\alpha/2}\\sqrt{\\boldsymbol x_0&#39;I(\\boldsymbol \\beta)^{-1}\\boldsymbol x_0} \\] Para encontrar el intervalo de confianza para \\(\\mu_i\\), se hace la transformación \\(g^{-1}\\) a los límites de confianza. 6.4.1.1 Ejemplo mortalidad de escarabajos summary(modBin) ## ## Call: ## glm(formula = cbind(dead, n - dead) ~ logdose, family = binomial) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5941 -0.3944 0.8329 1.2592 1.5940 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -60.717 5.181 -11.72 &lt;2e-16 *** ## logdose 34.270 2.912 11.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 284.202 on 7 degrees of freedom ## Residual deviance: 11.232 on 6 degrees of freedom ## AIC: 41.43 ## ## Number of Fisher Scoring iterations: 4 pred.link = predict(modBin,newdata = pred.x,type=&#39;link&#39;,se.fit = T) lim.sup = ilogit(pred.link$fit + qnorm(0.975)*pred.link$se.fit) lim.inf = ilogit(pred.link$fit - qnorm(0.975)*pred.link$se.fit) plot(logdose,dead/n,xlab=&#39;log dosis&#39;,ylab=&#39;proporción de escarabajos muertos&#39;,ylim=c(0,1)) lines(pred.x$logdose,pred) lines(pred.x$logdose,lim.sup, col=2) lines(pred.x$logdose,lim.inf, col=2) 6.4.1.2 Ataques epilépticos summary(modPois) ## ## Call: ## glm(formula = seizure.rate ~ base + treatment + base:treatment, ## family = poisson, data = epilepsy) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0482 -1.2217 -0.1162 0.5115 4.0478 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.2453159 0.1176157 10.588 &lt;2e-16 *** ## base 0.0209356 0.0019144 10.936 &lt;2e-16 *** ## treatmentProgabide -0.3635857 0.1624995 -2.237 0.0253 * ## base:treatmentProgabide 0.0008523 0.0022778 0.374 0.7083 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 476.25 on 58 degrees of freedom ## Residual deviance: 149.54 on 55 degrees of freedom ## AIC: 345.3 ## ## Number of Fisher Scoring iterations: 5 plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy, xlab=&quot;Num. ataques epiléptico(pre tratamiento)&quot;, ylab = &quot;Num. ataques epiléptico(4ta semana)&quot;) lines(x,predpois[order(y$treatment, y$base)[1:50]], lwd = 2) lines(x,predpois[order(y$treatment, y$base)[51:100]],col=2, lwd = 2) pred.link = predict(modPois,newdata = y,type=&#39;link&#39;,se.fit = T) lim.sup = exp(pred.link$fit + qnorm(0.975)*pred.link$se.fit) lim.inf = exp(pred.link$fit - qnorm(0.975)*pred.link$se.fit) lines(x,lim.sup[order(y$treatment, y$base)[1:50]], lty=2) lines(x,lim.inf[order(y$treatment, y$base)[1:50]], lty=2) lines(x,lim.sup[order(y$treatment, y$base)[51:100]],col=2, lty=2) lines(x,lim.inf[order(y$treatment, y$base)[51:100]],col=2, lty=2) Figure 6.5: Ajuste del modelo Poisson. Placebo(negro), Progabide(rojo) 6.5 Devianza Considere un GLM con observaciones \\(\\boldsymbol y_i=(y_1,...,y_n)\\). Sea \\(ℓ(\\boldsymbol \\mu)\\) la log-verosimilitud (expresada en función de \\(\\boldsymbol \\mu\\)). Por lo tanto, \\(ℓ(\\boldsymbol \\mu)\\) es la log-verosimilitud evaluada en el MLE. La máxima verosimilitud que se puede alcanzar corresponde a \\(ℓ(\\boldsymbol y)\\) (un modelo con ajuste perfecto \\(y_i=\\mu_i\\)). A este modelo, se le conoce como el modelo saturado. La idea es comparar el modelo propuesto contra el modelo saturado: \\[ D=-2[\\ell(\\hat{\\boldsymbol \\mu})-\\ell(\\boldsymbol y)] \\] Donde: \\(\\ell(\\boldsymbol y)=\\sum_{i=1}^n[y_i\\widetilde{\\theta}_i-b(\\widetilde{\\theta}_i)]/a(\\phi)\\). \\(\\ell(\\hat{\\boldsymbol \\mu})=\\sum_{i=1}^n[y_i\\hat{\\theta_i}-b(\\hat{\\theta_i})]/a(\\phi)\\). \\(\\widetilde{\\theta}_i\\) corresponde a la estimación de \\(\\theta_i\\) en el modelo saturado \\((\\widetilde{\\mu}_i=y_i)\\) Si el ajuste es pobre, se espera que \\(D\\) sea grande. Modelo binomial: \\[ D=2\\sum_{i=1}^n[y_ilog(\\frac{y_i}{\\hat{\\mu}_i})+(n_i-y_i)log(\\frac{n_i-y_i}{n_i-\\hat{\\mu}_i})], \\] donde \\(\\hat{\\mu}_i=n_i\\hat{\\pi}_i.\\) Modelo Poisson: \\[ D=2\\sum_{i=1}^ny_ilog(\\frac{y_i}{\\hat{\\mu}_i}), \\quad \\text{donde} \\quad \\hat{\\mu}_i=\\hat{\\lambda}_i. \\] Modelo normal: \\[ D=2\\sum_{i=1}^n(y_i-\\hat{\\mu}_i)^2. \\] 6.5.1 El estadístico chi-cuadrado de Pearson El estadístico chi-cuadrado de Pearson: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-\\hat{\\mu}_i)^2}{v(\\hat{\\mu}_i)}. \\] Para el modelo Binomial, tenemos que: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-n_i\\hat{\\pi}_i)^2}{n_i\\hat{\\pi}_i(1-\\hat{\\pi})}. \\] y para el modelo Poisson: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-\\hat{\\mu}_i)^2}{\\hat{\\mu}_i}. \\] 6.6 Bondad de ajuste Tanto \\(D\\) como \\(X^2\\) se pueden utilizar para evaluar la bondad del ajuste. \\(H_0\\) indica que el modelo ajusta bien a los datos, y \\(H_1\\) lo contrario. Por ejemplo, en el caso de un modelo logístico: \\[ H_0:logit(\\pi_i)=\\beta_0+\\sum_{i=1}^k\\beta_kx_{ik} \\] Si \\(H_0\\) es cierta, entonces \\(D\\) y \\(X_2\\) siguen una distribución \\(\\chi_2\\) con \\((n - p)\\) grados de libertadad*. *La aproximación es buena para datos agrupados. Mortalidad de escarabajos Los valores de la devianza y el \\(\\chi^2\\) de Pearson: # devianza D = deviance(modBin) 1-pchisq(D,6) # valor p ## [1] 0.08145881 # chi-cuadrado de Pearson X2 = sum(residuals(modBin,type=&#39;pearson&#39;)^2) 1-pchisq(X2,6) # valor p ## [1] 0.1235272 est gl valor-p Devianza 6 11.232 0.0814 \\(X^2\\) 6 10.026 0.1235 6.7 Pseudo \\(R^2\\) El pseudo \\(R^2\\) está definido como: \\[ \\qquad \\text{pseudo}R^2=\\frac{\\ell_M-\\ell_0}{\\ell_S-\\ell_0} \\] donde: \\(\\ell_M\\): log-versimilitud del modelo ajustado (evaluada en \\(\\hat{\\beta}\\). \\(\\ell_0\\): log-versimilitud del modelo nulo (solo con intercepto). \\(\\ell_S\\): log-versimilitud del modelo saturado. El pseudo \\(R^2\\) se encuentra entre 0 y 1. Se utiliza para comparar modelos. 6.8 Selección de variables La selección de variables se puede realizar de la misma forma que en los LMs basándose en indicadores como el AIC y BIC (o alguna modificación de ellos). AIC: \\[ \\text{AIC}=-2\\ell_M+2p. \\] BIC: \\[ \\text{BIC}=-2\\ell_M+plog(n). \\] "],["modelo-logístico-1.html", "Capítulo 7 Modelo logístico 7.1 Casos de estudio 7.2 Datos agrupados o datos no agrupados 7.3 Modelo logístico 7.4 Curva característica operativa del receptor(ROC) 7.5 Sobredispersión 7.6 Distribución beta-binomial", " Capítulo 7 Modelo logístico 7.1 Casos de estudio 7.1.1 Mortalidad de escarabajos Número de escarabajos muertos después de cinco horas de exposición a disulfuro de carbono gaseoso \\((CS2mgl^{-1})\\) en diversas concentraciones log.dosis Escarabajos Total Escarabajos Muertos 1.6907 59 6 1.7242 60 13 1.7552 62 18 1.7842 56 28 1.8113 63 52 1.8369 59 53 1.8610 62 61 1.8839 60 60 ¿Hay una relación entre la dosis y la mortalidad de escarabajos? logdose &lt;- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839) dead &lt;- c(6, 13, 18, 28, 52, 53, 61, 60) # numbers dead n &lt;- c(59, 60, 62, 56, 63, 59, 62, 60) # binomial sample sizes Datos=data.frame(logdose,n,dead) plot(logdose,dead/n,xlab=&#39;log dosis&#39;,ylab=&#39;proporción de muertos&#39;,ylim=c(0,1),pch=16) Figure 7.1: Datos escarabajos. Variable respuesta \\(y_i=\\sum_{j=1}^{n_i}y_{ij}/n_{i}\\): proporción de escarabajos muertos. Distribución de probabilidad: binomial: \\[ f(n_iy_i;\\pi_i)=\\begin{pmatrix} n_i \\\\ n_iy_i \\end{pmatrix} \\pi_i^{n_iy_i}(1-\\pi_i)^{(n_i-n_iy_i)}. \\] Función de enlace: \\[ log \\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\beta_0+\\beta_1log\\text{dosis}_i. \\] 7.1.2 Bajo peso al nacer Se busca identificar factores de riesgo asociados con el nacimiento de niños con bajo peso (\\(&lt;2.5\\) kgs). datos: data(birthwt,package= 'MASS') Muestra:189 madres atendidas en una clínica. Variable respuesta: Bajo peso al nacer (1 si peso \\(&lt;\\) 2.5 kgs, 0 si peso \\(\\geq\\) 2.5 kgs). Posibles covariables: age: edad de la madre (años). lwt: peso de la madre antes del embarazo (libras). smoke: estado de tabaquismo durante el embarazo (0 no, 1 si). ptl: historia de parto prematuro (número de casos). Variable respuesta \\(y_i\\): bajo peso al nacer (0: no, 1: si) Distribución de probabilidad: Bernoulli: \\[ f(y_i;\\pi_i)=\\pi_i^{y_i}(1-\\pi_i)^{(1-y_i)}. \\] Función de enlace: \\[ log\\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\beta_0+\\beta_1\\text{age}_i+\\beta_2\\text{lwt}_i+\\beta_3\\text{smoke}_i+\\beta_4\\text{ptl}_i. \\] 7.1.3 Estudio de teratología Se quiere investigar los efectos de agentes químicos en el desarrollo fetal de ratas. datos: data(lirat,package = 'VGAM'). Muestra: 58 ratas hembras con dietas deficientes en hierro. Variable respuesta: proporción de fetos muertos. Posibles covariables: grp: tratamiento (1: placebo, 2-4: diferentes concentraciones de inyecciones de suplementos de hierro) hb: nivel de hemoglobina. data(lirat,package = &#39;VGAM&#39;) par(mfrow=c(1,2)) #Proporcion de fetos muestos vs tratamiento plot(lirat$grp,lirat$R/lirat$N,xlab=&quot;Tratamiento&quot;,ylab=&quot;Proporción de fetos muertos&quot;) #Proporcion de fetos muestos vs Hemoglobina plot(lirat$hb,lirat$R/lirat$N,xlab=&quot;Hemoglobina&quot;,ylab=&quot;Proporción de fetos muertos&quot;) Figure 7.2: Datos teratoligía Variable respuesta \\(y_i=\\sum_{j=1}^{n_i}y_{ij}/n_i\\): proporción de fetos muertos por hembra. Distribución de probabilidad:binomial: \\[ f(n_iy_i;\\pi_i)=\\begin{pmatrix} n_i \\\\ n_iy_i \\end{pmatrix} \\pi_i^{n_iy_i}(1-\\pi_i)^{(n_i-n_iy_i)}. \\] Función de enlace: \\[ log\\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\beta_0+\\beta_1\\text{grp}_{i1}+\\beta_2\\text{grp}_{i2}+\\beta_3\\text{grp}_{i3}+\\beta_4\\text{hb}_i. \\] 7.2 Datos agrupados o datos no agrupados Datos agrupados: Hay \\(n_i\\) observaciones que tienen los mismos valores de las covariables \\(x_i\\). Datos no agrupados: Hay \\(n_i=1\\) (o muy pocas) observaciones por cada \\(x_i\\). Las propiedades asintóticas de las inferencias para los datos no agrupados aplican cuando \\(n \\to \\infty\\). Mientras que para datos agrupados, aplican cuando \\(\\sum_{i=1}^nn_i\\to\\infty\\). 7.2.1 Datos agrupados Para datos agrupados, \\(D\\) y \\(X^2\\) sirven para evaluar si el ajuste del modelo es bueno o no. \\(H_0\\) indica que el modelo se ajusta bien a los datos, \\(H_1\\) lo contrario. Si \\(H_0\\) es cierta (y \\(\\sum_{i=1}^nn_i\\to\\infty\\)), entonces \\(D\\) y \\(X^2\\) siguen una distribución \\(\\chi^2\\) con \\((n-p)\\) grados de libertad. 7.2.2 Datos no agrupados Las distribuciones límite para \\(D\\) y \\(X^2\\) no aplican para datos no agrupados. Tampoco para datos agrupados con \\(n\\) grande y con algunos \\(n_i\\) muy pequeños. Se puede aproximar \\(D\\) y \\(X^2\\) agrupando \\((\\boldsymbol x_i,\\hat{\\boldsymbol y})\\) por particiones del espacio de covariables o por particiones de \\(\\hat{\\pi}\\). La falta de ajuste se puede hacer comparando el modelo propuesto contra modelos más generales. 7.3 Modelo logístico Modelo: \\[ n_iy_i\\sim\\text{binomial}(n_i,\\pi_i),\\quad \\text{donde} \\quad \\pi_i=g(\\boldsymbol x_i\\boldsymbol \\beta). \\] Entonces: \\(E(y_i|\\boldsymbol x_i)=\\pi_i\\) y \\(V(y_i|\\boldsymbol x_i)=\\pi_i(1-\\pi_i)/n_i\\). Función de enlace logit: \\(log\\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\boldsymbol x_i\\boldsymbol \\beta\\). Lo que implica que: \\[ \\pi_i=g^{-1}(\\boldsymbol x_i,\\boldsymbol \\beta)=\\frac{exp(\\boldsymbol x_i&#39;\\boldsymbol \\beta)}{1+exp(\\boldsymbol x_i&#39;\\boldsymbol \\beta)}=\\frac{1}{1+exp(-\\boldsymbol x_i&#39;\\boldsymbol \\beta)} \\] Aunque podemos utilizar otras funciones de enlace. 7.3.1 Funciones de enlace alternativas Probit: \\[ \\Phi^{-1}(\\pi_i)=\\boldsymbol x&#39;_i\\boldsymbol \\beta\\quad \\pi_i=\\Phi(\\boldsymbol x&#39;_i\\boldsymbol \\beta), \\] donde \\(\\Phi(\\cdot)\\) es función acumulativa de la distribución normal estándar. Log-log complementaria: \\[ \\pi_i=1-exp[-exp(\\boldsymbol x&#39;_i\\boldsymbol \\beta)] \\quad log[-log(1-\\pi_i)]=\\boldsymbol x&#39;_i\\boldsymbol \\beta. \\] Log-log: \\[ \\pi_i=exp[-exp(\\boldsymbol x&#39;_i\\boldsymbol \\beta)] \\quad -log[-log(\\pi_i)]=\\boldsymbol x&#39;_i\\boldsymbol \\beta. \\] Figure 7.3: Estimaciones con funciones de enlace: logit(negro), probit(rojo) y cloglog(verde). logit porbit cloglog Effect Parm est. s.e est. s.e est. s.e Intercept \\(\\beta_0\\) -60.72 5.18 -34.94 2.65 -39.57 3.24 Dosis \\(\\beta_1\\) 34.27 2.91 19.73 1.49 22.04 180 log.lik -18.71 -18.16 -14.82 AIC 41.43 40.32 33.64 modbw.logit = glm(low~age+lwt+ptl+smoke,family=binomial(logit),data=birthwt) summary(modbw.logit) ## ## Call: ## glm(formula = low ~ age + lwt + ptl + smoke, family = binomial(logit), ## data = birthwt) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9339 -0.8646 -0.6816 1.2182 1.9904 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.275961 1.031063 1.238 0.2159 ## age -0.048859 0.033796 -1.446 0.1483 ## lwt -0.010412 0.006219 -1.674 0.0941 . ## ptl 0.695720 0.336895 2.065 0.0389 * ## smoke 0.550111 0.334455 1.645 0.1000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 234.67 on 188 degrees of freedom ## Residual deviance: 218.36 on 184 degrees of freedom ## AIC: 228.36 ## ## Number of Fisher Scoring iterations: 4 modbw.probit = glm(low~age+lwt+smoke+ptl,family=binomial(probit),data=birthwt) summary(modbw.probit) ## ## Call: ## glm(formula = low ~ age + lwt + smoke + ptl, family = binomial(probit), ## data = birthwt) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9017 -0.8663 -0.6763 1.2301 1.9996 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.749466 0.603409 1.242 0.2142 ## age -0.030224 0.019886 -1.520 0.1286 ## lwt -0.006079 0.003576 -1.700 0.0891 . ## smoke 0.342834 0.201795 1.699 0.0893 . ## ptl 0.406405 0.202487 2.007 0.0447 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 234.67 on 188 degrees of freedom ## Residual deviance: 218.24 on 184 degrees of freedom ## AIC: 228.24 ## ## Number of Fisher Scoring iterations: 5 7.4 Curva característica operativa del receptor(ROC) Para evaluar el poder predictivo del modelo se puede construir una tabla de contingencia: Table 7.1: Tabla de clasificación y 0 1 0 1 El recuento de casillas en estas tablas permite estimar la sensibilidad=\\((\\hat y=1|y=1)\\) y especificidad=\\((\\hat y=0|y=0)\\). Para datos no agrupados, la predicción \\(\\hat{y}=1\\) cuando \\(\\hat{\\pi_i}\\geq\\pi_0\\)(por ejemplo 0,5). Para \\(\\pi_0\\), se calcula: La sensibilidad \\(P(\\hat y=1|y=1)\\). La especificidad \\(P(\\hat y=0|y=0)\\). Dado que estos valores dependen de \\(\\pi_0\\), se pueden calcular para todos los posibles \\(\\pi_0\\). Ejemplo - Datos de peso al nacer Por ejemplo, si \\(\\pi_0\\), tenemos: \\(\\hat{y}=0\\) \\(\\hat{y}=1\\) y 0 1 \\(y=0\\) 123 7 \\(y=1\\) 49 10 Sensibilidad: \\(10/59 = 0,169\\) - especificidad: \\(123/130 = 0,946\\). Ahora, si \\(\\pi_0= 0,3\\): \\(\\hat{y}=0\\) \\(\\hat{y}=1\\) y 0 1 \\(y=0\\) 79 51 \\(y=1\\) 19 40 Sensibilidad: \\(40/59 = 0,678\\) - especificidad: \\(79/130 = 0,607\\). Figure 7.4: Sensibilidad (linea negra) - especificidad (linea roja) Gráfico de \\(P(\\hat y=1|y=1)\\) vs \\(P(\\hat y=0|y=0)\\) El area bajo la curva de la curva ROC (llamado índice de concordancia) es una medida del poder predictivo del modelo. 7.4.1 Ejemplo mortalidad de escarabajos Curva ROC para el modelo logístico: “grafico roc escarabajos” 7.4.2 Bajo peso al nacer modbw.logit = glm(low~age+lwt+ptl+smoke,family=binomial(logit),data=birthwt) modbw.probit = glm(low~age+lwt+smoke+ptl,family=binomial(probit),data=birthwt) # curvas roc #library(pROC) ROCbw.logit = roc(birthwt$low~modbw.logit$fitted.values) ROCbw.probit = roc(birthwt$low~modbw.probit$fitted.values) plot(ROCbw.logit) lines(ROCbw.probit,col=2) Figure 7.5: Curva ROC para el modelo logístico (negro) y probit (rojo) modelo logístico: \\(AUC= 0,6884\\). modelo probit: \\(AUC= 0,6866\\). 7.5 Sobredispersión En un experimento Bernoulli se asume que los \\(n_i\\) ensayos para la observación \\(i,\\quad y_{i1},...y_in_i\\) son independientes. Por lo tanto, tenemos que: \\[ E(y_i)=\\pi_i\\quad \\text{y} \\quad V(y_i)=\\pi_i(1-\\pi_i)\\frac{1}{n_i}. \\] Pero, hay casos donde las observaciones \\(y_{i1},...y_in_i\\) están correlacionadas, es decir \\(cor(y_{is},y_{it})\\ne0\\). Por lo general, se asume que \\(cor(y_{is},y_{it})=\\phi\\), para todo \\(s\\ne t\\) (exchangeability property). Entonces: \\[ V(y_{it})=\\pi_i(1-\\pi_i)\\quad \\text{y} \\quad cor(y_{it},y_{is})=\\phi\\pi_i(1-\\pi_i). \\] En este caso, tenemos que: \\[ V(y_i)=V\\begin{pmatrix} \\sum_{j=1}^{n_i}\\frac{y_{ij}}{n_i}\\end{pmatrix}=\\frac{1}{n_i^2} \\left[ \\sum_{j=1}^{n_i}V(y_{it})+2\\sum\\sum_{s&lt;t}cov(y_{is},y_{it}) \\right]=[1+\\phi(n_i-1)]\\frac{\\pi_i(1-\\pi_i)}{n_i}. \\] Si: \\(\\phi=0\\), \\(y_i\\sim binomial(n_i,\\pi)\\). \\(\\phi&gt;0\\), tenemos sobredispersión. \\(-(n_i-1)^{-1}&lt;\\phi&lt;0\\), tenemos subdispersión (menos frecuente). En el caso del modelo binomial, tenemos que: \\[ v(\\pi_i)=\\pi_i(1-\\pi_i)/n_i. \\] El estadístico de \\(\\chi^2\\) de Pearson es: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-\\hat{\\pi}_i)^2}{\\pi_i(1-\\pi_1)/n_i} \\] Un indicador de posible inflación de varianza es: \\[ \\hat{\\phi}=\\frac{X^2}{n-p}. \\] Si no hay problemas de sobredispersión \\(\\hat{\\phi}\\approx1\\). 7.6 Distribución beta-binomial Modelo: \\(y|\\pi\\sim binomial(n,\\pi)\\) Donde \\(\\pi\\sim beta(\\alpha_1,\\alpha_2)\\), esto es: \\[ f(\\pi;\\alpha_1,\\alpha_2)=\\frac{\\Gamma(\\alpha_1+\\alpha_2)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}\\pi^{\\alpha_1-1}(1-\\pi)^{\\alpha_2-1} \\] con \\(\\alpha_1&gt;0\\) y \\(\\alpha_2&gt;0\\). Asumiendo \\(\\mu=\\frac{\\alpha_1}{\\alpha_1+\\alpha_2}\\) y \\(\\theta=1/(\\alpha_1+\\alpha_2)\\), tenemos: \\[ E(\\pi)=\\mu\\quad\\text{y}\\quad V(\\pi)=\\mu(1-\\mu)\\frac{\\theta}{1+\\theta}. \\] La distribución beta-binomial se obtiene al marginalizar \\(y\\). Esto es: \\[ f(y;n,\\mu,\\theta)=\\int f(y|\\pi)f(\\pi)d\\pi. \\] La función de densidad de la distribución beta-binomial es: \\[ f(y;n,\\mu,\\theta)=\\begin{pmatrix} n \\\\ y \\end{pmatrix}\\frac{\\begin{bmatrix} \\Pi_{k=0}^{y-1}(\\mu+k\\theta)\\end{bmatrix} \\begin{bmatrix} \\Pi_{k=0}^{n-y-1}(1-\\mu+k\\theta)\\end{bmatrix}}{\\begin{bmatrix} \\Pi_{k=0}^{n-1}(1+k\\theta) \\end{bmatrix}}, \\] para \\(y=0,1,...,n.\\) Valor esperado y varianza de \\(y=s/n\\): \\[ E(y)=\\mu\\quad\\text{y}\\quad V(y)=\\begin{bmatrix} 1+(n-1)\\frac{\\theta}{1+\\theta} \\end{bmatrix}\\mu(1-\\mu)\\frac{1}{\\mu}. \\] Por lo cual, \\(\\phi=\\theta/(1+\\theta)\\) es la correlación entre ensayos Bernoulli. Binomial(15,0.3)(negro), beta-binomial(15,0.3,\\(\\phi\\)=0,1)(rojo), beta-binomial(15,0.3,\\(\\phi\\)=0,3)(verde) 7.6.1 Modelo beta-binomial Modelo: \\[ n_iy_i|\\pi_i\\sim binomial(n_i,\\pi_i)\\\\ \\pi_i\\sim beta(\\mu_i,\\phi), \\] Por lo cuál: \\[ E(y_i)=\\mu_i\\quad\\text{y}\\quad V(y_i)=[1+(n-1)\\phi]\\mu_i(1-\\mu_i)/n_i \\] La estimación de los parámetros \\((\\beta,\\phi)\\) se hace por máxima verosimilitud. Dado que \\(\\phi&gt;0)\\), el modelo beta-binomial no puede modelar datos con subdispersión. 7.6.1.1 Estudio de teratología data(lirat,package = &#39;VGAM&#39;) modlirat.binom = glm(cbind(R,N-R)~hb+as.factor(grp),family=binomial,data=lirat) summary(modlirat.binom) ## ## Call: ## glm(formula = cbind(R, N - R) ~ hb + as.factor(grp), family = binomial, ## data = lirat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.6331 -0.9684 -0.0471 1.3945 2.8256 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1795 0.5056 4.311 1.63e-05 *** ## hb -0.2190 0.1020 -2.148 0.031728 * ## as.factor(grp)2 -2.4748 0.5045 -4.906 9.30e-07 *** ## as.factor(grp)3 -3.1527 0.9433 -3.342 0.000832 *** ## as.factor(grp)4 -2.0520 1.0629 -1.931 0.053525 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 509.43 on 57 degrees of freedom ## Residual deviance: 168.91 on 53 degrees of freedom ## AIC: 250.38 ## ## Number of Fisher Scoring iterations: 5 modlirat.betabinom =betabin(cbind(R,N-R)~as.factor(grp)+hb,data=lirat,random=~1) modlirat.betabinom ## Beta-binomial model ## ------------------- ## betabin(formula = cbind(R, N - R) ~ as.factor(grp) + hb, random = ~1, ## data = lirat) ## ## Convergence was obtained after 433 iterations. ## ## Fixed-effect coefficients: ## Estimate Std. Error z value Pr(&gt; |z|) ## (Intercept) 2.130e+00 8.676e-01 2.455e+00 1.408e-02 ## as.factor(grp)2 -2.441e+00 8.508e-01 -2.869e+00 4.113e-03 ## as.factor(grp)3 -2.836e+00 1.329e+00 -2.134e+00 3.287e-02 ## as.factor(grp)4 -2.285e+00 1.817e+00 -1.258e+00 2.085e-01 ## hb -1.694e-01 1.772e-01 -9.557e-01 3.392e-01 ## ## Overdispersion coefficients: ## Estimate Std. Error z value Pr(&gt; z) ## phi.(Intercept) 2.356e-01 6.041e-02 3.9e+00 4.813e-05 ## ## Log-likelihood statistics ## Log-lik nbpar df res. Deviance AIC AICc ## -9.302e+01 6 52 1.146e+02 1.98e+02 1.997e+02 AIC(modlirat.binom) ## [1] 250.3777 AIC(modlirat.betabinom) ## df AIC AICc ## modlirat.betabinom 6 198.0319 199.6789 deviance(modlirat.binom) ## [1] 168.9083 deviance(modlirat.betabinom) ## [1] 114.5625 Modelo Deviance AIC BIC Binomial 168.908 250.378 260.680 Beta-Binomial 114.563 198.032 210.395 "],["modelo-para-conteos.html", "Capítulo 8 Modelo para conteos 8.1 Casos de estudio 8.2 Modelo Poisson 8.3 Distribución binomial negativa 8.4 Modelo Poisson con ceros inflados 8.5 Modelo Hurdle", " Capítulo 8 Modelo para conteos 8.1 Casos de estudio 8.1.1 Ataques de epilepsia Ensayo clínico para evaluar el impacto de progabida sobre las crisis epilépticas (data(epilepsy) de la librería HSAUR2). Datos: age: edad del paciente. base: número de ataques epilépticos (x 8 semanas) antes delensayo. treatment: tratamiento (placebo, progabida). seizure.rate(variable respuesta): número de ataques epilépticos (x dos semanas) luego de 8 semanas. 8.1.2 Datos de cáncer de pulmón Incidencia de cáncer de pulmón en cuatro ciudades de Dinamarca entre 1968 y 1971 (data(eba1977,package='ISwR')). Variable respuesta: número de casos de cáncer de pulmón. Covariables: Ciudad (Fredericia, Horsens, Kolding, Vejle). Grupo de edad (40-54, 55-59, 60-64, 65-69, 70-74, &gt;75). Dado que el número de casos depende del tamaño de la población, se modela la tasa de casos de cáncer de pulmón: \\(Y_i/t_i\\). \\(t_i\\) es el tamaño de la población por grupo de edad y ciudad (offset). cancer.data = eba1977[order(eba1977$city),] # casos por 1000 habitantes tasa = 1000*cancer.data$cases/cancer.data$pop plot(1:6,tasa[cancer.data$city==&#39;Fredericia&#39;],type=&#39;b&#39;,xlab = &#39;grupo de edad&#39;, ylim=range(tasa),ylab=&#39;casos por 1000 habitantes&#39;,xaxt=&#39;n&#39;,lwd=2) axis(1,1:6,c(&#39;40-54&#39;,&#39;55-59&#39;,&#39;60-64&#39;,&#39;65-69&#39;,&#39;70-74&#39;,&#39;&gt;75&#39;)) lines(1:6,tasa[cancer.data$city==&#39;Horsens&#39;],col=2,type=&#39;b&#39;,lwd=2) lines(1:6,tasa[cancer.data$city==&#39;Kolding&#39;],col=3,type=&#39;b&#39;,lwd=2) lines(1:6,tasa[cancer.data$city==&#39;Vejle&#39;],col=4,type=&#39;b&#39;,lwd=2) Figure 8.1: Fredericia(línea negra), Horsens(línea roja), Kolding(línea verde), Vejle(línea azul). 8.1.3 Número de cangrejos satélites Los datos crabs de la librería asbio son de un estudio de hembras de cangrejos herradura durante el periodo de desove en una isla en el Golfo de México. Variable respuesta:el número de “satélites de cangrejos” herradura hembras (\\(n = 173\\)). Las posibles covariables son: color: color (1, medio-claro; 2, medio; 3, medio-oscuro; 4,oscuro). spine: condición de la espina dorsal (1, ambos bien; 2, uno gastado o roto; 3, ambos gastados o rotos). width: ancho del caparazón (cm). weight: peso (kg) par(mfrow=c(1,2)) hist(crabs$satell,breaks = 20,main=&#39;&#39;,xlab=&#39;número de satálites&#39;,ylab=&#39;frecuencia&#39;) plot(crabs$satell~weight,data=crabs,xlab=&#39;peso (kg)&#39;,ylab=&#39;número de satálites&#39;) Figure 8.2: Histograma(izquierda), número de satélites vs peso (derecha) 8.2 Modelo Poisson La distribución Poisson se utiliza para modelar variables de tipo conteo (número de eventos por intervalo de tiempo o espacio). Modelo: \\[ y_i\\sim Poisson(\\lambda_i), \\quad i=1,...,n,\\quad \\text{donde} \\quad \\lambda_i=exp(\\boldsymbol x&#39;_i\\boldsymbol \\beta), \\] Por lo tanto: \\(E(y_i|\\boldsymbol x_i)=V(y_i|\\boldsymbol x_i)=\\lambda_i\\). El modelo Poisson también se puede usar como aproximación al modelo binomial cuando \\(n_i\\) es grande y \\(\\pi_i\\) es pequeño, con \\(\\mu_i=n_i\\pi_i\\). Los conteos \\(y_i\\) son proporcionales a un índice \\(t_i\\) (intervalo de tiempo, área de espacio, tamaño de población). Cuando \\(t_i\\) no es fijo, se debe incluir un término llamado offset. El modelo Poisson asume que \\(E(y_i|\\boldsymbol x_i) = V (y_i|\\boldsymbol x_i)\\). En la práctica, es común encontrar que la varianza es mayor que la media que predice la Poisson (sobredispersión). En otros casos, la frecuencia de ceros puede ser mayor de lo esperado en el modelo ajustado (exceso de ceros). 8.2.1 Modelo de conteo con offset El valor esperado de una variable de de conteo \\(y_i\\) es proporcional a un índice \\(t_i\\). Por lo que se puede modelar la tasa \\((y_i/t_i)\\) con valor esperado \\(\\lambda_i/t_i\\). En este caso, el predictor lineal queda de la forma: \\[ log\\begin{pmatrix}\\frac{\\lambda_i}{t_i}\\end{pmatrix}=\\boldsymbol x&#39;_i\\boldsymbol \\beta. \\\\ log\\lambda_i=\\boldsymbol x_i&#39;\\boldsymbol \\beta+logt_i, \\] donde \\(logt_i\\) es llamado offset. Por lo tanto, el valor esperado de \\(y_i\\) es: \\[ \\lambda_i=t_iexp(x_i\\beta). \\] 8.2.1.1 Datos de cáncer de pulmón Modelo 1: \\[ log(\\mu_i/\\text{pop}_i)=\\beta_0+\\beta_1\\text{Horsens}_i+\\beta_2\\text{Kolding}_i+\\beta_3\\text{Vejle}_i+\\beta_4I\\text{(55-59)}_i+\\beta_5I\\text{(60-64)}_i+\\beta_6I\\text{(65-69)}_i+\\beta_7I\\text{(70-74)}_i+\\beta_8I\\text{(&gt;75)}_i \\] poisson.model = glm(cases ~ city + age+ offset(log(pop)), family = poisson(link = &quot;log&quot;), data = cancer.data) summary(poisson.model) ## ## Call: ## glm(formula = cases ~ city + age + offset(log(pop)), family = poisson(link = &quot;log&quot;), ## data = cancer.data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.63573 -0.67296 -0.03436 0.37258 1.85267 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.6321 0.2003 -28.125 &lt; 2e-16 *** ## cityHorsens -0.3301 0.1815 -1.818 0.0690 . ## cityKolding -0.3715 0.1878 -1.978 0.0479 * ## cityVejle -0.2723 0.1879 -1.450 0.1472 ## age55-59 1.1010 0.2483 4.434 9.23e-06 *** ## age60-64 1.5186 0.2316 6.556 5.53e-11 *** ## age65-69 1.7677 0.2294 7.704 1.31e-14 *** ## age70-74 1.8569 0.2353 7.891 3.00e-15 *** ## age75+ 1.4197 0.2503 5.672 1.41e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 129.908 on 23 degrees of freedom ## Residual deviance: 23.447 on 15 degrees of freedom ## AIC: 137.84 ## ## Number of Fisher Scoring iterations: 5 Devianza= 23:447 y AIC= 137:84. # ajuste del modelo plot(1:6,tasa[cancer.data$city==&#39;Fredericia&#39;],xlab = &#39;grupo de edad&#39;, ylim=range(tasa),ylab=&#39;casos por 1000 habitantes&#39;,xaxt=&#39;n&#39;) axis(1,1:6,c(&#39;40-54&#39;,&#39;55-59&#39;,&#39;60-64&#39;,&#39;65-69&#39;,&#39;70-74&#39;,&#39;&gt;75&#39;)) points(1:6,tasa[cancer.data$city==&#39;Horsens&#39;],col=2) points(1:6,tasa[cancer.data$city==&#39;Kolding&#39;],col=3) points(1:6,tasa[cancer.data$city==&#39;Vejle&#39;],col=4) pred.tasa = 1000*poisson.model$fitted.values/cancer.data$pop lines(1:6,pred.tasa[cancer.data$city==&#39;Fredericia&#39;],type=&#39;l&#39;,lwd=2) lines(1:6,pred.tasa[cancer.data$city==&#39;Horsens&#39;],col=2,type=&#39;l&#39;,lwd=2) lines(1:6,pred.tasa[cancer.data$city==&#39;Kolding&#39;],col=3,type=&#39;l&#39;,lwd=2) lines(1:6,pred.tasa[cancer.data$city==&#39;Vejle&#39;],col=4,type=&#39;l&#39;,lwd=2) Figure 8.3: Fredericia (línea negra), Horsens (línea roja), Kolding (línea verde), Vejle (línea azul). Modelo 2: \\[ log(\\mu_i/\\text{pop}_i)=\\beta_0+\\beta_1\\text{Fredericia}_i+\\beta_2\\text{edad}_i^*+\\beta_3\\text{edad}_i^{*2}, \\] donde \\(\\text{edad}_i^{*2}\\) es el punto medio de los grupos de edad. cancer.data$t = c(47,57,62,67,72,77) cancer.data$Fredericia = as.double(cancer.data$city==&#39;Fredericia&#39;) poisson.model2 = glm(cases ~ Fredericia + t + I(t^2)+ offset(log(pop)), family = poisson(link = &quot;log&quot;), data = cancer.data) summary(poisson.model2) ## ## Call: ## glm(formula = cases ~ Fredericia + t + I(t^2) + offset(log(pop)), ## family = poisson(link = &quot;log&quot;), data = cancer.data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.91887 -0.50366 -0.04555 0.45458 1.98780 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.154e+01 2.784e+00 -7.738 1.01e-14 *** ## Fredericia 3.288e-01 1.480e-01 2.221 0.0263 * ## t 5.002e-01 9.062e-02 5.520 3.39e-08 *** ## I(t^2) -3.609e-03 7.263e-04 -4.969 6.72e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 129.908 on 23 degrees of freedom ## Residual deviance: 25.364 on 20 degrees of freedom ## AIC: 129.75 ## ## Number of Fisher Scoring iterations: 4 Prueba de razón de verosimilitud: anova(poisson.model2,poisson.model,test=&#39;LRT&#39;) ## Analysis of Deviance Table ## ## Model 1: cases ~ Fredericia + t + I(t^2) + offset(log(pop)) ## Model 2: cases ~ city + age + offset(log(pop)) ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 20 25.364 ## 2 15 23.448 5 1.9161 0.8606 plot(c(47,57,62,67,72,77),tasa[cancer.data$city==&#39;Fredericia&#39;],xlab = &#39;grupo de edad&#39;, ylim=range(tasa),ylab=&#39;casos por 1000 habitantes&#39;) points(c(47,57,62,67,72,77),tasa[cancer.data$city==&#39;Horsens&#39;],col=2) points(c(47,57,62,67,72,77),tasa[cancer.data$city==&#39;Kolding&#39;],col=2) points(c(47,57,62,67,72,77),tasa[cancer.data$city==&#39;Vejle&#39;],col=2) pred.tasa = 1000*poisson.model2$fitted.values/cancer.data$pop lines(c(47,57,62,67,72,77),pred.tasa[cancer.data$city==&#39;Fredericia&#39;],type=&#39;l&#39;,lwd=2) lines(c(47,57,62,67,72,77),pred.tasa[cancer.data$city==&#39;Horsens&#39;],col=2,type=&#39;l&#39;,lwd=2) Figure 8.4: Fredericia (línea negra), otra ciudad (línea roja). 8.3 Distribución binomial negativa Modelo: \\(y|\\lambda\\sim Poisson(\\lambda)\\), donde \\(\\lambda\\sim gamma(k,\\mu).\\) Esto es: \\[ f(\\lambda;k,\\mu)=\\frac{(k/\\mu)^k}{\\Gamma(k)}exp(-k\\lambda/\\mu)\\lambda^{k-1}, \\] Entonces tenemos: \\[ E(\\lambda)=\\mu\\quad\\text{y}\\quad V(\\lambda)=\\mu^2/k \\] La distribución binomial negativa (Poisson-gamma) se obtiene al marginalizar \\(y\\). Esto es: \\[ f(y;\\mu,k)=\\int f(y|\\lambda)f(\\lambda)d\\lambda. \\] La función de densidad de la binomial negativa es: \\[ f(y;\\mu,k)=\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\begin{pmatrix} \\frac{\\mu}{\\mu+k}\\end{pmatrix}^y \\begin{pmatrix} \\frac{k}{\\mu+k} \\end{pmatrix}^k \\] para \\(y=0,1,...\\) Si definimos \\(\\gamma=1/k\\), tenemos que: \\[ E(y)=\\mu\\quad\\text{y}\\quad V(y)=\\mu(1+\\gamma\\mu), \\] para \\(\\gamma&gt;0\\) (parámetro de sobredispersión). 8.3.1 Modelo binomial negativo (Poisson-gamma) Modelo: \\[ y_i|\\lambda_i\\sim Poisson(\\lambda_i)\\\\ \\lambda_i\\sim gamma(\\mu_i,\\gamma), \\] donde \\[ \\lambda_i=exp(\\boldsymbol x&#39;_i\\boldsymbol \\beta). \\] Por lo cuál: \\[ E(y_i)=\\mu_i\\quad\\text{y}\\quad V(y_i)=\\mu_i(1+\\gamma\\mu_i) \\] La estimación de los parámetros \\((\\boldsymbol \\beta,\\gamma)\\) se hace por máxima verosimilitud. Dado que \\(\\gamma&gt;0\\), el modelo binomial negativo no puede modelar datos con subdispersión. Poisson(3) (negro) - Poisson-gamma(\\(3,\\gamma=0.5\\)) (rojo) - Poisson-gamma(\\(3,\\gamma=2\\)) (verde) 8.3.1.1 Ataques epilépticos epilepsy4= epilepsy[epilepsy$period==4,] modPois = glm(seizure.rate~treatment+base,data=epilepsy4,family=poisson) summary(modPois) ## ## Call: ## glm(formula = seizure.rate ~ treatment + base, family = poisson, ## data = epilepsy4) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.1373 -1.2275 -0.0668 0.5381 4.0197 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.214772 0.085543 14.201 &lt; 2e-16 *** ## treatmentProgabide -0.315159 0.098469 -3.201 0.00137 ** ## base 0.021536 0.001039 20.733 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 476.25 on 58 degrees of freedom ## Residual deviance: 149.68 on 56 degrees of freedom ## AIC: 343.44 ## ## Number of Fisher Scoring iterations: 5 #library(MASS) modbinNeg = glm.nb(seizure.rate~treatment+base,data=epilepsy4) summary(modbinNeg) ## ## Call: ## glm.nb(formula = seizure.rate ~ treatment + base, data = epilepsy4, ## init.theta = 4.483069947, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.36228 -0.80991 0.02734 0.41451 2.55898 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.080878 0.153419 7.045 1.85e-12 *** ## treatmentProgabide -0.327892 0.168163 -1.950 0.0512 . ## base 0.025017 0.002688 9.308 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(4.4831) family taken to be 1) ## ## Null deviance: 174.166 on 58 degrees of freedom ## Residual deviance: 70.124 on 56 degrees of freedom ## AIC: 313.32 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 4.48 ## Std. Err.: 1.55 ## ## 2 x log-likelihood: -305.324 Modelo Devianza g.l. AIC Poisson 149.680 56 343.44 Binomial-negativa 70.124 56 313.32 8.4 Modelo Poisson con ceros inflados El modelo Poisson con ceros inflados (ZIP): \\[ y_i=\\begin{cases} 0 \\qquad &amp; \\text{con probabilidad}\\quad 1-\\phi_i \\\\ Poisson(\\lambda_i) \\quad &amp; \\text{con probabilidad}\\quad \\phi_i\\end{cases} \\] La distribución de probabilidad incondicional es: \\[ P(y_i=0)=(1-\\phi_i)+\\phi_iexp(-\\lambda_i) \\] \\[ P(y_i=j)=\\phi_i\\frac{exp(-\\lambda_i)\\lambda_i^j}{j!},\\quad \\text{para}\\quad j=1,2,... \\] Los parámetros \\(\\phi_i\\) y \\(\\lambda_i\\) pueden ser modelados a través de covariables: \\[ logit\\phi_i=\\boldsymbol x&#39;_{1i}\\boldsymbol \\beta_1 \\quad\\text{y}\\quad log\\log\\lambda_i\\boldsymbol x&#39;_{2i}\\boldsymbol \\beta_2 \\] Poisson(3) (negro), ZI-Poisson(\\(3,\\phi=0.9\\))(rojo), Poisson-gamma(\\(3,\\phi=0.7\\))(verde). El valor esperado y varianza de \\(y_i\\) son: \\[ E(y_i)=\\phi_i\\lambda_i\\quad\\text{y}\\quad V(y_i)=\\phi_i\\lambda_i[1+(1-\\phi_i)\\lambda_i]. \\] Dado que \\(E(y_i)&lt;V(y_i)\\) el modelo ZIP tiene en cuenta algo de sobredispersión. En presencia de una sobredispersión mayor, el modelo de ceros inflados puede combinarse con una distribución binomial negativa. Esto es: \\[ y_i=\\begin{cases} 0 &amp; \\text{con probabilidad} \\quad1-\\phi_i \\\\ \\text{binomial negativa}(\\lambda_i,\\gamma)&amp; \\text{con probabilidad} \\quad\\phi_i \\end{cases} \\] 8.5 Modelo Hurdle El modelo Hurdle: \\[ P(Y_i=j)=\\begin{cases} 1-\\pi_i &amp; \\text{si}\\quad j=0,\\\\ \\pi_i\\frac{f(j;\\mu_i)}{1-f(0;\\mu_i)} &amp;\\text{si}\\quad j=1,2,... \\end{cases} \\] La función \\(f(\\cdot;\\mu_i)\\) puede ser una Poisson o binomial negativa. \\(\\pi_i\\) se puede modelar usando un modelo logístico y un modelo log-lineal para \\(\\mu_i\\): \\[ logit\\phi_i=\\boldsymbol x&#39;_{1i}\\boldsymbol \\beta_1\\quad\\text{y}\\quad log\\lambda_i=\\boldsymbol x&#39;_{2i}\\boldsymbol \\beta_2. \\] 8.5.1 Número de cangrejos satélites # modelo Poisson Pois.sat = glm(satell~weight,data = crabs,family = poisson) summary(Pois.sat) ## ## Call: ## glm(formula = satell ~ weight, family = poisson, data = crabs) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9307 -1.9981 -0.5627 0.9298 4.9992 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.42841 0.17893 -2.394 0.0167 * ## weight 0.58930 0.06502 9.064 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 632.79 on 172 degrees of freedom ## Residual deviance: 560.87 on 171 degrees of freedom ## AIC: 920.16 ## ## Number of Fisher Scoring iterations: 5 # modelo binomial negativo NB.sat = glm.nb(satell~weight,data=crabs) summary(NB.sat) ## ## Call: ## glm.nb(formula = satell ~ weight, data = crabs, init.theta = 0.9310592338, ## link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8394 -1.4122 -0.3247 0.4744 2.1279 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8647 0.4048 -2.136 0.0327 * ## weight 0.7603 0.1578 4.817 1.45e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.9311) family taken to be 1) ## ## Null deviance: 216.43 on 172 degrees of freedom ## Residual deviance: 196.16 on 171 degrees of freedom ## AIC: 754.64 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.931 ## Std. Err.: 0.168 ## ## 2 x log-likelihood: -748.644 #library(pscl) # modelo de inflación de cero ZINB.sat1 = zeroinfl(satell~weight | weight,data=crabs,dist=&#39;negbin&#39;) summary(ZINB.sat1) ## ## Call: ## zeroinfl(formula = satell ~ weight | weight, data = crabs, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.3647 -0.7899 -0.3112 0.5086 3.8908 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.8979 0.3053 2.941 0.00327 ** ## weight 0.2171 0.1119 1.941 0.05229 . ## Log(theta) 1.6013 0.3553 4.507 6.57e-06 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.7565 0.9841 3.817 0.000135 *** ## weight -1.9131 0.4322 -4.426 9.59e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 4.9595 ## Number of iterations in BFGS optimization: 8 ## Log-likelihood: -352.8 on 5 Df # modelo hurdle HNB.sat = hurdle(satell~weight | weight,data=crabs,dist=&#39;negbin&#39;) summary(HNB.sat) ## ## Call: ## hurdle(formula = satell ~ weight | weight, data = crabs, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.3768 -0.7890 -0.3133 0.5121 3.8869 ## ## Count model coefficients (truncated negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.9131 0.3088 2.957 0.00311 ** ## weight 0.2104 0.1136 1.852 0.06399 . ## Log(theta) 1.5782 0.3593 4.392 1.12e-05 *** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.6947 0.8802 -4.198 2.70e-05 *** ## weight 1.8151 0.3767 4.819 1.45e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta: count = 4.8462 ## Number of iterations in BFGS optimization: 15 ## Log-likelihood: -352.6 on 5 Df Modelo P NB ZINB HNB AIC 920.1 754.6 715.5 715.3 BIC 876.9 764.1 731.3 731.0 "],["modelo-lineal-mixto.html", "Capítulo 9 Modelo lineal mixto 9.1 Casos de estudio 9.2 Datos correlacionados 9.3 Modelo lineal mixto", " Capítulo 9 Modelo lineal mixto 9.1 Casos de estudio 9.1.1 Peso al nacer de corderos La base de datos (harville.lamb de la librería agridat) contiene información de 62 corderos machos de un solo parto. Estos corderos eran progenie de 23 carneros (machos), por lo que cada cordero tenía una madre diferente. Variables weight: peso al nacer del cordero en kgs (respuesta). damage: edad de la madre (1: entre 1-2 años, 2: entre 2-3 años, 3: más de 3 años) sire: padre. par(mfrow=c(1,2)) plot(weight~damage,xlab=&quot;Grupo de edad de la madre&quot;, ylab=&quot;Peso del carnero (kgs)&quot;, data = harville.lamb) plot(weight~sire,xlab=&quot;Padre&quot;,ylab=&quot;Peso del carnero (kgs)&quot;, data = harville.lamb) abline(v=1:27,col=&#39;lightgray&#39;) Figure 9.1: Peso al nacer de corderos 9.1.2 Crecimiento craneofacial de ratas La base de datos (disponible en el campus virtual) tiene como objetivo evaluar el efecto de la inhibición de la producción de testosterona en el crecimiento craneofacial de ratas Wistar macho. Variables: response: longitud de cráneo medida como la distancia en píxeles entre dos puntos bien definidos en las imágenes de rayos X (respuesta). treat: tratamiento (control, dosis baja de Decapeptyl y dosis alta de Decapeptyl). age: edad de la rata. Este es un estudio longitudinal, es decir a cada una de las 34 ratas se le tomaron diferentes mediciones en el tiempo. “grafica” 9.2 Datos correlacionados En ambos estudios se tienen datos que pueden estar potencialmente correlacionados. En los datos de los corderos se espera que el peso los corderos recién nacidos del mismo padre estén correlacionados. Datos en “cluster” o jerárquicos. En los datos de las ratas, por lo tanto las observaciones tomadas sobre la misma rata pueden estar correlacionados. Datos longitudinales. 9.3 Modelo lineal mixto Modelo: \\[ y_{ij}=\\boldsymbol x¡_{ij}\\boldsymbol \\beta+\\boldsymbol z&#39;_{ij}\\boldsymbol b_i+\\epsilon_{ij}, \\] donde: \\(\\boldsymbol x_{ij}\\) y \\(\\boldsymbol x_{ij}\\) son vectores de covariables de la \\(j\\)-ésima observación y el \\(i\\)-ésimo cluster. \\(\\boldsymbol \\beta=(\\beta_0,\\beta_1,...,\\beta_{p-1})&#39;\\) es el vector de coeficientes de efectos fijos. \\(\\boldsymbol b_i=(b_{0i,},b_{1i},...,b_{q-1,i})&#39;\\) el vector de coeficientes aleatorios. \\(\\epsilon_{ij}\\) es el término del error. En forma matricial: \\[ \\boldsymbol y_i=\\boldsymbol X_i\\boldsymbol \\beta+\\boldsymbol Z_i\\boldsymbol b_i+\\boldsymbol \\epsilon_i, \\] donde: \\(\\boldsymbol y_i=(y_{i1},y_{i2},...,y_{in_1})&#39;\\). \\(\\boldsymbol X_i=(\\boldsymbol x_{i1},\\boldsymbol x_{i2},...,\\boldsymbol x_{in_1})&#39;\\) es una matriz \\(n_i\\times p\\) . \\(\\boldsymbol Z_i=(\\boldsymbol z_{i1},\\boldsymbol z_{i2},...,\\boldsymbol z_{in_1})&#39;\\) es una matriz \\(n_i\\times q\\). \\(\\boldsymbol \\beta=(\\beta_{0},\\beta_{1},...,\\beta_{p-1})&#39;\\) es un vector de longitud \\(p\\). \\(\\boldsymbol \\epsilon_i=(\\epsilon_{i1},\\epsilon_{i2},...,\\epsilon_{in_1})&#39;\\). El LMM asume que: \\(\\boldsymbol \\epsilon_i\\sim N(\\boldsymbol 0,\\sigma^2\\boldsymbol I)\\). \\(\\boldsymbol b_i\\sim N(\\boldsymbol 0,\\boldsymbol D\\). \\(\\boldsymbol \\epsilon_i\\) y \\(\\boldsymbol b_i\\) son independientes. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
