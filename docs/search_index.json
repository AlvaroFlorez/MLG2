[["index.html", "Notas de clase: Modelo lineal general II Introducción", " Notas de clase: Modelo lineal general II Alvaro J. Flórez 2024-08-12 Introducción Estas son las notas de clase del curso Modelo Lineal General II. Los temas que se tratan son: Variables indicadoras. Modelos polinomiales. Multicolinealidad. Selección de variables. Introducción a modelos no lineales. Introducción al modelo lineal generalizado (modelo logístico - modelo Poisson). Introducción al modelo lineal mixto Tenga en cuenta que el propósito de estas notas de clase no es reemplazar los textos guías. Para el estudio más detallado de los temas revisados, se recomiendan las siguientes lecturas: Introduction to Linear Regression Analysis, Fifth Ed., 2012, by Montgomery, D. C., Peck, E. A. and Vining, G. G. (Texto guía) Applied Regression Analysis, Third Ed., 1998, by Draper, N. R. and Smith, H., Wiley. Theory and Applications of the Linear Models, 2000, by Graybill, F. A., Duxbury. Applied Linear Statistical Models, Fifth Ed., 2005, by Kutner, M. H, Nachtsheim, C. J., Neter, J. and Li, W., McGraw-Hill. Análisis de Regresión. Introducción Teórica y Práctica basada en R, 2011, by F. Tusell. Applied Linear Regression, Fourth Ed., 2014, by S. Weisberg. Applied Regression Analysis &amp; Generalized Linear Models, 2016, by J. Fox. Agradecimientos especiales a John Sebastian Manjarres por la ayuda en la elaboración de estas notas. "],["variables-indicadoras-o-dummies.html", "Capítulo 1 Variables indicadoras (o dummies) 1.1 Ejemplos 1.2 Variables indicadoras", " Capítulo 1 Variables indicadoras (o dummies) 1.1 Ejemplos 1.1.1 Datos de atletas australianos Los datos ais, de la libreria alr4, tienen información sobre 202 atletas australianos de élite (102 hombres y 100 mujeres). Se quiere evaluar la relación entre la concentración de hemoglobina (Hg, g/dl) y el índice de masa corporal (BMI, kg/m\\(^2\\)). data(ais) par(mfrow=c(1,2)) plot(density(ais$Hg[ais$Sex==0]),xlim=c(11,20),lwd=2,main = &#39;&#39;,ylab=&#39;Densidad&#39;,xlab=&#39;Hg (g/dl)&#39;) lines(density(ais$Hg[ais$Sex==1]),col=2,lwd=2) plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) Figure 1.1: Datos de atletas. Densidad de la hemoglobina para hombres y mujeres (derecha) y diagrama de dispersión entre la homoglobina y el índice de masa corporal (izquierda). Negro para hombres y rojo para mujeres. En la Figura 1.1 (izquierda) vemos que los niveles de hemoglobina son mayores para hombres que para mujeres. En la Figura 1.1 (derecha) observamos que hay una relación positiva entre la hemoglobina y el índice de masa corporal tanto para hombres como para mujeres. Esto nos puede indicar que ingresar el sexo del atleta en el modelo puede mejorarnos el ajuste. 1.1.2 Datos de la ONU Retomemos los datos de la ONU (UN11 de la librería alr4). Las variables de interés son: fertility: Número esperado de nacidos vivos por mujer. ppgdp: producto nacional bruto per cápita (PNB, en dólares). Purban: el porcentaje de la población que vive en un área urbana. lifeExpF: esperanza de vida femenina (años). group: si el país pertenece a la OCDE (Organización para la Cooperación y el Desarrollo Económicos), África o otros. Por ahora consideremos la relación entre la fertilidad y el PNB per cápita, teniendo en cuenta el grupo al que pertenece cada país. data(UN11) plot(log(fertility)~log(ppgdp),data=UN11,col=UN11$group,xlab=&#39;log PNB per cápita (dólares)&#39;, ylab=&#39;log # esperado de nacidos vivos por mujer&#39;) Figure 1.2: Datos de la ONU. Relación entre la fertilidad y el PNB per cápita para los países de OCDE (puntos negros), países africanos (puntos verdes) y los otros países (rojos) En la Figura 1.2 podemos observar que, en general, cuando el PNB aumenta, la tasa de fertilidad disminuye. Sin embargo, esta relación puede variar según la categoría del país. Para los países de la OCDE, esta relación no es fuerte. Mientras que para los demás se mantiene esta relación negativa. Por esta razón sería de gran importancia incluir esta variable categórica dentro del modelo. 1.2 Variables indicadoras Las covariables categóricas entran en un modelo como variables indicadoras (o también llamadas dummies). En el caso que la covariable \\((X)\\) tenga dos categorías, entonces se crea una variable indicadora. Por ejemplo, para los datos de los atletas, el sexo requiere una indicadora: \\[ u_{i} = \\begin{cases} 1 &amp; \\mbox{ si la observación i es de una mujer}, \\\\ 0 &amp; \\mbox{ si la observación i es de un un hombre}. \\\\ \\end{cases} \\] Aquí la categoría hombre es llamada la categoría de referencia. En caso que la covariable categórica tenga \\(K\\) categorías, se tienen que crear \\(K-1\\) variables indicadoras. Por ejemplo, para la variable grupo de país en los datos de la ONU se requieren 2 variables indicadoras \\((u_{1} y u_{2})\\) como lo muestra la Tabla 1.1. En este caso, la categoría de referencia es OECD. Table 1.1: Variables indicadoras para la variable group de los datos de la ONU Categoría \\(u_{1}\\) \\(u_{2}\\) OECD 0 0 otro 1 0 África 0 1 1.2.1 Modelos con covariables categóricas Suponga que se quiere ajustar un modelo para una variable respuesta \\(Y\\) en función de dos covariables: una continua \\(X\\) y una indicadora \\(Z\\) (es decir una variable categórica con dos 2 categorías). El modelo propuesto es el siguiente: \\[\\begin{equation} y_{i} = \\beta_{0} + x_{i}\\beta_{1} + z_{i}\\beta_{2} + x_{i}z_{i}\\beta_{3} + \\varepsilon_{i}, \\tag{1.1} \\end{equation}\\] donde \\(\\varepsilon_{i} \\sim N(0,\\sigma^{2})\\). Tenemos que, si \\(z_i=0\\): \\[ E(Y | X=x_i, Z=0) = \\beta_{0} + x_{i}\\beta_{1}. \\] Mientras que, si \\(z_i=1\\): \\[ E(Y | X=x_i, Z=1) = (\\beta_{0}+\\beta_{2}) + x_{i}(\\beta_{1}+\\beta_{3}). \\] Por lo que el modelo (1.1) genera dos rectas, una para cada categoría. \\(\\beta_2\\) indica la diferencia de intercepto entre las dos categorías y \\(\\beta_3\\) la diferencia entre pendientes. En la Figura 1.3(izquierda) se observan las dos rectas que se obtienen a partir de este modelo. Si se elimina la interacción entre variables (es decir, \\(\\beta_3=0\\)), se obtienen dos rectas paralelas (Figura 1.3, derecha). Figure 1.3: Efecto de la interacción entre variable continua e indicadora. Modelo general (izquierda) y modelo de líneas paralelas (derecha). Hay que tener en cuenta que la asignación de la categoría de referencia no afecta en nada al ajuste del modelo. Sin embargo, la interpretación de los coeficientes cambia. 1.2.2 Modelo para los datos de atletas australianos Para los datos de los atletas, se sugiere el siguiente modelo: \\[\\begin{equation} \\begin{split} \\mbox{Hg}_{i} =&amp; \\beta_{0} + \\mbox{Sex}_{i}\\beta_{1} + \\mbox{BMI}_{i}\\beta_{2} + \\mbox{Sex}_{i}\\mbox{BMI}_{i}\\beta_{3} + \\varepsilon_{i}, \\end{split} \\nonumber \\end{equation}\\] donde: \\[ \\mbox{Sex}_{i} = \\begin{cases} 1 &amp; \\mbox{ si la observación i corresponde a una mujer}, \\\\ 0 &amp; \\mbox{ de otra forma}. \\\\ \\end{cases} \\] Note que en la base de datos ais, la variable sex ya está codificada de esta forma. Si la variable no está codificada de forma numérica, R eligirá la categoría de referencia de forma automática. Sin embargo, esta puede modificarse con la función relevel(). Para ajustar el modelo utilizamos la función lm (aquí estamos incluyendo los efectos de BMI y Sex, así como la interacción): mod.ais = lm(Hg~Sex*BMI, data=ais) summary(mod.ais) ## ## Call: ## lm(formula = Hg ~ Sex * BMI, data = ais) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9997 -0.6625 -0.0478 0.5784 3.5583 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.21316 0.78662 16.797 &lt;2e-16 *** ## Sex -0.66140 1.09835 -0.602 0.5477 ## BMI 0.09788 0.03269 2.994 0.0031 ** ## Sex:BMI -0.05203 0.04761 -1.093 0.2758 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9092 on 198 degrees of freedom ## Multiple R-squared: 0.5613, Adjusted R-squared: 0.5546 ## F-statistic: 84.44 on 3 and 198 DF, p-value: &lt; 2.2e-16 Tenemos que \\(\\widehat{\\beta}_{2} =0.1\\), lo que nos indica que el valor esperado del nivel de hemoglobina aumenta en \\(0.1\\) g/dl por cada aumento unitario en el índice de masa corporal de los hombres. Para las mujeres, el efecto del índice de masa corporal sobre el nivel de hemoglobina también es positivo, pero con una pendiente menor \\(\\widehat{\\beta}_{2} +\\widehat{\\beta}_{3} = 0.1 - 0.05 = 0.05\\). La representación gráfica del modelo se puede observar en la Figura 1.4. Aquí vemos que la pendiente para las mujeres es un poco menor que para los hombres. plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) abline(a=mod.ais$coefficients[1],b=mod.ais$coefficients[3],lwd=2) abline(a=mod.ais$coefficients[1]+mod.ais$coefficients[2],b=mod.ais$coefficients[3]+mod.ais$coefficients[4],col=2,lwd=2) Figure 1.4: Datos de atletas. Ajuste del modelo para la homoglobina en función del índice de masa corporal y sexo. Línea negra para hombres y línea roja para mujeres. Si miramos la significancia del \\(\\widehat{\\beta}_3\\) (valor-\\(p\\) igual a 0.276), podemos concluir que las diferencias en pendiente no son significativas. Por lo que, el efecto del índice de masa corporal sobre los niveles de hemoglobina es el mismo para mujeres que para hombres. Si queremos evaluar si los niveles de hemoglobina son iguales para hombres y mujeres, debemos evaluar la siguiente hipótesis: \\[ H_{0}: \\beta_{1} = \\beta_3 = 0. \\] En R, esto lo podemos realizar usando la función anova() (la cuál realiza una prueba F) comparando el modelo completo contra el modelo reducido sin la variable Sex: mod.ais.red = lm(Hg~BMI, data=ais) anova(mod.ais.red,mod.ais) ## Analysis of Variance Table ## ## Model 1: Hg ~ BMI ## Model 2: Hg ~ Sex * BMI ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 200 318.52 ## 2 198 163.69 2 154.82 93.637 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Vemos que se rechaza \\(H_0\\), es decir que hay diferencias en los niveles de homoglobina entre hombres y mujeres. \\(\\beta_1\\) o \\(\\beta_3\\) es diferente de cero, pero no ambos (esto por los resultados del modelo completo). Eliminando la interacción obtenemos un modelo con lineas paralelas: \\[ \\mbox{Hg}_{i} = \\beta_{0} + \\mbox{Sex}_{i}\\beta_{1} + \\mbox{BMI}_{i}\\beta_{2} + \\varepsilon_{i}, \\] donde \\(\\varepsilon_{i} \\sim N(0, \\sigma^{2})\\), y se calcula de la siguiente forma: mod.ais.lp = lm(Hg~Sex+BMI, data=ais) summary(mod.ais.lp) ## ## Call: ## lm(formula = Hg ~ Sex + BMI, data = ais) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0131 -0.6530 -0.0263 0.6249 3.5806 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.79954 0.57549 23.979 &lt; 2e-16 *** ## Sex -1.85251 0.13587 -13.634 &lt; 2e-16 *** ## BMI 0.07335 0.02378 3.085 0.00233 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9097 on 199 degrees of freedom ## Multiple R-squared: 0.5586, Adjusted R-squared: 0.5542 ## F-statistic: 125.9 on 2 and 199 DF, p-value: &lt; 2.2e-16 La estimación del efecto asociado al sexo indica la diferencia media del nivel de hemoglobina entre hombres y mujeres. En la Figura 1.5 vemos que el modelo de líneas paralelas tiene casi el mismo ajuste que el modelo general. plot(Hg~BMI,data=ais,col=ais$Sex+1,ylab=&#39;Hg (g/dl)&#39;,xlab=&#39;BMI&#39;) abline(a=mod.ais$coefficients[1],b=mod.ais$coefficients[3],lwd=2) abline(a=mod.ais$coefficients[1]+mod.ais$coefficients[2],b=mod.ais$coefficients[3]+mod.ais$coefficients[4],col=2,lwd=2) abline(a=mod.ais.lp$coefficients[1],b=mod.ais.lp$coefficients[3],lwd=2,lty=2) abline(a=mod.ais.lp$coefficients[1]+mod.ais.lp$coefficients[2],b=mod.ais.lp$coefficients[3],col=2,lwd=2,lty=2) Figure 1.5: Datos de atletas. Ajuste del modelo general (línea solida) y el modelo líneas paralelas (línea discontinua) para la homoglobina en función del índice de masa corporal y sexo. Líneas negra para hombres y líneas roja para mujeres. En conclusión, hay diferencia en los niveles de hemoglobina entre hombres y mujeres (es, en promedio, mayor para lo hombres en 1.85g/dl). Sin embargo, el efecto del índice de masa corporal es el mismo para ambos sexos. 1.2.3 Modelo para los datos de la ONU El modelo propuesto es el siguiente: \\[\\begin{equation} \\begin{split} \\log\\mbox{fertility}_{i} =&amp; \\beta_{0} + \\mbox{u}_{1i}\\beta_{1}+\\mbox{u}_{2i}\\beta_{2} + \\log\\mbox{ppgdp}_{i}\\beta_{3} + \\\\ &amp; \\mbox{u}_{1i}\\log\\mbox{ppgdp}_{i}\\beta_{4} + \\mbox{u}_{2i}\\log\\mbox{ppgdp}_{i}\\beta_{5} + \\varepsilon_{i}, \\end{split} \\nonumber \\end{equation}\\] donde: \\[ \\mbox{u}_{1i} = \\begin{cases} 1 &amp; \\mbox{ si el país i pertenece a la categoría otro}, \\\\ 0 &amp; \\mbox{ de otra forma}, \\\\ \\end{cases} \\mbox{ y } \\mbox{u}_{2i} = \\begin{cases} 1 &amp; \\mbox{ si el país i pertenece a África}, \\\\ 0 &amp; \\mbox{ de otra forma}. \\\\ \\end{cases} \\] Por lo tanto, OECD es la categoría de referencia. El modelo ajustado es: mod.UN11 = lm(log(fertility)~group*log(ppgdp), data=UN11) summary(mod.UN11) ## ## Call: ## lm(formula = log(fertility) ~ group * log(ppgdp), data = UN11) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.69358 -0.16963 0.02005 0.16838 0.73633 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.21836 0.81290 0.269 0.78851 ## groupother 1.87888 0.83290 2.256 0.02520 * ## groupafrica 2.54072 0.84299 3.014 0.00293 ** ## log(ppgdp) 0.03217 0.07832 0.411 0.68170 ## groupother:log(ppgdp) -0.18418 0.08106 -2.272 0.02417 * ## groupafrica:log(ppgdp) -0.22637 0.08430 -2.685 0.00788 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2739 on 193 degrees of freedom ## Multiple R-squared: 0.6305, Adjusted R-squared: 0.6209 ## F-statistic: 65.86 on 5 and 193 DF, p-value: &lt; 2.2e-16 A partir de estos resultados obtenemos tres rectas que describen el valor esperado del logarítmo de la fertilidad en función del logarítmo del PNB per cápita, una para cada tipo de país. Para los países de la OCDE, tenemos que: \\[ E(\\log\\mbox{fertility}) = 0.218 + 0.032\\log \\mbox{ppgdp}. \\] Para los países Africanos: \\[ E(\\log\\mbox{fertility}) = 2.759 -0.194\\log \\mbox{ppgdp}. \\] Finalmente, para los otros países: \\[ E(\\log\\mbox{fertility}) = 2.097 -0.152\\log \\mbox{ppgdp}. \\] Aquí vemos que para los países que no son de la OCDE, el PNB tiene un efecto negativo significativo sobre la tasa de fertilidad. Mientras que para los países de la OCDE, este efecto es positivo, aunque no es significativo. Esto mismo lo podemos ver gráficamente en la Figura 1.6. Beta.UN11 = mod.UN11$coefficients plot(log(fertility)~log(ppgdp),data=UN11,col=UN11$group,xlab=&#39;log PNB per cápita (dólares)&#39;, ylab=&#39;log # esperado de nacidos vivos por mujer&#39;) abline(a=Beta.UN11[1],b=Beta.UN11[4],lwd=2) abline(a=Beta.UN11[1]+Beta.UN11[2],b=Beta.UN11[4]+Beta.UN11[5],col=2,lwd=2) abline(a=Beta.UN11[1]+Beta.UN11[3],b=Beta.UN11[4]+Beta.UN11[6],col=3,lwd=2) Figure 1.6: Datos de la ONU. Ajuste del modelo para la fertilidad en función del PNB y tipo de país. Países de OCDE (línea negra), países africanos (línea verde) y los otros países (línea roja) Usando la función relevel() se puede cambiar la categoría de referencia. Por ejemplo, si queremos que la categoría de referencia sea other: UN11.alt = UN11 UN11.alt$group = relevel(UN11.alt$group,ref =&#39;other&#39;) mod.UN11.alt = lm(log(fertility)~group*log(ppgdp), data=UN11.alt) summary(mod.UN11.alt) Note que cambiar la categoría de referencia no altera en nada los resultados del ajuste. Solo cambian la interpretación de los coeficientes. Se podría hacer la siguiente pregunta, ¿el efecto del PNB sobre la fertilidad es el mismo para cada tipo de país?. Para resolver esta pregunta, se plantea la siguiente hipótesis: \\[ H_0: \\beta_4 = \\beta_5 = 0. \\] El estadístico de prueba lo podemos obtener usando la función anova() (prueba F) en R: mod.UN11.red = lm(log(fertility)~group+log(ppgdp), data=UN11) anova(mod.UN11.red,mod.UN11) ## Analysis of Variance Table ## ## Model 1: log(fertility) ~ group + log(ppgdp) ## Model 2: log(fertility) ~ group * log(ppgdp) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 195 15.033 ## 2 193 14.484 2 0.54848 3.6542 0.02769 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Este resultado indica que hay evidencia suficiente para concluir que el efecto del PNB sobre la fertilidad no es el mismo para cada tipo de país. Ahora, podríamos preguntarnos: ¿las pendientes son las mismas para las categorías de otro y África?. Para esto, se plantea la siguiente hipótesis: \\[ H_0: \\beta_4 = \\beta_5. \\] También se puede expresar de la siguiente forma: \\[ H_{0}: \\boldsymbol L\\boldsymbol \\beta= \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5 \\end{pmatrix} = 0 \\] Esto, en R es: L = matrix(c(0,0,0,0,1,-1),1,6,byrow = T) linearHypothesis(mod.UN11, hypothesis.matrix=L) ## Linear hypothesis test ## ## Hypothesis: ## groupother:log(ppgdp) - groupafrica:log(ppgdp) = 0 ## ## Model 1: restricted model ## Model 2: log(fertility) ~ group * log(ppgdp) ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 194 14.579 ## 2 193 14.484 1 0.094808 1.2633 0.2624 Dado que no se rechaza \\(H_0\\), el efecto del PNB sobre la fertilidad es el mismo para los países Africanos y los de la categoría de otros. En resumen, a partir del análisis de regresión se tiene que: el efecto del PNB per cápita sobre la fertilidad depende del tipo de país. Para los países miembros de la OECD, no hay un efecto significativo. Sin embaro, para los demás países, el efecto es negativo (y no es significativamente diferente para las categorías de África y otros). "],["modelos-polinomiales.html", "Capítulo 2 Modelos polinomiales 2.1 Ejemplos 2.2 Modelos polinomiales 2.3 Regresión por segmentos", " Capítulo 2 Modelos polinomiales 2.1 Ejemplos 2.1.1 Pasteles Con los datos cakes de la libreria alr4 se tienen dos objetivos. Primero, evaluar el efecto de la temperatura y tiempo de horneado sobre la palatabilidad de mazclas de pasteles para hornear. Segundo, encontrar la combinación de estos factores que maximizan la palatabilidad. Como variable respuesta (Y) se tiene el promedio de calificación de la palatabilidad de cuatro pasteles horneados. Mientras que las covariables son: el tiempo de horneado (X1, en minutos) y la temperatura (X2, en grados Fahrenheit) library(alr4) data(cakes) plot(cakes[,-1]) Figure 2.1: Datos de pasteles. Diagrama de dispersión. 2.1.2 Datos de Boston La base de datos Boston de la libreria MASS contiene información sobre 506 suburbios del area metropolitana de Boston. El objetivo del estudio es evaluar la relación del precio de las viviendas y la concentración de contaminación ambiental. En esta sección evaluaremos la relación entre la concentración anual de óxido de nitrógeno (\\(y\\), en partes por diez millones) y la distancia a cinco centros de empleo. library(MASS) data(Boston) plot(nox~dis,data=Boston,ylab=&#39;NOx&#39;,xlab=&#39;distancia a centros de empleo&#39;) Figure 2.2: Datos de Boston. Relación entre el óxido de nitrógeno y la distancia a centros de empleo. 2.2 Modelos polinomiales Considere el modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_i, \\] donde \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\) y \\(cov(\\varepsilon_i,\\varepsilon_j)=0\\) para todo \\(i\\neq j\\). Este describe la relación lineal entre \\(y\\) y \\(x_1\\). Si las relación entre las variables presentan curvaturas, se puede considerar un modelo polinómico de la forma: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{i}^{2} + \\ldots + \\beta_{r}x_{i}^{r} + \\varepsilon_i. \\] Este modelo se sigue considerando como un modelo lineal, dado que es lineal en los parámetros (es una función lineal de \\(\\boldsymbol \\beta\\)). En la Figura 2.3 se puede observar diferentes curvas para modelos lineales (orden 1), cuadráticos (orden 2) y cúbicos (orden 3). Aquí vemos que este tipo de modelos son muy versatiles. Cualquier función suave se puede ajustar meidante un polinomio de grado suficientemente alto. Por esta razón, los modelos polinomicos son usados en casos donde las relaciones entre las variables son no-lineales y se pueden aproximar por un polinomio. Figure 2.3: Polinomio de grado 1 (linea negra), grado 2 (linea roja) y grado 3 (linea verde). En el caso que se tengan dos covariables, un modelo de orden 2 se expresa de la forma: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}^2 + \\beta_{4}x_{2i}^2 + \\beta_{5}x_{1i}x_{2i} + \\varepsilon_i. \\] En las Figuras 2.4-2.6 muestran el valor esperado de \\(Y\\) en un modelo lineal (asumiendo \\(\\beta_3=\\beta_4=\\beta_5=0\\)), con interacción (\\(\\beta_3=\\beta_4=0\\)) y cuadrático. Figure 2.4: Valor esperado de \\(Y\\) en un modelo lineal (izquierda) y gráfico de contorno (derecha). El número de parámetros incrementa rápidamente con el número de covariables. Con \\(k\\) covariables, tenemos: un intercepto, \\(k\\) términos lineales, \\(k\\) términos cuadráticos, y \\(k(k - 1)/2\\) interacciones. Por esta razón, en muchos casos, no se toman en cuentas las interacción cuando el número de covariables es grande. Figure 2.5: Valor esperado de \\(Y\\) en un modelo cuadrático (izquierda) y gráfico de contorno (derecha). Figure 2.6: Valor esperado de \\(Y\\) en un modelo lineal con interacción (izquierda) y gráfico de contorno (derecha). Hay aspectos que se deben tener en la práctica cuando se implementa un modelo polinomial: Selección del orden: La idea es mantener el orden del polinomio bajo. Si embargo, si es muy bajo no logra capturar la curvatura presente en los datos. En caso que el orden sea grande, el modelo es innecesariamente más complejo y puede haber problemas de multicolinealidad. Si los datos exige un modelo de orden alto \\((k &gt; 3)\\), se pueden hacer transformación sobre las variables, y así, poder ajustar un modelo polinomial de orden bajo (por ejemplo cuadrático). La selección del orden puede hacerse de dos formas. (1) Hacia delante: empezar con un modelo de orden \\(1\\) e incrementar el orden uno a uno hasta que un término mayor ya no sea significativo. Hacia atrás: empezar con el modelo más complejo y eliminar los términos mayores uno a uno hasta que todos sean significativos. Extrapolación: La extrapolación con modelos polinomiales puede ser muy peligrosa. Por ejemplo en la Figura 2.7 podemos ajustar un modelo de orden dos a los datos (linea negra). Si hacemos una predicción fuera del rango de los datos, el valor esperado predicho sigue el comportamiento cuadrático propuesto. Sin embargo, el valor esperado de \\(Y\\) puede seguir un comportamiento diferente (linea roja discontinua). Lo que lleva a tener una predicción sesgada. Figure 2.7: Problema de extrapolación. Multicolinealidad: Al aumentar el polinomio, la matriz \\(\\boldsymbol X&#39;\\boldsymbol X\\) se vuelve mal acondicionada. Es decir, las estimaciones pueden ser inestables y los errores estándar se inflan. Este problema se puede solucionar centrando las covariables. Por ejemplo en un modelo de orden 2: \\[ E(Y | X=x) = \\beta_{0} + \\beta_{1}(x-\\bar{x}) + \\beta_{2}(x-\\bar{x})^{2}. \\] Otra solución es usando polinomios ortogonales. 2.2.1 Interpretación de los coeficientes Considere un modelo de orden 2. El valor esperado de \\(Y\\) está dado por: \\[ E(Y| X_{1}=x_1,X_{2}=x_2) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}^{2} + \\beta_{4}x_{2}^{2} + \\beta_{5}x_{1}x_{2}. \\] Si \\(x_{1}\\) cambia en \\(\\delta\\) unidades \\((x_{1} + \\delta)\\), tenemos que: \\[ E(Y| X_{1}=x_1+\\delta,X_{2}=x_2) = \\beta_{0} + \\beta_{1}(x_{1}+\\delta) + \\beta_{2}x_{2} + \\beta_{3}(x_{1} + \\delta)^{2} + \\beta_{4}x_{2}^{2} + \\beta_{5}(x_{1}+\\delta)x_{2}. \\] Ahora calculando la diferencia: \\[ E(Y| X_{1}=x_1+\\delta,X_{2}=x_2) - E(Y| X_{1}=x_1,X_{2}=x_2) = (\\beta_{1}\\delta + \\beta_{3}\\delta^{2}) + 2\\beta_{3}\\delta x_{1} + \\beta_{5}\\delta x_{2}. \\] Note que el efecto del cambio \\(\\delta\\) en \\(X_1\\) depende de ambas covariables y del valor de \\(\\delta\\). Por esta razón, la interpretación de los coeficientes es complicada. 2.2.2 Pasteles Para los datos de los pasteles, se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}^2 + \\beta_{4}x_{2i}^{2} + \\beta_{5}x_{1i}x_{2i} + \\varepsilon_{i}. \\] El ajuste del modelo es: mod.cakes = lm(Y ~ X1*X2 + I(X1^2)+I(X2^2),data=cakes) summary(mod.cakes) ## ## Call: ## lm(formula = Y ~ X1 * X2 + I(X1^2) + I(X2^2), data = cakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4912 -0.3080 0.0200 0.2658 0.5454 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.204e+03 2.416e+02 -9.125 1.67e-05 *** ## X1 2.592e+01 4.659e+00 5.563 0.000533 *** ## X2 9.918e+00 1.167e+00 8.502 2.81e-05 *** ## I(X1^2) -1.569e-01 3.945e-02 -3.977 0.004079 ** ## I(X2^2) -1.195e-02 1.578e-03 -7.574 6.46e-05 *** ## X1:X2 -4.163e-02 1.072e-02 -3.883 0.004654 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4288 on 8 degrees of freedom ## Multiple R-squared: 0.9487, Adjusted R-squared: 0.9167 ## F-statistic: 29.6 on 5 and 8 DF, p-value: 5.864e-05 Además, los factores de inflación de varianza son los siguientes: car::vif(mod.cakes) ## X1 X2 I(X1^2) I(X2^2) X1:X2 ## 3778.083 5921.833 1328.089 5309.339 3063.500 Aquí vemos que los VIF presentan valores muy altos, producto de ajustar un modelo cuadrático. Ahora consideremos el modelo con las covariables centradas: cakes$X1c = cakes$X1 - mean(cakes$X1) cakes$X2c = cakes$X2 - mean(cakes$X2) modc.cakes = lm(Y ~ X1c*X2c + I(X1c^2)+I(X2c^2),data=cakes) summary(modc.cakes) ## ## Call: ## lm(formula = Y ~ X1c * X2c + I(X1c^2) + I(X2c^2), data = cakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4912 -0.3080 0.0200 0.2658 0.5454 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.070000 0.175044 46.103 5.41e-11 *** ## X1c 0.367558 0.075796 4.849 0.001273 ** ## X2c 0.096392 0.015159 6.359 0.000219 *** ## I(X1c^2) -0.156875 0.039446 -3.977 0.004079 ** ## I(X2c^2) -0.011950 0.001578 -7.574 6.46e-05 *** ## X1c:X2c -0.041625 0.010719 -3.883 0.004654 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4288 on 8 degrees of freedom ## Multiple R-squared: 0.9487, Adjusted R-squared: 0.9167 ## F-statistic: 29.6 on 5 and 8 DF, p-value: 5.864e-05 Los VIFs del modelo con las covariables centradas son: car::vif(modc.cakes) ## X1c X2c I(X1c^2) I(X2c^2) X1c:X2c ## 1.000000 1.000000 1.005952 1.005952 1.000000 En este caso, los VIF decrecieron considerablemente con respecto al modelo con las covariables originales. En la figura 2.8 podemos observar el valor esperado estimado de la palatabilidad para diferentes valores de tiempo y temperatura de horneado. Figure 2.8: Datos de pasteles. Valor esperado de la palatabilidad para diferentes valores de tiempo y temperatura de horneado. En la Figura 2.8 (izquierda) vemos que, cuando la temperatura es de 350 grados Fahrenheit, el máximo de palatabilidad que se obtiene en un tiempo entre 36 y 37 minutos. Sin embargo, cuando la temperatura se incrementa a 360, la máxima palatibilidad que se puede lograr es menor. Además, se obtiene en un tiempo también menor. El efecto de la interacción también se puede observar en la Figura 2.9 (derecha), pero ahora fijando el tiempo de horneado y variando la temperatura. El gráfico de contornos (Figura 2.9) muestra que la máxima palatabilidad se observa cuando la temperatura está alrededor de 355 y el tiempo de horneado está entre 35 y 36 minutos. X1 = seq(32, 38, length.out = 50) X2 = seq(335, 365, length= 50) y &lt;- outer(X= X1, Y = X2, FUN = function(x, y) { predict(modc.cakes, newdata = data.frame(X1c = x-mean(cakes$X1), X2c = y-mean(cakes$X2))) }) contour(X1, X2, y,xlab=&#39;tiempo de horneado (minutos)&#39;, ylab=&#39;temperatura de horneado (Fahrenheit)&#39;) Figure 2.9: Datos de pasteles. Gráfica de contornos. Para determinar en que combinación de tiempo (X1) y temperatura de horneado (X2) se obtiene la máxima palatabilidad, debemos resolver la siguiente ecuación: \\[ \\frac{\\partial E(Y)}{\\partial \\boldsymbol x} = \\frac{\\partial}{\\partial \\boldsymbol x} \\left( \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{1}^2 + \\beta_{4}x_{2}^{2} + \\beta_{5}x_{1}x_{2} \\right) = \\boldsymbol 0, \\] verificando que se obtiene un máximo. Resolviendo la ecuación anterior, el máximo valor esperado de palatabilidad \\((8.3)\\) se obtiene en un tiempo de horneado de 35.8 minutos y a una temperatura de F. El intervalo del 95% de confianza en este punto es: \\((7.9, 8.7)\\). 2.2.3 Datos de Boston Para los datos de contaminación se puede proponer el siguiente modelo: \\[ \\mbox{NOx}_{i}^{-1.5} = \\beta_{0} + \\beta_{1}\\mbox{dis}_{i} + \\beta_{2}\\mbox{dis}_{i}^{2} + \\beta_{3}\\mbox{dis}_{i}^{3} + \\varepsilon_{i}. \\] La potencia en la variable respuesta se seleccionó usando el método de Box-Cox. El ajuste del modelo es el siguiente: Boston$disc = Boston$dis - mean(Boston$dis) mod3.Boston = lm(I(nox)^(-1.5)~disc+I(disc^2)+I(disc^3),data=Boston) summary(mod3.Boston) ## ## Call: ## lm(formula = I(nox)^(-1.5) ~ disc + I(disc^2) + I(disc^3), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86825 -0.21380 0.00826 0.26905 0.68913 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.7939644 0.0244485 114.279 &lt; 2e-16 *** ## disc 0.3675414 0.0104022 35.333 &lt; 2e-16 *** ## I(disc^2) -0.0461689 0.0059438 -7.768 4.54e-14 *** ## I(disc^3) 0.0020017 0.0009723 2.059 0.04 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3494 on 502 degrees of freedom ## Multiple R-squared: 0.7826, Adjusted R-squared: 0.7813 ## F-statistic: 602.4 on 3 and 502 DF, p-value: &lt; 2.2e-16 La Figura 2.10 muestra el ajuste del modelo cúbico. Para hacer comparaciones, se muestra también el ajuste lineal y cuadrático. Se observa que el modelo cúbico presenta un buen ajuste. El modelo explica alrededor del \\(78.3\\)% de la variabilidad de la concentración anual de óxido de nitrógeno. Con el modelo cuadrático, el coeficiente de determinación es de \\(0.781\\). Sin embargo, se puede observar que este ajuste presenta deficiencias cuando las distancias son muy grandes (mayores a 11). Figure 2.10: Datos de Boston. Valor esperado de la concentración anual de óxido de nitrógeno (en partes por diez millones) en función de las distancias a cinco centros de empleo (media ponderada). Modelo lineal (negro), cuadrático (rojo) y cúbico (verde). 2.3 Regresión por segmentos El modelo por segmentos se puede expresar como: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(x_{i}-t)^{0}_{+} + \\beta_{3}(x_{i}-t)^{1}_{+} + \\varepsilon_{i}. \\] donde: \\[ (x_{i}-t)_{+}^{r} = \\begin{cases} 0 &amp; \\mbox{ si } x_{i} + t \\leq 0, \\\\ (x_{i}-t)^{r} &amp; \\mbox{ si } x_{i} + t &gt; 0. \\\\ \\end{cases} \\nonumber \\] Por lo tanto, si \\(x_{i} \\leq t\\), \\(E(y_{i}| x_{i}) = \\beta_{0} + \\beta_{1}x_{i}\\). Mientras que, si \\(x_{i} &gt; t\\), tenemos que \\(E(y_{i}| x_{i}) = (\\beta_{0} + \\beta_{2} - \\beta_{1}t) + (\\beta_{1} + \\beta_{3})x_{i}\\). Este modelo está representado graficamente en la Figura 2.11. Cuando \\(\\beta_2 \\neq 0\\), el modelo presenta una discontinuidad en \\(t\\). Mientras que, cuando se fija \\(\\beta_2 = 0\\), el modelo presenta un cambio de pendiente en el punto \\(t\\). Además, \\(\\beta_3\\) indica el cambio de pendiente. Figure 2.11: Modelo de regresión por segmentos. Modelo con discontinuidad en \\(t\\) (izquierda). Modelo con cambio de pendiente en \\(t\\) (derecha). Aquí asumimos que \\(t\\) es conocido. Si este valor se asume como desconocido, debe estimarse a partir de los datos como un parámetro adicional. Sin embargo, se tendría que recurrir a un método de estimación para modelos no-lineales. 2.3.1 Ejemplo library(MPV) data(p7.11) plot(p7.11,ylab=&#39;costo de producción por unidad (USD)&#39;,xlab=&#39;Unidades por lote&#39;) Figure 2.12: Datos de costos por lote. Diagram de dispersión de el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). Considere la base de datos p7.11 de la librería MPV. Aquí se quiere modelar la relación entre el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). Este relación se puede observar en la Figura 2.12. Se puede observar que la relación entre las variables es lineal. Sin embargo, se aprecia un posible cambio de pendiente en el punto \\(x=200\\). Por esta razón se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(x_{i}-200)_{+}^{1} + \\varepsilon_{i}. \\] Note que no se asume ninguna discontinuidad. El ajuste del modelo es: p7.11$x2 = p7.11$x - 200 p7.11$x2[p7.11$x &lt; 200] = 0 mod.lotes = lm(y~.,data=p7.11) summary(mod.lotes) ## ## Call: ## lm(formula = y ~ ., data = p7.11) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.37596 -0.16641 -0.09677 0.20363 0.51734 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.116481 0.535383 28.235 2.67e-09 *** ## x -0.050199 0.003332 -15.065 3.73e-07 *** ## x2 0.038852 0.005946 6.534 0.000181 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3157 on 8 degrees of freedom ## Multiple R-squared: 0.9829, Adjusted R-squared: 0.9787 ## F-statistic: 230.4 on 2 and 8 DF, p-value: 8.474e-08 A partir del ajuste podemos concluir que, por cada unidad que incrementa el lote, el costo de producción disminuye en \\(0.05\\)USD. Si embargo, si el tamaño del lote es mayor a \\(200\\), el costo de producción disminuye solamente \\(0.01\\)USD cuando el lote aumenta en un artículo. Gráficamente, el ajuste se puede observar en la Figura 2.13. b.lotes = mod.lotes$coefficients plot(p7.11$x,p7.11$y,ylab=&#39;costo de producción por unidad (USD)&#39;,xlab=&#39;unidades por lote&#39;) x = c(100,200) lines(x,b.lotes[1]+x*b.lotes[2],lwd=2) x = c(200,300) lines(x,b.lotes[1]-200*b.lotes[3]+x*(b.lotes[2]+b.lotes[3]),lwd=2) abline(v=200,lty=2) Figure 2.13: Datos de costos por lote. Diagram de dispersión de el costo de producción promedio por unidad (USD) y el tamaño del lote (unidades). "],["multicolinealidad.html", "Capítulo 3 Multicolinealidad 3.1 Ejemplos 3.2 Multicolinealidad 3.3 Detección de multicolinealidad 3.4 Datos de cemento 3.5 Datos de grasa corporal 3.6 Solución al problema de multicolinealidad 3.7 Estimador de ridge 3.8 Estimador por componentes principales", " Capítulo 3 Multicolinealidad 3.1 Ejemplos 3.1.1 Cemento Los datos data(cement), de la librería MASS, corresponden a un experimento para evaluar el efecto de diferentes combinaciones químicas sobre el calor emanado en el fraguado del cemento Portland. Para esto, se tiene una muestra de \\(13\\) fraguados de cemento. En cada muestra, se midió con precisión los porcentajes de los cuatro ingredientes químicos principales (covariables). Mientras el cemento fraguaba, también se midió la cantidad de calor desprendido (cals/gm, variable respuesta). Los cuatro ingredientes químicos son: x1 aluminato tricálcico (%), x2 silicato tricálcico (%), x3 tetra-aluminio ferrita de calcio (%), y x4 silicato dicálcico (%). En la Figura 3.1 podemos observar que hay relaciones lineales positivas entre la variable respuesta y las covariables aluminato tricálcico y silicato tricálcico. Mientras que la relación con la variable silicato dicálcico es negativa. También podemos notar que hay una relación negativa fuerte en las covariables aluminato tricálcico y tetra-aluminio ferrita de calcio, y entre silicato tricálcico y silicato dicálcico. data(cement,package = &#39;MASS&#39;) plot(cement[,c(5,1:4)]) Figure 3.1: Datos de cemento. Diagrama de dispersión de las variables. Para estos datos, se propone el siguiente modelo: \\[ y_{i} = \\beta_{0} + x_{1i}\\beta_{1} + x_{2i}\\beta_{2} + x_{3i}\\beta_{3} + x_{4i}\\beta_{4} + \\varepsilon_{i}, \\] donde \\(\\varepsilon_{i} \\sim N(0, \\sigma^2)\\) y \\(cov(\\varepsilon_{i},\\varepsilon_{j})=0\\) para todo \\(i \\neq j\\). Los resultados del ajuste son: mod.cement = lm(y ~ x1+x2+x3+x4,data=cement) summary(mod.cement) ## ## Call: ## lm(formula = y ~ x1 + x2 + x3 + x4, data = cement) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1750 -1.6709 0.2508 1.3783 3.9254 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.4054 70.0710 0.891 0.3991 ## x1 1.5511 0.7448 2.083 0.0708 . ## x2 0.5102 0.7238 0.705 0.5009 ## x3 0.1019 0.7547 0.135 0.8959 ## x4 -0.1441 0.7091 -0.203 0.8441 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.446 on 8 degrees of freedom ## Multiple R-squared: 0.9824, Adjusted R-squared: 0.9736 ## F-statistic: 111.5 on 4 and 8 DF, p-value: 4.756e-07 Estos resultados suguieren que el ajuste es muy bueno, el \\(98.2\\)% de la variabilidad de la cantidad de calor desprendido durante la fraguado es explicada por el modelo. Sin embargo, los resultados de las pruebas de hipótesis individuales sobre los coeficientes muestran valores \\(p\\) muy grandes. Estos resultados parecen contradictorios. 3.1.2 Grasa corporal Se tiene una muestra de \\(20\\) mujeres saludables con edades entre \\(25\\) y \\(34\\) años (data(bodyfat) en la librería isdals). La medición del porcentaje de grasa corporal es caro y engorroso, por lo tanto se quiere buscar un modelo que proporcione predicciones fiables. Como variables de este modelo se utiliza: Triceps: pliegue cutáneo del tríceps (cm), Thigh: circunferencia del muslo (cm), y Midarm: circunferencia del brazo medio (cm). La Figura 3.2 muestra la relación entre variables. Podemos observar que hay una relación fuerte del % de masa corporal con el pliegue cutáneo del triceps y la circunferencia del brazo medio. Además, hay una relación lineal fuerte entre esas dos covariables. library(isdals) data(bodyfat) plot(bodyfat) Figure 3.2: Datos de grasa corporal. Diagrama de dispersión de las variables. El modelo propuesto es: \\[ \\mbox{Fat}_{i} = \\beta_{0} + \\mbox{Triceps}_{i}\\beta_{1} + \\mbox{Thigh}_{i}\\beta_{2} + \\mbox{Midarm}_{i}\\beta_{3} + \\varepsilon_{i}, \\] donde \\(\\varepsilon_{i} \\sim N(0, \\sigma^2)\\) y \\(cov(\\varepsilon_{i},\\varepsilon_{j})=0\\) para todo \\(i \\neq j\\). El ajuste del modelo es el siguiente: mod.bodyfat = lm(Fat ~ Triceps+Thigh+Midarm,data=bodyfat) summary(mod.bodyfat) ## ## Call: ## lm(formula = Fat ~ Triceps + Thigh + Midarm, data = bodyfat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7263 -1.6111 0.3923 1.4656 4.1277 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 117.085 99.782 1.173 0.258 ## Triceps 4.334 3.016 1.437 0.170 ## Thigh -2.857 2.582 -1.106 0.285 ## Midarm -2.186 1.595 -1.370 0.190 ## ## Residual standard error: 2.48 on 16 degrees of freedom ## Multiple R-squared: 0.8014, Adjusted R-squared: 0.7641 ## F-statistic: 21.52 on 3 and 16 DF, p-value: 7.343e-06 Así como en el caso anterior, observamos que el modelo explica gran parte de la variabilidad del % de grasa corporal. Sin embargo, los valores \\(p\\) de las pruebas individuales sobre los coeficientes son altos. 3.2 Multicolinealidad Para el modelo: \\[ \\boldsymbol y= \\boldsymbol X\\boldsymbol \\beta+ \\boldsymbol \\varepsilon, \\mbox{ con } \\boldsymbol \\varepsilon\\sim N(\\boldsymbol 0, \\sigma^2 \\boldsymbol I), \\] el estimador de \\(\\boldsymbol \\beta\\) es \\(\\widehat{\\boldsymbol \\beta}= (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol y\\). Además, se tiene que \\(V(\\widehat{\\boldsymbol \\beta}) = \\sigma^{2}(\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\). Por lo tanto, se requiere que \\(\\boldsymbol X\\) sea de rango completo. Es decir, que no hayan columnas que sean linealmente dependientes. En algunos casos, las columnas de \\(\\boldsymbol X\\) son casi linealmente dependientes o colineales,lo que lleva a que \\(\\boldsymbol X&#39;\\boldsymbol X\\) sea casi singular, lo que puede provocar problemas a la hora de hacer inferencias. Sea \\(\\boldsymbol x_j\\) la \\(j\\)-ésima columna de la matriz \\(\\boldsymbol X\\), por lo tanto \\(\\boldsymbol X= (\\boldsymbol 1,\\boldsymbol x_{1},\\ldots,\\boldsymbol x_{p-1})\\). Los vectores \\(\\boldsymbol x_{1},\\boldsymbol x_{2},\\ldots,\\boldsymbol x_{p-1}\\) son linealmente dependientes si hay conjunto de constantes \\(a_{1},a_{2},\\ldots, a_{p-1}\\) no todas igual a cero, tal que: \\[ \\sum_{j=1}^{p-1}a_{j}\\boldsymbol x_{j} = c, \\mbox{ donde }c\\mbox{ es una constante.} \\] Si esto se cumple, el rango de la matriz \\(\\boldsymbol X&#39;\\boldsymbol X\\) es menor que \\(p\\), y por lo tanto, \\((\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\) no existe. Si la relación es aproximada \\((\\sum_{j=1}^{p-1}a_{j}\\boldsymbol x_{j} \\approx c)\\), existe el problema de multicolinealidad. Vamos a ilustrar el efecto de la multicolinealidad con un ejemplo sencillo. Consideremos un modelo lineal con dos covariables: \\[ y_{i}^{*} = z_{i1}\\beta_1 +z_{i2}\\beta_2 +\\varepsilon_i, \\] donde las covariables están escaladas con longitud unitaria. Esto es: \\[ y_{i}^{*} = \\frac{y_{i}-\\bar{y}}{\\sqrt{SST}}, \\mbox{ y } z_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{\\sqrt{s_{jj}}}, \\] donde \\(s_{jj} = \\sum_{i=1}^{n}(x_{ij}-\\bar{x}_{j})^{2}\\). Además, sea \\(\\boldsymbol Z\\) la matriz de covariables. Con esta transformación, tenemos que \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) es la matriz de correlación de las covariables, \\[ \\boldsymbol Z&#39;\\boldsymbol Z= \\begin{pmatrix} 1 &amp; r_{12} \\\\ r_{12} &amp; 1 \\end{pmatrix}, \\] y \\(\\boldsymbol Z&#39;\\boldsymbol y\\) es el vector de correlaciones entre \\(y\\) y las dos covariables: \\[ \\boldsymbol Z&#39;\\boldsymbol y^{*} = \\begin{pmatrix} r_{y1} \\\\ r_{y2} \\end{pmatrix}. \\] Por lo que el estimador por MCO de \\(\\boldsymbol b\\) está dado por: \\[ \\widehat{\\boldsymbol b}= \\begin{pmatrix} 1 &amp; r_{12} \\\\ r_{12} &amp; 1 \\end{pmatrix}^{-1}\\begin{pmatrix} r_{y1} \\\\ r_{y2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1-r_{12}^{2}} &amp; \\frac{-r_{12}}{1-r_{12}^{2}} \\\\ \\frac{-r_{12}}{1-r_{12}^{2}} &amp; \\frac{1}{1-r_{12}^{2}} \\end{pmatrix} \\begin{pmatrix} r_{1y} \\\\ r_{2y} \\end{pmatrix}. \\] Particularmente, tenemos que: \\[ \\widehat{b}_{1} = \\frac{r_{1y}-r_{12}r_{2y}}{1-r_{12}^{2}} \\mbox{ y } \\widehat{b}_{2} = \\frac{r_{2y}-r_{12}r_{1y}}{1-r_{12}^{2}}. \\] Además, la matriz de varianzas-covarianzas de \\(\\widehat{\\boldsymbol b}\\) es: \\[\\begin{equation} V(\\widehat{\\boldsymbol b}) = \\sigma^{2} \\begin{pmatrix} \\frac{1}{1-r_{12}^{2}} &amp; \\frac{-r_{12}}{1-r_{12}^{2}} \\\\ \\frac{-r_{12}}{1-r_{12}^{2}} &amp; \\frac{1}{1-r_{12}^{2}} \\end{pmatrix}. \\tag{3.1} \\end{equation}\\] En (3.1) podemos observar que la varianza de los coeficientes \\(\\widehat{\\boldsymbol b}\\) tienden a infinito cuando \\(|r_{12}|\\rightarrow 1\\). Mientras que se hace mínima cuando las covariables están incorrelacionadas \\((r_{12}=0)\\). Es decir, una fuerte correlación entre \\(x_1\\) y \\(x_2\\) da como resultado grandes varianzas y covarianzas para los coeficientes \\(\\widehat{\\boldsymbol b}\\). Por esta razón es preferible tener covariables que sean ortogonales. Puesto que así se garantiza la menor varianza posible para \\(\\widehat{\\boldsymbol \\beta}\\). Sin embargo, las covariables son díficiles de controlar en estudios observacionales. Ahora consideremos un modelo con \\(p-1\\) covariables, \\[ y_{i}^{*} = b_{1}z_{i1} + b_{2}z_{i2} + \\ldots + b_{p-1}z_{i,p-1} + \\varepsilon_{i}. \\] Las ecuaciones normales son: \\[ \\begin{pmatrix} 1 &amp; r_{12} &amp; r_{13} &amp; \\ldots &amp; r_{1,p-1} \\\\ r_{12} &amp; 1 &amp; r_{23} &amp; \\ldots &amp; r_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{1,p-1} &amp; r_{2,p-1} &amp; r_{3,p-1} &amp; \\ldots &amp; 1 \\end{pmatrix} \\begin{pmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{p-1} \\end{pmatrix} = \\begin{pmatrix} r_{y1} \\\\ r_{y2} \\\\ \\vdots \\\\ r_{y,p-1} \\end{pmatrix}. \\] Por lo que el estimador de \\(\\boldsymbol b\\) es: \\[ \\widehat{\\boldsymbol b}= \\boldsymbol R^{-1}\\boldsymbol Z&#39;\\boldsymbol y^{*}, \\mbox{donde }\\boldsymbol R= \\boldsymbol Z&#39;\\boldsymbol Z. \\] La Varianza de \\(\\widehat{\\boldsymbol b}\\) está determinada por: \\[ V(\\widehat{\\boldsymbol b})=\\sigma^{2}\\boldsymbol R^{-1} = \\sigma^{2} \\begin{pmatrix} 1 &amp; \\boldsymbol r&#39; \\\\ \\boldsymbol r&amp; \\boldsymbol R_{22}, \\end{pmatrix}^{-1}, \\] donde \\(\\boldsymbol r= (r_{12},r_{13},\\ldots,r_{1,p-1})\\) y \\(\\boldsymbol R_{22}\\) es la matriz \\(\\boldsymbol R\\) eliminando la primer fila y columna. Particularmente para \\(\\widehat{b}_1\\), tenemos que: \\[ V(\\widehat{b}_1) = \\sigma^{2}\\frac{1}{1 - \\boldsymbol r&#39;\\boldsymbol R_{22}^{-1}\\boldsymbol r}. \\] Ahora asumamos que hacemos una regresión de \\(Z_1\\) en función de los demás regresores. Esto es: \\[\\begin{equation} z_1 = \\alpha_1 z_2 + \\alpha_2 z_3 + \\ldots + \\alpha_{p-2} z_{p-1}+\\varepsilon_1. \\tag{3.2} \\end{equation}\\] Las sumas de cuadrados totales y de los residuos de este modelo son: \\[ SST = \\sum_{i=1}^{n}z_{1i}^2 = 1, \\] y \\[ SS_{\\mbox{res}}= SST - SS_{\\mbox{reg}}= 1 - \\boldsymbol z_1&#39; \\boldsymbol Z_{(1)}(\\boldsymbol Z_{(1)}&#39;\\boldsymbol Z_{(1)})^{-1}\\boldsymbol Z_{(1)}&#39;\\boldsymbol z_1= 1 - \\boldsymbol r&#39;R_{22}^{-1}\\boldsymbol r, \\] respectivamente, donde \\(\\boldsymbol Z_{(1)}\\) es la matriz \\(\\boldsymbol Z\\) sin la primer columna. Además, el coeficiente de determinación de (3.1) es: \\[ R^{2}_1 = 1 - \\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}} = \\boldsymbol r&#39;R_{22}^{-1}\\boldsymbol r. \\] De aquí obtenemos que: \\[ V(\\widehat{b}_1) = \\sigma^{2} \\frac{1}{1-R^{2}_1}. \\] De forma similar se puede demostrar que: \\[ V(\\widehat{b}_j) = \\sigma^{2} \\frac{1}{1-R^{2}_j}, j=1,\\ldots,p-1, \\] donde \\(R^{2}_{j}\\) es el coeficiente de determinación de la regresión de \\(z_{j}\\) en función de las \\((p-2)\\) covariables restantes. Por lo que, si \\(R^{2}_{j} \\rightarrow 1\\), entonces \\(V(\\widehat{b}_{j})\\rightarrow \\infty\\). La multicolinealidad también produce que los estimadores \\(\\widehat{b}_{j}\\) sean muy grandes en valor absoluto. Para verificar esto considere: \\[ L_{1}^{2} = \\sum_{j=1}^{p}(\\widehat{b}_{j}-b_{j})^{2} = (\\widehat{\\boldsymbol b}- \\boldsymbol b)&#39;(\\widehat{\\boldsymbol b}- \\boldsymbol b). \\] El valor esperado de \\(L_{1}^{2}\\) es: \\[ E(L_{1}^{2}) = E[ (\\widehat{\\boldsymbol b}- \\boldsymbol b)&#39;(\\widehat{\\boldsymbol b}- \\boldsymbol b) ] = \\sum_{j=1}^{p-1}\\sigma^{2}\\mbox{tr}\\boldsymbol R^{-1} \\] Dado que la traza de una matriz es igual a la suma de sus valores propios, tenemos que: \\[ E(L_{1}^{2}) = \\sigma^{2}\\sum_{j=1}^{p-1}1/\\lambda_{j}, \\] donde \\(\\lambda_{j} &gt; 0\\), para \\(j=1,\\ldots,p-1\\), son los valores propios de \\(\\boldsymbol R\\). Por lo tanto, si \\(\\boldsymbol R\\) está mal condicionada debido a la multicolinealidad, al menos un \\(\\lambda_{j}\\) será muy pequeño. Cabe notar que, el problema de multicolinealidad afecta ningún supuesto sobre los errores. Por lo tanto, el estimador por MCO siguen siendo BLUE. Pero, en presencia de multicolinealidad: Las varianzas de los coeficientes estimados \\((\\widehat{\\beta}_{j})\\) se incrementan. Lo que reduce los valores \\(t\\) asociados. La estimación de los coeficientes del modelo \\((\\widehat{\\boldsymbol \\beta})\\) son sensibles a las especificaciones. Estos pueden cambiar drásticamente cuando se agregan o eliminan covariables. Sin embargo, el ajuste general del modelo, y por lo tanto las predicciones, no se verá tan afectado. 3.2.1 Causas de la multicolinealidad La multicolinealidad se puede deber a múltiples razones, entre las que están: Con frecuencia se presentan problemas donde intervienen procesos de producción o químicos, donde las covariables son los componentes de un producto y ésos suman una constante. Variables que son componentes de un sistema pueden mostrar dependencias casi lineales debido a las limitaciones biológicas, físicas o químicas del sistema. Variables que tienen una tendencia común y evolucionan de forma muy parecida en el tiempo. Inclusión de variables irrelevantes en el modelo. La información que contienen estas variables ya estarían incluidas en otras y no aportan a la explicación de la variabilidad de la variable respuesta. Por ejemplo, en los datos del cemento, tenemos que el problema de multicolinealidad se presenta porque: \\[ x_{i1} + x_{i2} + x_{i3} + x_{i4} \\approx \\mbox{constante}. \\] En los datos de grasa corporal, el problema es causado por la alta correlación entre el pliegue cutáneo del triceps \\((X_{1})\\) y la circunferencia del muslo \\((X_{2})\\). 3.3 Detección de multicolinealidad Dado que la multicolinealidad provoca una inflación de la varianza, un indicio de este problema está en que aunque el modelo presenta un buen ajuste (un \\(R^2\\) por ejemplo), las estimaciones de coeficientes asociados a covariables releventas tienen valores \\(t\\) pequeños. Además, al eliminar covariables, las estimacioes cambian considerablemente. A manera inicial, la multicolinealidad se puede detectar por medio de la evaluación de la matriz de correlación. Sin embargo, hay indicadores más formales para ello. Estos son los factores de inflación de varianza (VIF), y los índices de condiciones de \\(\\boldsymbol R\\) o \\(\\boldsymbol Z\\). 3.3.1 Factores de inflación de varianza Dado que \\(V(\\widehat{\\boldsymbol b}) = \\sigma^2\\boldsymbol R^{-1}\\), los elementos de la diagonal de \\(\\boldsymbol R^{-1}\\) son buenos indicadores de multicolinealidad: \\[ VIF_j =\\{\\boldsymbol R^{-1}\\}_{jj} = (1-R_{j}^{2})^{-1}, \\] donde \\(R_{j}^{2}\\) es el coeficiente de determinación de ajustar un modelo de \\(x_{j}\\) en función de las demás covariables. Si \\(x_{j}\\) es casi ortogonal a las demás covariables, \\(R^{2}_{j}\\) es pequeño, y por lo tanto, \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) cercano a \\(1\\). Mientras que, si \\(x_{j}\\) es casi linealmente dependiente a las demás covariables, entonces \\(R^{2}_{j}\\) es cercano a \\(1\\) y \\(\\{\\boldsymbol R^{-1}\\}_{jj}\\) grande. Por lo tanto, \\(VIF_j\\) puede ser visto como un factor de cuanto se incrementa \\(V(\\widehat{b}_{j})\\) debido a la colinealidad entre las covariables. De aquí el nombre de factor de inflación de varianza (VIF). Generalmente, uno o mas valores grandes de VIF (\\(5\\) o \\(10\\)) es un indicador de problemas de multicolinealidad. 3.3.2 Valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\) Dado que \\(\\boldsymbol R= \\boldsymbol Z&#39;\\boldsymbol Z\\) es una matriz simétrica y positiva semi-definida, entonces se puede descomponer de la siguiente forma: \\[ \\boldsymbol R= \\boldsymbol T\\boldsymbol \\Lambda\\boldsymbol T&#39;, \\] donde \\(\\boldsymbol T= (\\boldsymbol t_{1},\\ldots,\\boldsymbol t_{k})\\) es una matriz ortogonal de vectores propios, y \\(\\boldsymbol \\Lambda\\) una matriz diagonal con los valores propios \\(\\lambda_{j}\\), para \\(j=1,\\ldots,p-1\\), en la diagonal principal. Por lo tanto: \\[ V(\\widehat{\\boldsymbol \\beta}) = \\sigma^{2}\\boldsymbol T\\boldsymbol \\Lambda^{-1}\\boldsymbol T&#39;. \\] Entonces, \\[ V(\\widehat{b}_{j}) = \\sigma^{2}\\sum_{k=1}^{p-1}\\frac{t_{jk}^{2}}{\\lambda_{k}}. \\] De la expresión anterior se puede ver la relación de los valores propios con el VIF: \\[ VIF_{j} = \\sum_{j=1}^{p-1}\\frac{t_{ij}^{2}}{\\lambda_{j}}, \\] Por lo tanto, uno o más valores propios pequeños pueden inflar la varianza de \\(\\widehat{b}_{j}\\). De aquí salen los indicadores llamados índices de condición y número de condición. Los índices de condición están definidos como: \\[ \\eta_{j} = \\frac{\\lambda_{\\mbox{max}}}{\\lambda_{j}}. \\] El número de condición está definido como el máximo índice de condición. Un número de condición mayor de \\(100\\) es un indicador de multicolinealidad. 3.3.3 Valores singulares de \\(\\boldsymbol Z\\) El número de condición también se puede calcular a partir de la descomposición en valores singulares (SVD) de la matriz de covariables \\(\\boldsymbol Z\\). Esto es: \\[ \\boldsymbol Z=\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39;, \\] donde \\(\\boldsymbol U\\) y \\(\\boldsymbol T\\) son matrices \\(n\\times (p-1)\\) y \\((p-1)\\times (p-1)\\), respectivamente, \\(\\boldsymbol U\\boldsymbol U= \\boldsymbol I\\) y \\(\\boldsymbol T&#39;\\boldsymbol T= \\boldsymbol I\\), y \\(\\boldsymbol D\\) es una matriz diagonal con elementos \\(\\mu_{j}\\), \\(j=1,\\ldots,p-1\\), llamados valores singulares. Hay una relación entre los valores singulares de \\(\\boldsymbol Z\\) y los valores propios de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\): \\[ \\boldsymbol Z&#39;\\boldsymbol Z= (\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39;)&#39;\\boldsymbol U\\boldsymbol D\\boldsymbol T&#39; = \\boldsymbol T\\boldsymbol D^2\\boldsymbol T&#39; = \\boldsymbol T\\boldsymbol \\Lambda\\boldsymbol T&#39;. \\] El índice de condición de \\(\\boldsymbol Z\\) está definido como: \\[ \\kappa_{j} = \\frac{\\mu_{\\mbox{max}}}{\\mu_{j}}. \\] El número de condición de \\(\\boldsymbol Z\\) está definido como el máximo \\(\\kappa_{j}\\). Un número de condición mayor de 10, 15 o 30 es un indicador de problemas de multicolinealidad. Note que \\(\\kappa_{j} = \\sqrt{\\eta_{j}}\\). Por lo que el punto de corte de \\(10\\) para el número de condición de \\(\\boldsymbol Z\\) es equivalente al número de condición de \\(100\\) para el número de condición de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\). 3.3.4 Proporciones de descomposición de varianza Hay una relación entre el VIF, los valores propios y los valores singulares. Esta es: \\[ VIF_{j} = \\sum_{j=1}^{p-1}\\frac{t_{ij}^{2}}{\\lambda_{j}} = \\sum_{j=1}^{p-1}\\frac{t_{ij}^{2}}{\\mu_{j}^2}. \\] De aquí salen los indicadores llamados proporciones de descomposición de varianza. Estos se calculan de la siguiente forma: \\[ \\pi_{ij} = \\frac{t_{ij}^2 / \\mu_{i}^2 }{VIF_{j}}, j = 1,\\ldots, p-1. \\] Si se agrupan los \\(\\pi_{ij}\\) en una matriz \\((p-1) \\times (p-1)\\) \\(\\boldsymbol \\pi\\), la columna \\(j\\) son proporciones de la varianza de \\(\\widehat{\\boldsymbol b}\\). Valores \\(\\pi_{ij}\\) mayores de \\(0.5\\) indican problemas de multicolinealidad. 3.4 Datos de cemento Los VIF para el modelo ajustado para los datos de cemento son los siguientes: car::vif(mod.cement) ## x1 x2 x3 x4 ## 38.49621 254.42317 46.86839 282.51286 Aquí vemos que todos los VIFs son muy altos, mostrando que hay problemas graves de multicolinealidad. Para calcular los índices de condición (a partir de los valores singulares de \\(\\boldsymbol Z\\)) y las proporciones de descomposición de varianza se utiliza la función eigprop() de la librería mctest. Sin embargo, es necesario ajustar : ### escalamiento de variables cement.scale = as.data.frame(scale(cement)/sqrt(nrow(cement)-1)) ### modelo con datos escalados mod.cement.scale =lm(y~x1+x2+x3+x4,data=cement.scale) ### calculo de los índices de condición y proporciones de descomposición ### de varianza library(mctest) eigprop(mod.cement.scale) ## ## Call: ## eigprop(mod = mod.cement.scale) ## ## Eigenvalues CI (Intercept) x1 x2 x3 x4 ## 1 2.2357 1.0000 0 0.0026 0.0006 0.0015 0.0005 ## 2 1.5761 1.1910 0 0.0043 0.0004 0.0050 0.0005 ## 3 1.0000 1.4952 1 0.0000 0.0000 0.0000 0.0000 ## 4 0.1866 3.4613 0 0.0635 0.0021 0.0465 0.0007 ## 5 0.0016 37.1063 0 0.9296 0.9969 0.9471 0.9983 ## ## =============================== ## Row 5==&gt; x1, proportion 0.929579 &gt;= 0.50 ## Row 5==&gt; x2, proportion 0.996931 &gt;= 0.50 ## Row 5==&gt; x3, proportion 0.947067 &gt;= 0.50 ## Row 5==&gt; x4, proportion 0.998343 &gt;= 0.50 El número de condición \\((37.106)\\) es muy alto reafirmando el problema de multicolinealidad. Además, también se puede observar que varias de las proporciones de descomposición de varianza son mayores de \\(0.5\\). 3.5 Datos de grasa corporal Para el modelo ajustado a los datos de grasa corporal, los VIF son: car::vif(mod.bodyfat) ## Triceps Thigh Midarm ## 708.8429 564.3434 104.6060 Además, los índices de condición y las proporciones de descomposición de varianza son: ### escalamiento de variables bodyfat.scale = as.data.frame(scale(bodyfat)/sqrt(nrow(bodyfat)-1)) ### modelo con datos escalados mod.bodyfat.scale =lm(Fat~Triceps+Thigh+Midarm,data=bodyfat.scale) ### calculo de los índices de condición y proporciones de descomposición ### de varianza eigprop(mod.bodyfat.scale) ## ## Call: ## eigprop(mod = mod.bodyfat.scale) ## ## Eigenvalues CI (Intercept) Triceps Thigh Midarm ## 1 2.0665 1.0000 0 0.0003 0.0003 0.0006 ## 2 1.0000 1.4375 1 0.0000 0.0000 0.0000 ## 3 0.9328 1.4884 0 0.0000 0.0004 0.0082 ## 4 0.0007 53.3287 0 0.9997 0.9993 0.9912 ## ## =============================== ## Row 4==&gt; Triceps, proportion 0.999667 &gt;= 0.50 ## Row 4==&gt; Thigh, proportion 0.999292 &gt;= 0.50 ## Row 4==&gt; Midarm, proportion 0.991205 &gt;= 0.50 Todos estos indicadores muestran que hay problemas de multicolinealidad. 3.6 Solución al problema de multicolinealidad Los problemas de multicolinealidad se pueden solucionar de las siguientes formas: recolección de datos adicionales: si se tiene control sobre las covariables, es posible tomar más observaciones para romper con la casi dependencia en \\(\\boldsymbol X\\). Pero esto en muchas casos es díficil por la naturaleza de las covariables. Por ejemplo, sería imposible buscar personas con circunferencia del muslo grande y pliegue cutáneo del tríceps bajo (o viceversa). re-especificación del modelo: se pueden eliminar del modelo las covariables que me generan el problema de multicolinealidad y que pueden tener poco aporte explicativo dentro del modelo. Por ejemplo, en los datos de la grasa corporal podemos eliminar la variable asociada al muslo (o al tríceps, pues ambas proporcionan la misma información). Si quitamos esta covariable: mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat) car::vif(mod.bodyfat0) ## Triceps Midarm ## 1.265118 1.265118 los VIF disminuyen considerablemente. Además, no hay una reducción notable del \\(R^2\\). Con las tres covariables temenos que \\(R^{2}=0.801\\). Mientras que al remover Thigh, tenemos que \\(R^{2} = 0.786\\) Uso de estimadores sesgados: Considerando que el estimador por MCO es el mejor estimador lineal insesgado de \\(\\boldsymbol \\beta\\), podemos relajar la condición de insesgamiento y buscar estimadores que aunque sean sesgados tengan menor varianza que los estimadores por MCO. Dos de estas alternativas son el estimador de ridge y el estimadores por componentes principales. 3.7 Estimador de ridge El estimador de ridge tiene como objetivo minimizar la siguiente suma de cuadrados penalizada: \\[\\begin{equation} \\begin{split} S_{k}(\\boldsymbol b) &amp;= \\sum_{i=1}^{n}(y_{i} - \\boldsymbol z_{i}&#39;\\boldsymbol b)^{2} + k \\sum_{j=1}^{p-1}\\boldsymbol b_{j}^2 \\\\ &amp;= (\\boldsymbol y- \\boldsymbol Z\\boldsymbol b)&#39;(\\boldsymbol y- \\boldsymbol Z\\boldsymbol b) + k ||\\boldsymbol b||_{2}^{2}, \\end{split} \\nonumber \\end{equation}\\] con \\(k \\geq 0\\) (el cual es seleccionado por el investigador). Lo que lleva a las siguientes ecuaciones normales: \\[ (\\boldsymbol R+ k \\boldsymbol I)\\widehat{\\boldsymbol b}_{k} = \\boldsymbol Z&#39;\\boldsymbol y. \\] La solución de las ecuaciones normales lleva al estimador ridge: \\[ \\widehat{\\boldsymbol b}_{k} = (\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol Z&#39;\\boldsymbol y. \\] Note que, incluso si \\(\\boldsymbol R\\) es no es invertible, un \\(k &gt; 0\\) resuelve el problema. Además, la solución depende de \\(k\\) (por cada \\(k\\), hay una estimación diferente). Se puede considerar a \\(k\\) como un parámetro de contracción. Si \\(k \\rightarrow 0\\), tenemos que \\(\\widehat{\\boldsymbol b}_{R} \\rightarrow \\widehat{\\boldsymbol b}\\). Mientras que, si \\(k \\rightarrow \\infty\\), encontramos que \\(\\widehat{\\boldsymbol b}_{R} \\rightarrow \\boldsymbol 0\\) (excepto el intercepto). Ahora veamos las propiedades de \\(\\widehat{\\boldsymbol b}_{k}\\). Este se puede expresar como: \\[ \\widehat{\\boldsymbol b}_k = (\\boldsymbol R+ k \\boldsymbol I)^{-1}\\boldsymbol R\\boldsymbol y= (\\boldsymbol I+ k \\boldsymbol R^{-1})^{-1}\\widehat{\\boldsymbol b}= \\boldsymbol C\\widehat{\\boldsymbol b}. \\] Por lo que, su valor esperado es: \\[ E(\\widehat{\\boldsymbol b}_{k}) =\\boldsymbol C\\boldsymbol b\\neq \\boldsymbol b, \\] de aquí vemos que \\(\\widehat{\\boldsymbol b}_{k}\\) es sesgado, y el sesgo aumenta con \\(k\\) (por lo que este parámetro también es llamado de sesgo). La varianza de \\(\\widehat{\\boldsymbol b}_k\\) está dada por: \\[ V(\\widehat{\\boldsymbol b}_{k}) = \\sigma^2 \\boldsymbol C&#39; \\boldsymbol R^{-1} \\boldsymbol C. \\] A partir de las cantidades anteriores se puede determinar el error cuadrático medio (ECM) de \\(\\widehat{\\boldsymbol b}_{R}\\): \\[\\begin{equation} \\begin{split} \\mbox{MSE}(\\widehat{\\boldsymbol b}_{k}) &amp;= \\mbox{tr}V(\\widehat{\\boldsymbol b}_k) + \\left[ E(\\widehat{\\boldsymbol b}_{k}) - \\boldsymbol b\\right]&#39;\\left[ E(\\widehat{\\boldsymbol b}_{k}) - \\boldsymbol b\\right] \\\\ &amp;= \\sigma^2\\mbox{tr}\\left( R^{-1}\\boldsymbol C&#39;\\boldsymbol C\\right) + \\boldsymbol b&#39;(\\boldsymbol C-\\boldsymbol I)&#39;(\\boldsymbol C-\\boldsymbol I)\\boldsymbol b\\\\ &amp;=\\sigma^{2}\\sum_{j=1}^{p-1}\\frac{\\lambda_{j}}{(\\lambda_{j}+ k)^2} + \\sum_{j=1}^{p-1}\\frac{\\alpha_j^{2}k^2}{(\\lambda_{j}+ k)^2}, \\end{split} \\nonumber \\end{equation}\\] donde \\(\\boldsymbol \\alpha= \\boldsymbol T&#39;\\boldsymbol b\\). Finalmente, la suma de cuadrados de los residuos usando \\(\\widehat{\\boldsymbol b}_{k}\\) es: \\[ SS_{res}(\\widehat{\\boldsymbol b}_{k}) =(\\boldsymbol y- \\boldsymbol Z\\widehat{\\boldsymbol b})&#39;(\\boldsymbol y- \\boldsymbol Z\\widehat{\\boldsymbol b}) + (\\widehat{\\boldsymbol b}_{k}-\\widehat{\\boldsymbol b})&#39;\\boldsymbol R(\\widehat{\\boldsymbol b}_{k}-\\widehat{\\boldsymbol b}) = SS_{res}(\\widehat{\\boldsymbol b}) + (\\widehat{\\boldsymbol b}_{k}-\\widehat{\\boldsymbol b})&#39;\\boldsymbol R(\\widehat{\\boldsymbol b}_{k}-\\widehat{\\boldsymbol b}). \\] A partir de estos resultados, vemos que: (1) Si \\(k\\) crece, disminuye la varianza, pero aumenta el sesgo. (2) Si \\(k\\) disminuye, aumenta la varianza, pero disminuye el sesgo. (3) el estimador ridge proporciona un coeficiente de determinación mas pequeño que el estimador por MCO. Sin embargo, las estimaciones de \\(\\boldsymbol b\\) son más estables. La idea de la regresión de ridge es encontrar un valor de \\(k\\) tal que \\(\\mbox{ECM}(\\widehat{\\boldsymbol b}_{k}) &lt; V(\\widehat{\\boldsymbol b})\\). En la Figura 3.3 podemos ver la representación del ECM del estimador de ridge. A medida que aumenta \\(k\\), el sesgo incrementa y la varianza disminuye. Además, hay una región de \\(k\\) donde se puede obtener un ECM del estimador de ridge menor que el que se obtiene por medio de MCO. La idea es encontrar el valor de \\(k\\) que minimiza el ECM, o por lo menos algún valor en la región donde \\(\\mbox{ECM}(\\widehat{\\boldsymbol b}_{k}) &lt; V(\\widehat{\\boldsymbol b})\\). Figure 3.3: Representación del sesgo al cuadrado (linea cortada), varianza (linea punteada) y error cuadrático medio (linea solida) del estimador de ridge. La linea roja representa el error cuadrático medio del estimador por MCO. Algunos métodos de selección de \\(k\\) son: Traza de ridge \\(k\\): el efecto de \\(k\\) sobre las estimaciones de \\(\\widehat{\\boldsymbol b}_{k}\\) es mas fuerte para valores bajos. De igual forma, si \\(k\\) es muy grande se introduce mucho sesgo. Por lo que se puede hacer es incrementar \\(k\\) hasta que parezca que su influencia sobre \\(\\widehat{\\boldsymbol b}_{k}\\) se atenúe. Validación cruzada (CV): sea \\(\\widehat{y}_{(i),k}\\) la estimación de \\(E(y_i)\\) por medio del estimador de ridge con el parámetro \\(k\\) y usando una muestra excluyendo la i-ésima observación. La validación cruzada está definida como: \\[ CV(k) = \\sum_{i=1}^{n} (y_i - \\widehat{y}_{(i),k})^2. \\] Por lo que la selección de \\(k\\) es: \\[ k_{CV} = \\arg \\min_{k} CV(k). \\] 3.7.1 Datos de cemento Para ajustar el modelo usando el estimador de ridge podemos usar la función lmridge del paquete lmridge. Primero, ajustamos el modelo usando diferentes valores de \\(k\\): library(lmridge) K = seq(from=0,to=0.3,length.out = 100) ridge.cement = lmridge(y~., data=cement,K=K,scaling=&#39;sc&#39;) En el objeto ridge.cement tenemos las estimaciones por el estimador de ridge para \\(100\\) valores de \\(k\\) entre \\(0\\) y \\(0.3\\). Para observar como cambian las estimaciones para los diferentes valores de \\(k\\) podemos graficar la traza de ridge así (en términos de las covariables en su escala orginal): EstRidge.cement = coef(ridge.cement) plot(K,EstRidge.cement[,2],type=&#39;l&#39;,ylim=range(EstRidge.cement[,-1]),lwd=2, ylab=&#39;Estimaciones de los coeficientes&#39;,xlab=&#39;k&#39;) lines(K,EstRidge.cement[,3],col=2,lwd=2) lines(K,EstRidge.cement[,4],col=3,lwd=2) lines(K,EstRidge.cement[,5],col=4,lwd=2) abline(h=0,lty=2) Figure 3.4: Datos de cemento. Traza de ridge. Coeficiente asociado a X1 (negro), coeficiente asociado a X2 (rojo), coeficiente asociado a X3 (verde) y coeficiente asociado a X4 (azul) En la Figura 3.4 podemos observar que, cuando incrementamos \\(k\\), las estimaciones cambian rápidamente y luego parecen estabilizarse cuando \\(k\\) es grande. Además hay un cambio de signo para el coeficiente asociado a X3. También puede usarse plot(mod.r) (traza de ridge para las estimaciones de los coeficientes asociados a las covariables escaladas). La selección del \\(k\\) óptimo por medio de validación cruzada (CV) se hace de la siguiente manera: Criterios.cement = kest(ridge.cement) plot(K,Criterios.cement$CV,type=&#39;l&#39;,xlab=&#39;K&#39;,ylab=&#39;validación cruzada&#39;) Figure 3.5: Datos de cemento. Validación cruzada. K[Criterios.cement$CV==min(Criterios.cement$CV)] ## [1] 0.009090909 Aquí podemos ver que el valor de \\(k\\) que minimiza la validación cruzada es \\(0.009\\). El valor óptimo de \\(k\\) por medio de otros criterios son: Criterios.cement ## Ridge k from different Authors ## ## k values ## Minimum CV at K 0.00909 ## Minimum GCV at K 0.02424 ## Thisted (1976): 0.00581 ## LW (lm.ridge) 0.05183 ## LW (1976) 0.00797 ## HKB (1975) 0.01162 ## Dwividi &amp; Srivastava (1978): 0.00291 ## Kibria (2003) (AM) 0.28218 ## Kibria 2003 (GM): 0.07733 ## Kibria 2003 (MED): 0.01718 ## Muniz et al. 2009 (KM2): 14.84574 ## Muniz et al. 2009 (KM3): 5.32606 ## Muniz et al. 2009 (KM4): 3.59606 ## Muniz et al. 2009 (KM5): 0.27808 ## Muniz et al. 2009 (KM6): 7.80532 ## Mansson et al. 2012 (KMN8): 14.98071 ## Mansson et al. 2012 (KMN9): 0.49624 ## Mansson et al. 2012 (KMN10): 6.63342 ## Mansson et al. 2012 (KMN11): 0.15075 ## Mansson et al. 2012 (KMN12): 8.06268 ## Dorugade et al. 2010: 0.00000 ## Dorugade et al. 2014: 101.64433 La estimación con \\(K=0.0101\\) se puede obtener usando la función lmridge: ridge.cement2 = lmridge(y~., data=cement,K=0.0101,scaling=&#39;sc&#39;) summary(ridge.cement2) ## ## Call: ## lmridge.default(formula = y ~ ., data = cement, K = 0.0101, scaling = &quot;sc&quot;) ## ## ## Coefficients: for Ridge parameter K= 0.0101 ## Estimate Estimate (Sc) StdErr (Sc) t-value (Sc) Pr(&gt;|t|) ## Intercept 82.7052 -267.6034 306.3344 -0.8736 0.4052 ## x1 1.3146 26.7886 3.9606 6.7637 0.0001 *** ## x2 0.3059 16.4871 5.2766 3.1246 0.0124 * ## x3 -0.1295 -2.8734 3.9360 -0.7300 0.4841 ## x4 -0.3432 -19.8985 5.4000 -3.6849 0.0051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Ridge Summary ## R2 adj-R2 DF ridge F AIC BIC ## 0.97180 0.96240 3.07629 133.92403 23.27712 58.35941 ## Ridge minimum MSE= 392.3519 at K= 0.0101 ## P-value for F-test ( 3.07629 , 9.74488 ) = 3.013147e-08 ## ------------------------------------------------------------------- Vemos algunas diferencias con las estimaciones por MCO. Hay un cambio de signo en la estimación del coeficiente asociado a x3. Si embargo, no hay mucha diferencia en las estimaciones de los otros coeficientes. También podemos observar que las covariables x1, x2 y x3 ahora tienen un aporte significativo. Como era de esperarse, hay una disminución del \\(R^{2}\\), sin embargo es muy leve. 3.8 Estimador por componentes principales Considere el modelo en su forma canónica: \\[ \\boldsymbol y^{*} = \\boldsymbol Z\\boldsymbol T\\boldsymbol \\alpha+ \\boldsymbol \\varepsilon, \\] donde \\(\\boldsymbol \\alpha= \\boldsymbol T&#39;\\boldsymbol b\\), \\((\\boldsymbol Z\\boldsymbol T)&#39;\\boldsymbol Z\\boldsymbol T=\\boldsymbol \\Lambda\\), \\(\\boldsymbol \\Lambda\\) es una matriz diagonal de valores propios \\((\\lambda_1,\\ldots,\\lambda_p)\\) de \\(\\boldsymbol Z&#39;\\boldsymbol Z\\), y \\(\\boldsymbol T= (\\boldsymbol t_{1},\\ldots,\\boldsymbol t_{p-1})\\) es la matriz ortogonal de vectores propios asociados \\(\\boldsymbol \\Lambda\\). Las columnas de \\(\\boldsymbol P= \\boldsymbol Z\\boldsymbol T= (\\boldsymbol p_{1},\\ldots,\\boldsymbol p_{p-1})\\) son un conjunto de regresores ortogonales (llamados componentes principales): \\[ \\boldsymbol p_{k} = \\sum_{j=1}^{p-1} t_{kj}\\boldsymbol z_{j}. \\] El estimador de \\(\\boldsymbol \\alpha\\) por MCO es: \\[ \\widehat{\\boldsymbol \\alpha}= (\\boldsymbol P&#39;\\boldsymbol P)^{-1}\\boldsymbol P&#39;\\boldsymbol y^{*} = \\boldsymbol \\Lambda^{-1}\\boldsymbol P&#39;\\boldsymbol y^{*}, \\] y la varianza de \\(\\widehat{\\boldsymbol \\alpha}\\) es: \\[ V(\\widehat{\\boldsymbol \\alpha}) = \\sigma^{2}(\\boldsymbol P&#39;\\boldsymbol P)^{-1} = \\sigma^{2}\\boldsymbol \\Lambda^{-1}. \\] De aquí podemos ver que valores propios de \\(\\boldsymbol R\\) están asociados con la varianza de los coeficientes de regresión. Si \\(\\lambda_j=1\\) (para \\(j=1,\\ldots,p\\)), las covariables originales son ortogonales. Mientras que valores propios cercanos a cero indican problemas de multicolinealidad dado que inflan la varianza de \\(\\widehat{\\boldsymbol \\alpha}\\). Note que, para \\(\\widehat{\\boldsymbol b}= \\boldsymbol T\\widehat{\\boldsymbol \\alpha}\\), tenemos: \\[ V(\\widehat{\\boldsymbol b}) = V(\\boldsymbol T\\widehat{\\boldsymbol \\alpha}) =\\sigma^{2}\\boldsymbol T\\boldsymbol \\Lambda^{-1}\\boldsymbol T&#39; = \\sigma^{2} \\sum_{j=1}^{p-1}\\frac{\\boldsymbol t_{j}\\boldsymbol t_{j}&#39;}{\\lambda_{j}}, \\] lo que implica que \\(V(\\widehat{b}_{k}) = \\sigma^{2} \\sum_{j=1}^{p-1}t_{kj}^{2}/\\lambda_{j}\\), la varianza de \\(\\widehat{b}_j\\) es una combinación lineal de los valores propios. Para combatir el problema de multicolinealidad, la idea de la regresión por componentes principales es usar un subconjunto de \\(\\boldsymbol P\\) como regresores (en vez de todos los compontes). Para esto, eliminamos los componentes principales \\((\\boldsymbol p_{r+1},\\ldots,\\boldsymbol p_{p-1})\\) asociados a los valores propios cercanos a cero \\((\\lambda_{r+1},\\ldots,\\lambda_{p-1})\\). Aquí estamos asumiendo que \\(\\lambda_1 \\leq \\lambda_2 \\leq \\ldots \\leq \\lambda_{p-1}\\). Esto es, \\[ \\widehat{\\boldsymbol \\alpha}_{r} = (\\overbrace{\\widehat{\\alpha}_{1},\\widehat{\\alpha}_{2},\\ldots, \\widehat{\\alpha}_{r}}^{r},\\overbrace{0, \\ldots,0}^{p-1-r})&#39;. \\] De esta forma el estimador de \\(\\boldsymbol b\\) por CP está dado por: \\[ \\widehat{\\boldsymbol b}_{r} = \\boldsymbol T\\widehat{\\boldsymbol \\alpha}_{r} =\\sum_{l=1}^{r} \\boldsymbol t_l \\widehat{\\alpha}_l, \\] El sesgo de \\(\\widehat{\\boldsymbol b}_{r}\\) es igual a: \\[ \\widehat{\\boldsymbol b}_{r} - \\boldsymbol b= \\sum_{l=1}^{r} \\boldsymbol t_l \\widehat{\\alpha}_l - \\sum_{l=1}^{p-1} \\boldsymbol t_l \\alpha_l = - \\sum_{l = r+1}^{p-1} \\boldsymbol t_l \\alpha_l. \\] La varianza de \\(\\widehat{\\boldsymbol b}_{r}\\) es: \\[ V(\\widehat{\\boldsymbol b}_{r}) = \\boldsymbol TV(\\widehat{\\boldsymbol \\alpha}_{r})\\boldsymbol T&#39; = \\sigma^{2}\\sum_{l=1}^{r} \\lambda_{l}^{-1}\\boldsymbol t_{l}\\boldsymbol t_{l}&#39; \\leq \\sigma^{2} \\sum_{l=1}^{p-1}\\lambda_l^{-1}\\boldsymbol t_{l}\\boldsymbol t_{l}&#39; = V(\\widehat{\\boldsymbol b}). \\] Por lo tanto, al eliminar componentes principales se aumenta el sesgo, pero se disminuye la varianza de \\(\\widehat{\\boldsymbol b}_r\\). Finalmente, el ECM de \\(\\widehat{\\boldsymbol b}_r\\) está dado por: \\[ MSE(\\widehat{\\boldsymbol b}_r) = \\sigma^{2}\\sum_{i=1}^{r}\\frac{1}{\\lambda_l} + \\sum_{l=r+1}^{p-1}\\alpha_l^2. \\] 3.8.1 Datos de cemento Antes de ajustar el modelo vamos a escalar las variables escalar &lt;- function(x) {(x-mean(x)) / sqrt(sum((x-mean(x))^2))} X = as.matrix(cement[,1:4]) y.e = escalar(cement$y) Z = apply(cement[,1:4],2,escalar) A partir de las variables escaladas podemos calcular los vectores y valores propios: T.mat = eigen(t(Z)%*%Z)$vectors lambda = eigen(t(Z)%*%Z)$values lambda ## [1] 2.235704035 1.576066070 0.186606149 0.001623746 Aquí podemos observar que un valor propio es cercano a cero, por lo que lo podemos eliminarlo para remediar el problema de multicolinealidad: P = Z%*%T.mat PCR.cement = lm(y.e~P[,-4]-1) summary(PCR.cement) ## ## Call: ## lm(formula = y.e ~ P[, -4] - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.058393 -0.036851 0.006961 0.022836 0.073963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## P[, -4]1 -0.656958 0.028271 -23.238 4.93e-10 *** ## P[, -4]2 -0.008309 0.033671 -0.247 0.8101 ## P[, -4]3 0.302770 0.097856 3.094 0.0114 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.04227 on 10 degrees of freedom ## Multiple R-squared: 0.9821, Adjusted R-squared: 0.9768 ## F-statistic: 183.2 on 3 and 10 DF, p-value: 4.895e-09 Las estimaciones de los coeficientes \\((\\boldsymbol b)\\) para las variables escaladas: beta.CP =T.mat%*%c(PCR.cement$coefficients,0) beta.CP ## [,1] ## [1,] 0.51297502 ## [2,] 0.27868114 ## [3,] -0.06078483 ## [4,] -0.42288461 La matriz de varianza de \\(\\boldsymbol b_{r}\\) es: sigma2.pc = sum(PCR.cement$residuals^2)/(13-4) Var.b = sigma2.pc*T.mat%*%diag(c(1/lambda[1:3],0))%*%t(T.mat) Var.b ## [,1] [,2] [,3] [,4] ## [1,] 0.005382414 -0.0022868445 0.004028698 -0.0013467849 ## [2,] -0.002286844 0.0015500408 -0.002015159 0.0001440783 ## [3,] 0.004028698 -0.0020151593 0.004925579 -0.0014780352 ## [4,] -0.001346785 0.0001440783 -0.001478035 0.0009294415 "],["selección-de-variables.html", "Capítulo 4 Selección de variables 4.1 Ejemplos 4.2 Problema de selección de variables 4.3 Métodos para la selección de variables 4.4 Comparación de los modelos 4.5 Regresión de LASSO", " Capítulo 4 Selección de variables 4.1 Ejemplos 4.1.1 Unidad quirúrgica Una unidad quirúrgica de un hospital está interesada en predecir la supervivencia de los pacientes sometidos a un tipo particular de operación hepática. Se dispuso de una selección aleatoria de \\(108\\) pacientes para el análisis. De cada registro del paciente, se extrajo la siguiente información de la evaluación preoperatoria: bcs: coagulación sanguínea. pindex: índice de pronóstico. enzyme: función enzimática. liver_test: función hepática. age: edad. gender: genero (0 = masculino, 1 = femenino). alc_mod: historial de consumo de alcohol (0 = Ninguno, 1 = Moderado). alc_heavy: &amp; historial de consumo de alcohol (0 = Ninguno, 1 = Fuerte). y: tiempo de supervivencia. El objetivo del estudio es determinar los factores que influyen sobre el tiempo de supervivencia (que se determinó posteriormente) en función de las demás variables. El modelo propuesto es el siguiente: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0}+\\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2}+ \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{liver}_{i}\\beta_{4} + \\mbox{age}_{i}\\beta_{5} + \\mbox{gender}_{i}\\beta_{6}+ \\\\ &amp; \\mbox{alc_mod}_{i}\\beta_{7} + \\mbox{alc_heavy}_{i}\\beta_{8} + \\varepsilon_{i} \\end{split} \\nonumber \\end{equation}\\] El ajuste del modelo es: library(olsrr) data(surgical) surgical$log.y = log(surgical$y) mod.surgical.completo = lm(log.y~bcs+pindex+enzyme_test+liver_test+age+gender+alc_mod+alc_heavy,data=surgical) summary(mod.surgical.completo) ## ## Call: ## lm(formula = log.y ~ bcs + pindex + enzyme_test + liver_test + ## age + gender + alc_mod + alc_heavy, data = surgical) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35555 -0.13849 -0.05179 0.14912 0.46349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.050949 0.251741 16.092 &lt; 2e-16 *** ## bcs 0.068551 0.025420 2.697 0.00982 ** ## pindex 0.013459 0.001947 6.913 1.37e-08 *** ## enzyme_test 0.014948 0.001809 8.261 1.44e-10 *** ## liver_test 0.007931 0.046706 0.170 0.86592 ## age -0.003567 0.002751 -1.296 0.20145 ## gender 0.084151 0.060746 1.385 0.17279 ## alc_mod 0.057313 0.067480 0.849 0.40019 ## alc_heavy 0.388190 0.088374 4.393 6.73e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2093 on 45 degrees of freedom ## Multiple R-squared: 0.8461, Adjusted R-squared: 0.8187 ## F-statistic: 30.93 on 8 and 45 DF, p-value: 7.823e-16 4.1.2 Grasa corporal La medición de la grasa corporal es un proceso complejo. Dado que los músculos y los huesos son más densos, el calculo de % de grasa corporal se basa, entre otros aspectos, en la medición de la densidad corporal la cuál requiere sumergir a las personas en el agua. Por esta razón, se quiere buscar un método más sencillo para determinarlo. Para esto, se registraron la edad, el peso, la altura y \\(10\\) medidas de la circunferencia corporal de \\(252\\) hombres. De igual forma, a cada uno de estos hombres se les midió el % de grasa corporal de forma precisa (usando la ecuación de Brozek, medición a partir de la densidad). Cómo variable respuesta se utiliza la medición por el método de Brozek, y las posibles covariables son: age: edad (en años). weight: peso (en libras). height: altura (en pulgadas). neck: circunferencia del cuello (en centímetros). chest: circunferencia del pecho (en centímetros). abdom: circunferencia del abdomen (en centímetros). hip: circunferencia de la cadera (en centímetros). thigh:circunferencia del muslo (en centímetros). knee:circunferencia de la rodilla (en centímetros). ankle:circunferencia del tobillo (en centímetros). biceps: circunferencia del bíceps extendido (en centímetros). forearm: circunferencia del antebrazo (en centímetros). wrist: circunferencia de la muñeca (en centímetros). El modelo propuesto es el siguiente: \\[\\begin{equation} \\mbox{brozek}_i = \\beta_{0} + \\mbox{age}_i\\beta_1+ \\mbox{weight}_i\\beta_2 + \\ldots + \\mbox{wrist}_i\\beta_{13} + \\varepsilon_i. \\tag{4.1} \\end{equation}\\] El ajuste del modelo es: library(faraway) data(fat) mod.fat &lt;- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat) summary(mod.fat) ## ## Call: ## lm(formula = brozek ~ age + weight + height + neck + chest + ## abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, ## data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.264 -2.572 -0.097 2.898 9.327 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -15.29255 16.06992 -0.952 0.34225 ## age 0.05679 0.02996 1.895 0.05929 . ## weight -0.08031 0.04958 -1.620 0.10660 ## height -0.06460 0.08893 -0.726 0.46830 ## neck -0.43754 0.21533 -2.032 0.04327 * ## chest -0.02360 0.09184 -0.257 0.79740 ## abdom 0.88543 0.08008 11.057 &lt; 2e-16 *** ## hip -0.19842 0.13516 -1.468 0.14341 ## thigh 0.23190 0.13372 1.734 0.08418 . ## knee -0.01168 0.22414 -0.052 0.95850 ## ankle 0.16354 0.20514 0.797 0.42614 ## biceps 0.15280 0.15851 0.964 0.33605 ## forearm 0.43049 0.18445 2.334 0.02044 * ## wrist -1.47654 0.49552 -2.980 0.00318 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.988 on 238 degrees of freedom ## Multiple R-squared: 0.749, Adjusted R-squared: 0.7353 ## F-statistic: 54.63 on 13 and 238 DF, p-value: &lt; 2.2e-16 4.2 Problema de selección de variables En problemas de regresión se tiene un conjunto grande de potenciales covariables. Si ajustamos un modelo considerandolas todas podemos estar incluyendo covariables que son irrelevante. Por el otro lado, si no las incluimos todas es posible que estemos omitiendo covariables importantes. En ambos casos hay consecuencias negativas. Para illustrar esto, considere el siguiente modelo: \\[\\begin{equation} \\begin{split} y_{i} &amp;= \\beta_{0} + \\sum_{j=1}^{p-1}\\beta_{j}x_{ij} + \\varepsilon_{i} \\\\ &amp;= \\beta_{0} + \\sum_{j=1}^{r}\\beta_{j}x_{ij} + \\sum_{j=r+1}^{p-1}\\beta_{j}x_{ij} + \\varepsilon_{i} \\\\ &amp;= \\boldsymbol x_{1i}&#39;\\boldsymbol \\beta_1 + \\boldsymbol x_{2i}&#39;\\boldsymbol \\beta_2 + \\varepsilon_i, \\end{split} \\tag{4.2} \\end{equation}\\] donde \\(\\boldsymbol x_{1i} = (1,x_{1i},x_{2i},\\ldots,x_{ri})\\), \\(\\boldsymbol x_{2i} = (x_{r+1,i},x_{r+2,i},\\ldots,x_{p-1,i})\\), \\(\\boldsymbol \\beta_1 = (\\beta_0,\\beta_1,\\beta_2,\\ldots,\\beta_r)&#39;\\), \\(\\boldsymbol \\beta_2 = (\\beta_{r+1},\\beta_{r+2},\\ldots,\\beta_{p-1})&#39;\\), y \\(\\varepsilon_i \\sim N(0,\\sigma^{2})\\). Es decir, se hace una partición de las covariables y los coeficientes de regresión en dos componentes. En forma matricial, el modelo es: \\[ \\boldsymbol y= \\boldsymbol X_{1}\\boldsymbol \\beta_{1} + \\boldsymbol X_{2}\\boldsymbol \\beta_{2}+ \\boldsymbol \\varepsilon, \\] donde \\(\\boldsymbol X_{1}\\) es una matriz \\(n \\times r\\) con la \\(i\\)-ésima fila igual a \\(\\boldsymbol x_{1i}\\) y \\(\\boldsymbol X_{2}\\) es una matriz \\(n \\times (p-r-1)\\) con la \\(i\\)-ésima fila igual a \\(\\boldsymbol x_{2i}\\). 4.2.1 ¿Qué pasa si ignoramos covariables importantes? Ahora, considere que el modelo generador de datos es (4.2), pero decidimos estimar: \\[ y_{i} = \\boldsymbol x_{1i}&#39;\\boldsymbol \\beta_1 + \\varepsilon_i. \\] Es decir, estamos omitiendo \\(\\boldsymbol x_{2i}\\) del modelo (puesto que \\(\\boldsymbol \\beta_2 \\neq 0\\), estas covariables son relevantes). El estimador por MCO de \\(\\boldsymbol \\beta_1\\) es: \\[ \\widehat{\\boldsymbol \\beta}_{1} = (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol y. \\] De aquí tenemos que \\(E(\\widehat{\\boldsymbol \\beta}_{1}) = \\boldsymbol \\beta_{1} + (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\boldsymbol \\beta_{2}\\). Es decir que \\(\\widehat{\\boldsymbol \\beta}_{1}\\) es un estimador sesgado, a menos que \\(\\boldsymbol X_{1}&#39;\\boldsymbol X_{2} = \\boldsymbol 0\\) (las columnas de \\(X_{1}\\) son ortogonales a las columnas de \\(X_{2}\\)). De igual forma, las predicciones también serán sesgadas. La predicción en el punto \\(\\boldsymbol x_{01}\\) es: \\[ \\widehat{y}_{0} = \\boldsymbol x_{01}&#39;\\widehat{\\boldsymbol \\beta}_{1}. \\] Su valor esperado es: \\[ E(\\widehat{y}_{0}) = \\boldsymbol x_{01}&#39;\\boldsymbol \\beta_{1} + \\boldsymbol x_{01}&#39;(\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\boldsymbol \\beta_{2} \\neq \\boldsymbol x_{01}&#39;\\beta_{1} + \\boldsymbol x_{02}&#39;\\beta_{2}. \\] Por lo tanto, si omitimos variables relevantes obtenemos sesgo en las estimaciones. 4.2.2 ¿Que pasa si incluimos covariables irrelevantes? Ahora, consideremos el caso en que \\(\\boldsymbol \\beta_2=0\\), es decir, las covariables \\(\\boldsymbol x_{2}\\) no tienen un aporte significativo en el modelo. Pero decidimos estimar el modelo completo. En este caso, el estimador de \\(\\boldsymbol \\beta\\) es: \\[ \\widehat{\\boldsymbol \\beta}= (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol y= \\begin{pmatrix} \\boldsymbol X_{1}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{1}\\boldsymbol X_{2} \\\\ \\boldsymbol X_{2}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{2}&#39;\\boldsymbol X_{2} \\end{pmatrix}^{-1} \\begin{pmatrix} \\boldsymbol X_{1}&#39; \\\\ \\boldsymbol X_{2}&#39; \\end{pmatrix}\\boldsymbol y. \\] El Valor esperado de \\(\\widehat{\\boldsymbol \\beta}\\) es: \\[\\begin{equation} \\begin{split} E(\\widehat{\\boldsymbol \\beta}) =&amp; (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;E(\\boldsymbol y) = (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol X_{1}\\boldsymbol \\beta_1 \\\\ = &amp; (\\boldsymbol X&#39;\\boldsymbol X)^{-1}\\boldsymbol X&#39;(\\boldsymbol X_{1} \\ \\boldsymbol X_{2}) \\begin{pmatrix} \\boldsymbol \\beta_1 \\\\ \\boldsymbol 0 \\end{pmatrix} = \\begin{pmatrix} \\boldsymbol \\beta_1 \\\\ \\boldsymbol 0 \\end{pmatrix}. \\end{split} \\nonumber \\end{equation}\\] Es decir que \\(\\widehat{\\boldsymbol \\beta}\\) es un estimador insesgado. La varianza de \\(\\widehat{\\boldsymbol \\beta}\\) es: \\[\\begin{equation} \\begin{split} V(\\widehat{\\boldsymbol \\beta}) =&amp; \\sigma^{2}(\\boldsymbol X&#39;\\boldsymbol X)^{-1} = \\sigma^{2}\\begin{pmatrix} \\boldsymbol X_{1}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{1}\\boldsymbol X_{2} \\\\ \\boldsymbol X_{2}&#39;\\boldsymbol X_{1} &amp; \\boldsymbol X_{2}&#39;\\boldsymbol X_{2} \\end{pmatrix}^{-1} \\\\ =&amp; \\sigma^{2} \\begin{pmatrix} (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1} + \\boldsymbol L\\boldsymbol M\\boldsymbol L&amp; - \\boldsymbol L\\boldsymbol M\\\\ -\\boldsymbol M\\boldsymbol L&#39; &amp; \\boldsymbol M \\end{pmatrix}, \\end{split} \\nonumber \\end{equation}\\] donde \\(\\boldsymbol L= (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1}\\boldsymbol X_{1}&#39;\\boldsymbol X_{2}\\) y \\(\\boldsymbol M= \\boldsymbol X_{2}&#39;(\\boldsymbol I- \\boldsymbol H_{1})\\boldsymbol X_{2}\\). Particularmente, para \\(\\widehat{\\boldsymbol \\beta}_1\\) tenemos que: \\[ V(\\widehat{\\boldsymbol \\beta}_{1}) = \\sigma^{2} \\left[ (\\boldsymbol X_{1}&#39;\\boldsymbol X_{1})^{-1} + \\boldsymbol L\\boldsymbol M\\boldsymbol L\\right]. \\] Dado que \\(\\boldsymbol M\\) (y por lo tanto \\(\\boldsymbol L\\boldsymbol M\\boldsymbol L\\)) es positiva-definida, la varianza de \\(\\widehat{\\boldsymbol \\beta}_{1}\\) se infla al incluir las covariables irrelevantes al modelo. La única excepción es cuando las columnas de \\(\\boldsymbol X_{1}\\) y \\(\\boldsymbol X_{2}\\) son ortogonales. De igual forma, las predicciones en el punto \\(\\boldsymbol x_{0}&#39; = (\\boldsymbol x_{01}&#39; \\ \\boldsymbol x_{02}&#39;)\\) son insesgadas: \\[ E(\\widehat{y}_{0}) = E(\\boldsymbol x_{0}&#39;\\widehat{\\boldsymbol \\beta}) = (\\boldsymbol x_{01}&#39; \\ \\boldsymbol x_{02}&#39;)\\begin{pmatrix} \\boldsymbol \\beta_{1} \\\\ \\boldsymbol 0 \\end{pmatrix} = \\boldsymbol x_{01}&#39;\\boldsymbol \\beta_{1}. \\] En conclusión: Cuando omitimos covariables relevantes, obtenemos sesgos en las estimaciones. Cuando incluimos covariables irrelevantes, se inflan las varianzas de las estimaciones. 4.3 Métodos para la selección de variables Si tenemos \\((p-1)\\) covariables, entonces hay \\((p-1)^2\\) modelos posibles. Por lo que podemos ajustar todos los posibles modelos y hacer una comparación entre ellos usando algunos criterios de decisión. Existen varios criterios para determinar que modelo es “mejor” que otro y este debe escogerse teniendo en cuenta cuál es el objetivo que se tiene al ajustar el modelo (descripción de la relación, predicción, control, etc.). Algunos de estos criterios son el coeficiente de determinación (\\(R^{2}\\) y \\(R^{2}_{adj}\\)), el estadístico \\(C_{p}\\) de Mallows, el estadístico PRESS y el \\(R^{2}\\) de predicción, los criterios de información (AIC y BIC). 4.3.1 Coeficiente de determinación Esté indicador está definido como: \\[ R^{2} = \\frac{SS_{\\mbox{reg}}}{SS_{\\mbox{T}}} = 1 - \\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}}. \\] El \\(R^{2}\\) cuantifica la cantidad de variabilidad de la variable respuesta que es explicada por el modelo. Se tiene que \\(0 \\leq R^{2} \\leq 1\\). Valores más cercanos a \\(1\\) implican que el modelo explica gran parte de la variabilidad de \\(y\\). Hay que tener en cuenta que el \\(R^{2}\\) siempre crece a medida que se adicionan más covariables al modelo. Por lo tanto, se puede puede agregar regresores hasta el punto en que una covariable adicional no propociona un aumento considerable en el \\(R^{2}\\). 4.3.2 Coeficiente de determinación ajustado Para evitar el incoviente del \\(R^{2}\\), se puede utlizar el el coeficiente de determinación ajustado definido como: \\[ R^{2}_{adj} = 1 - \\frac{n-1}{n-p}\\frac{SS_{\\mbox{res}}}{SS_{\\mbox{T}}} = 1- \\frac{MS_{\\mbox{res}}}{SS_{\\mbox{T}}/(n-1)} = 1- \\frac{n-1}{n-p}(1-R^{2}). \\] El \\(R^{2}_{adj}\\) no necesariamente aumenta al adicionar nuevos términos al modelo. Este solo aumenta si hay una disminución del \\(MS_{\\mbox{res}}\\). 4.3.3 C\\(_p\\) de Mallows Mallows propone un criterio basado en el error cuadrático medio (ECM) de \\(\\widehat{y}_i\\), esto es: \\[ E[\\widehat{y}_{i}- E(y_{i})]^2 = [E(y_{i}) - E(\\widehat{y}_{i})]^2 + V(\\widehat{y}_{i}), \\] donde \\(E(y_{i})\\) es el valor esperado de la respuesta (‘modelo real’), y \\(E(\\widehat{y}_{i})\\) es el valor esperado de la respuesta basado en el modelo propuesto (basado en \\(p\\) covariables). El ECM total estandarizado está definido como: \\[\\begin{equation} \\begin{split} \\Gamma_{p} =&amp; \\frac{1}{\\sigma^{2}}\\left\\{\\sum_{i=1}^{n}[E(y_{i}) - E(\\widehat{y}_{i})]^2 + \\sum_{i=1}^{n} V(\\widehat{y}_{i}) \\right\\} \\\\ =&amp; \\frac{1}{\\sigma^{2}}\\left\\{SS_{B}(p) + \\sum_{i=1}^{n} V(\\widehat{y}_{i}) \\right\\} = \\frac{1}{\\sigma^{2}}\\left\\{SS_{B}(p) + p\\sigma^{2} \\right\\} \\\\ =&amp; \\frac{1}{\\sigma^{2}}\\left\\{ E[SS_{\\mbox{res}}(p)] - (n-p)\\sigma^{2} + p\\sigma^{2} \\right\\} \\\\ =&amp; \\frac{E[SS_{\\mbox{res}}(p)]}{\\sigma^{2}} - n + 2p. \\end{split} \\nonumber \\end{equation}\\] Reemplazando \\(E[SS_{\\mbox{res}}(p)]\\) por \\(SS_{\\mbox{res}}(p)\\), y asumiendo que \\(MS_{\\mbox{res}}(p^{*})\\) (calculado usando el modelo completo) es un buen estimador de \\(\\sigma^{2}\\): \\[ C_{p} = \\frac{SS_{\\mbox{res}}(p)}{MS_{\\mbox{res}}(p^{*})} - n + 2p. \\] Por lo tanto, para el modelo completo \\(C_{p} = p^{*}\\). Si \\(E[SS_{\\mbox{res}}(p)] = (n-p)\\sigma^{2}\\) (asumiendo que \\(SS_{B}(p)=0\\)), tenemos que: \\[ E[C_{p}| \\mbox{Sesgo}=0] = \\frac{(n-p)\\sigma^{2}}{\\sigma^{2}} - n +2p = p. \\] Si el modelo propuesto es insesgado se espera que el \\(C_p\\) esté cercano a \\(p\\). Aunque se espera que el \\(C_p=p\\), es deseable que \\(C_p &lt; p\\). Por lo tanto, modelos con valores pequeños de \\(C_p\\) son mejores. 4.3.4 Estadístico PRESS El estadístico PRESS (prediction error sum of squares) está definido como: \\[ \\mbox{PRESS} = \\sum_{i=1}^{n} (y_{i} - \\widehat{y}_{(i)})^{2} = \\sum_{i=1}^{n} \\left( \\frac{e_{i}}{1-h_{ii}} \\right)^{2}. \\] Para comparar modelos, menor valor del PRESS indica que el modelo es mejor para hacer predicciones. A partir del PRESS se puede calcular el \\(R^{2}\\) de predicción: \\[ R^{2}_{pred} = 1 - \\frac{PRESS}{SST}. \\] Basado en este criterio, mayor es el valor de \\(R^{2}_{pred}\\) mejor es el modelo para hacer predicciones. La ventaja del PRESS y \\(R_{pred}^{2}\\) es que evitan el sobreajuste dado que se calculan utilizando observaciones no incluidas en la estimación del modelo. 4.3.5 Criterios de información La idea es comparar modelos estimados teniendo en cuenta la bondad de ajuste del modelo (verosimilitud, \\(L\\)) y su complejidad (número de parámetros). El criterio de información de Akaike está definido como: \\[ \\mbox{AIC} = -2\\log (L) + 2p. \\] El criterio de información bayesiano (o de Schwarz - SBC): \\[ \\mbox{BIC} = -2\\log (L) + p\\log n. \\] Es preferible modelos con valores menores de AIC o BIC. Dado que la penalización del BIC es mayor (si \\(n &gt; 7\\)), este indicador prefiere modelos con menor número de covariables. Recordemos que la log-verosimilitud es: \\[ \\log L(\\boldsymbol \\beta,\\sigma^{2}) = - \\frac{n}{2}\\log (2\\pi) - n\\log(\\sigma) - \\frac{1}{2\\sigma^{2}}(\\boldsymbol y- \\boldsymbol X\\boldsymbol \\beta)&#39;(\\boldsymbol y-\\boldsymbol X\\boldsymbol \\beta). \\] El estimador por máxima verosimilitud de \\(\\sigma^{2}\\) es \\(\\widehat{\\sigma}=SS_{\\mbox{res}}/n\\). Por lo tanto, el máximo valor de la log-verosimilitud es: \\[ \\log L(\\widehat{\\boldsymbol \\beta},\\widehat{\\sigma}^{2}) = -\\frac{n}{2}\\log (2\\pi) - \\frac{n}{2}\\log\\widehat{\\sigma}^{2} - \\frac{1}{2\\widehat{\\sigma}^{2}}SS_{\\mbox{res}}= -\\frac{n}{2}\\log (SS_{\\mbox{res}}/n) + \\mbox{constante}. \\] Por lo tanto: \\[ AIC \\propto n\\log(SS_{\\mbox{res}}/n) + 2p \\mbox{ y } BIC \\propto n\\log(SS_{\\mbox{res}}/n) + p\\log n. \\] Hay varias adaptaciones de estos criterios de información definiendo diferentes penalizaciones. 4.4 Comparación de los modelos 4.4.1 Todos los posibles modelos Con la función ols_step_all_possible() de la librería olsrr es posible ajustar todos posibles modelos y determinar el mejor bajo diferentes criterios. Otra alternativa es la función regsubsets() de la librería leaps. Está función es más rápida (se basa en un algoritmo más eficiente), pero no es user-friendly. 4.4.1.1 Datos de unidad quirúrgica A través de la función ols_step_all_possible podemos ajustar los \\(255\\) modelos que se pueden ajustar usando las ocho posibles covariables: surgical.all.mods=ols_step_all_possible(mod.surgical.completo) Además de ajustar los modelos, se calculan varios criterios (\\(R^{2}\\),\\(R^{2}_{adj}\\), \\(R^{2}_{pred}\\),AIC,BIC,…) para cada uno de ellos. Puesto que son muchos modelos, podemos organizar los resultados de tal forma que obtengamos los mejores modelos basándonos en cada uno de los criterios. Por ejemplo, los 5 mejores ajustes según el \\(R^{2}_{adj}\\) son: R2adj.order = order(surgical.all.mods$result$adjr,decreasing = T) as.data.frame(surgical.all.mods$result)[R2adj.order[1:5],c(2:8,10)] ## n predictors rsquare adjr rmse predrsq ## 226 6 bcs pindex enzyme_test age gender alc_heavy 0.8434664 0.8234834 0.1926586 0.7836037 ## 251 7 bcs pindex enzyme_test age gender alc_mod alc_heavy 0.8460095 0.8225761 0.1910872 0.7806940 ## 171 5 bcs pindex enzyme_test gender alc_heavy 0.8374622 0.8205312 0.1963187 0.7827597 ## 248 7 bcs pindex enzyme_test liver_test age gender alc_heavy 0.8436412 0.8198474 0.1925510 0.7749807 ## 169 5 bcs pindex enzyme_test age alc_heavy 0.8358522 0.8187535 0.1972887 0.7862369 ## cp sbic ## 226 5.772458 -159.4064 ## 251 7.028837 -157.6344 ## 171 5.528174 -160.2288 ## 248 7.721367 -157.0871 ## 169 5.998959 -159.8244 Basándonos en este criterio el mejor ajuste se obtiene considerando las covariables bcs, pindex, enzyme_test, age, gender, y alc_heavy. Es decir, eliminando las covariables función hepática y consumo de alcohol moderado. Note que no todos los demás críterios sugieren el mismo modelo. Si eliminamos las covariables gender obtenemos un modelo con un \\(R^{2}_{pred}\\) más alto. Ahora, si nos apoyamos en el AIC, los mejores 5 ajustes son: AIC.order = order(surgical.all.mods$result$aic,decreasing = F) as.data.frame(surgical.all.mods$result)[AIC.order[1:5],c(2:8,10)] ## n predictors rsquare adjr rmse predrsq ## 226 6 bcs pindex enzyme_test age gender alc_heavy 0.8434664 0.8234834 0.1926586 0.7836037 ## 171 5 bcs pindex enzyme_test gender alc_heavy 0.8374622 0.8205312 0.1963187 0.7827597 ## 97 4 bcs pindex enzyme_test alc_heavy 0.8299187 0.8160345 0.2008227 0.7862922 ## 169 5 bcs pindex enzyme_test age alc_heavy 0.8358522 0.8187535 0.1972887 0.7862369 ## 251 7 bcs pindex enzyme_test age gender alc_mod alc_heavy 0.8460095 0.8225761 0.1910872 0.7806940 ## cp sbic ## 226 5.772458 -159.4064 ## 171 5.528174 -160.2288 ## 97 5.733992 -160.5329 ## 169 5.998959 -159.8244 ## 251 7.028837 -157.6344 Con este críterio se escoge el mismo modelo que con el \\(R^{2}_{adj}\\). Sin embargo, podemos observar que el BIC sugiere eliminar la covariable asociada a la edad. En la Figura 4.2 muestra los \\(R^{2}\\),\\(R^{2}_{adj}\\), \\(R^{2}_{pred}\\),C\\(_p\\), AIC y BIC (SBC) para todos los posibles ajustes. Note que, dentro de cada subgrupo de modelos (determinado por el número de covariables), los criterios eligen los modelos en el mismo orden. La diferencia está en el número de covariables a elegir. Generalmente, el BIC prefiere modelos más parsimoniosos. Esto no ocurre con criterios de validación cruzada, como el PRESS o \\(R^{2}_{pred}\\). par(mfrow=c(1,2)) plot(surgical.all.mods) Figure 4.1: Valores de los criterios de selección calculados para cada uno de todos los posibles modelos. Figure 4.2: Valores de los criterios de selección calculados para cada uno de todos los posibles modelos. Teniendo en cuenta esto, con la función ols_step_best_subset() selecciona el mejor modelo para cada subconjunto de número de covariables basándose en los diferentes criterios: ols_step_best_subset(mod.surgical.completo) ## Best Subsets Regression ## ----------------------------------------------------------------------------- ## Model Index Predictors ## ----------------------------------------------------------------------------- ## 1 enzyme_test ## 2 pindex enzyme_test ## 3 pindex enzyme_test alc_heavy ## 4 bcs pindex enzyme_test alc_heavy ## 5 bcs pindex enzyme_test gender alc_heavy ## 6 bcs pindex enzyme_test age gender alc_heavy ## 7 bcs pindex enzyme_test age gender alc_mod alc_heavy ## 8 bcs pindex enzyme_test liver_test age gender alc_mod alc_heavy ## ----------------------------------------------------------------------------- ## ## Subsets Regression Summary ## -------------------------------------------------------------------------------------------------------------------------------- ## Adj. Pred ## Model R-Square R-Square R-Square C(p) AIC SBIC SBC MSEP FPE HSP APC ## -------------------------------------------------------------------------------------------------------------------------------- ## 1 0.4273 0.4162 0.3496 117.4783 51.4343 -105.4395 57.4013 7.6160 0.1463 0.0028 0.6168 ## 2 0.6632 0.6500 0.6044 50.4918 24.7668 -131.5971 32.7228 4.5684 0.0893 0.0017 0.3765 ## 3 0.7780 0.7647 0.7291 18.9015 4.2432 -150.4023 14.1881 3.0718 0.0610 0.0012 0.2575 ## 4 0.8299 0.8160 0.7863 5.7340 -8.1306 -160.5329 3.8033 2.4030 0.0486 9e-04 0.2048 ## 5 0.8375 0.8205 0.7828 5.5282 -8.5803 -160.2288 5.3426 2.3453 0.0482 9e-04 0.2032 ## 6 0.8435 0.8235 0.7836 5.7725 -8.6129 -159.4064 7.2990 2.3077 0.0482 9e-04 0.2032 ## 7 0.8460 0.8226 0.7807 7.0288 -7.4974 -157.6344 10.4035 2.3207 0.0492 0.0010 0.2076 ## 8 0.8461 0.8187 0.7711 9.0000 -5.5320 -155.2573 14.3579 2.3719 0.0511 0.0010 0.2154 ## -------------------------------------------------------------------------------------------------------------------------------- ## AIC: Akaike Information Criteria ## SBIC: Sawa&#39;s Bayesian Information Criteria ## SBC: Schwarz Bayesian Criteria ## MSEP: Estimated error of prediction, assuming multivariate normality ## FPE: Final Prediction Error ## HSP: Hocking&#39;s Sp ## APC: Amemiya Prediction Criteria A partir de estos resultados, y con la ayuda de expertos en el tema, se puede hacer una selección del mejor modelo para hacer las predicciónes. 4.4.2 Algorítmos de selección Para el proceso de selección, la mejor opción es evaluar todos los posibles modelos. Sin embargo, en la presencia de muchas posibles covariables este proceso puede requerir una carga computacional muy alta. Por esta razón, se han desarrollado varios algoritmos para evaluar solo un subconjunto de modelos agregando o eliminando covariables una a la vez. 4.4.2.1 Selección hacia delante (forward selection) Este algoritmo parte del modelo sin ninguna covariable (es decir, solo el intercepto) y el ajuste ``óptimo’’ se encuentra ingresando covariables una a la vez basándose en algún criterio (por ejemplo AIC). La primera covariable se escoge luego de ajustar los \\((p-1)\\) modelos simples con cada uno de los regresores. Por ejemplo, seleccionado la covariable que proporciona el mejor AIC. Luego se ajustan los modelos combinando la covariable previamente seleccionada con cada una de los restantes \\((p-2)\\) regresores. Si el mejor ajuste con dos covariables proporcina un menor AIC que en el paso anterior, continuamos seleccionando la tercer covariable de la misma forma. El algoritmo continua seleccionando covariables hasta que se satisface un criterio de parada (por ejemplo, hasta que el AIC aumente). 4.4.2.2 Selección hacia atrás (backward selection) Aquí se empieza evaluando el modelo con todas las covariables candidatas y se van eliminando covariables una a una hasta que un criterio de parada se satisface (por ejemplo, hasta que el AIC aumenta). 4.4.2.3 Selección por segmentos (stepwise selection) Aquí se siguen los mismos pasos que la selección hacia delante. Pero en cada paso se evalúan de nuevo los candidatos que ya habían ingresado en el modelo. Por lo tanto, una covariable que ya esté en el modelo puede ser eliminada en algún paso posterior. 4.4.2.4 Unidad quirúrgica Consideremos el modelo anterior adicionando las interacciones de las covariables continuas con las categóricas: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0}+\\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2}+ \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{liver}_{i}\\beta_{4} + \\mbox{age}_{i}\\beta_{5} + \\mbox{gender}_{i}\\beta_{6}+ \\mbox{alc_mod}_{i}\\beta_{7} + \\\\ &amp; \\mbox{alc_heavy}_{i}\\beta_{8} \\mbox{bcs}_{i}\\mbox{gender}_{i}\\beta_{9} + \\mbox{pindex}_{i}\\mbox{gender}_{i}\\beta_{10} + \\mbox{enzyme}_{i}\\mbox{gender}_{i}\\beta_{11} + \\mbox{liver}_{i}\\mbox{gender}_{i}\\beta_{12} + \\\\ &amp; \\mbox{age}_{i}\\mbox{gender}_{i}\\beta_{13} + \\mbox{bcs}_{i}\\mbox{alc_mod}_{i}\\beta_{14} + \\mbox{pindex}_{i}\\mbox{alc_mod}_{i}\\beta_{15} + \\mbox{enzyme}_{i}\\mbox{alc_mod}_{i}\\beta_{16} + \\\\ &amp; \\mbox{liver}_{i}\\mbox{alc_mod}_{i}\\beta_{17} + \\mbox{age}_{i}\\mbox{alc_mod}_{i}\\beta_{18} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{19} + \\mbox{pindex}_{i}\\mbox{alc_heavy}_{i}\\beta_{20} + \\\\ &amp; \\mbox{enzyme}_{i}\\mbox{alc_heavy}_{i}\\beta_{21} + \\mbox{liver}_{i}\\mbox{alc_heavy}_{i}\\beta_{22} + \\mbox{age}_{i}\\mbox{alc_heavy}_{i}\\beta_{23} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] En este caso tenemos \\(2^{23}=8&#39;388,608\\) posibles modelos. Lo que hace que sea difícil ajustarlos todos (aunque es posible usando la librería leaps). Por lo tanto, vamos a utilizar los algortimos de selección. Selección hacia delante. Podemos utilizar la función ols_step_forward_aic de la librería olsrr: mod.surgical.completo2 = lm(log.y~bcs*gender+pindex*gender+enzyme_test*gender+liver_test*gender+age*gender+ bcs*alc_mod+pindex*alc_mod+enzyme_test*alc_mod+liver_test*alc_mod+age*alc_mod+bcs*alc_heavy+pindex*alc_heavy+enzyme_test*alc_heavy+liver_test*alc_heavy+age*alc_heavy,data=surgical) ols_step_forward_aic(mod.surgical.completo2,details = F) ## ## ## Stepwise Summary ## --------------------------------------------------------------------------------- ## Step Variable AIC SBC SBIC R2 Adj. R2 ## --------------------------------------------------------------------------------- ## 0 Base Model 79.529 83.507 -77.331 0.00000 0.00000 ## 1 enzyme_test 51.434 57.401 -106.921 0.42725 0.41624 ## 2 pindex 24.767 32.723 -134.607 0.66318 0.64997 ## 3 bcs:alc_heavy -3.014 6.931 -162.432 0.80596 0.79432 ## 4 bcs -9.430 2.504 -169.458 0.83396 0.82041 ## 5 gender:pindex -10.781 3.142 -171.655 0.84395 0.82770 ## 6 gender:enzyme_test -19.676 -3.764 -180.194 0.87246 0.85618 ## 7 gender -23.040 -5.139 -183.572 0.88452 0.86695 ## 8 age -24.081 -4.192 -184.830 0.89085 0.87144 ## --------------------------------------------------------------------------------- ## ## Final Model Output ## ------------------ ## ## Model Summary ## --------------------------------------------------------------- ## R 0.944 RMSE 0.161 ## R-Squared 0.891 MSE 0.031 ## Adj. R-Squared 0.871 Coef. Var 2.741 ## Pred R-Squared 0.844 AIC -24.081 ## MAE 0.131 SBC -4.192 ## --------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## AIC: Akaike Information Criteria ## SBC: Schwarz Bayesian Criteria ## ## ANOVA ## ------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ------------------------------------------------------------------- ## Regression 11.407 8 1.426 45.909 0.0000 ## Residual 1.398 45 0.031 ## Total 12.805 53 ## ------------------------------------------------------------------- ## ## Parameter Estimates ## ----------------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## ----------------------------------------------------------------------------------------------- ## (Intercept) 3.949 0.238 16.577 0.000 3.469 4.429 ## enzyme_test 0.019 0.002 0.830 12.162 0.000 0.016 0.022 ## pindex 0.011 0.002 0.383 4.903 0.000 0.007 0.016 ## bcs 0.067 0.017 0.218 3.892 0.000 0.032 0.101 ## gender 0.655 0.277 0.671 2.362 0.023 0.097 1.214 ## age -0.004 0.002 -0.081 -1.615 0.113 -0.008 0.001 ## bcs:alc_heavy 0.052 0.010 0.288 5.234 0.000 0.032 0.073 ## pindex:gender 0.003 0.003 0.198 0.914 0.366 -0.003 0.009 ## enzyme_test:gender -0.010 0.002 -0.828 -4.002 0.000 -0.014 -0.005 ## ----------------------------------------------------------------------------------------------- Con el argumento details = T se puede ver la selección con más detalle. Usando este algoritmo el modelo óptimo es: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{age}_{i}\\beta_{4} + \\mbox{gender}_{i}\\beta_{5} + \\\\ &amp;\\mbox{gender}_{i}\\mbox{pindex}_{i}\\beta_{6} + \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{7} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{8} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Selección hacia atrás. Podemos utilizar la función ols_step_backward_aic de la librería olsrr: ols_step_backward_aic(mod.surgical.completo2,details = F) ## ## ## Stepwise Summary ## ------------------------------------------------------------------------------------- ## Step Variable AIC SBC SBIC R2 Adj. R2 ## ------------------------------------------------------------------------------------- ## 0 Full Model -9.135 40.589 -172.353 0.91740 0.85408 ## 1 age:alc_heavy -11.134 36.601 -173.941 0.91740 0.85878 ## 2 alc_heavy -13.133 32.614 -45.421 0.91740 0.86319 ## 3 bcs:alc_mod -15.057 28.701 -53.959 0.91728 0.86715 ## 4 age:alc_mod -16.852 24.916 -62.617 0.91697 0.87057 ## 5 gender:pindex -18.639 21.141 -1118.918 0.91664 0.87377 ## 6 alc_mod -19.980 17.811 -69.924 0.91562 0.87577 ## 7 enzyme_test:alc_heavy -20.698 15.104 -79.945 0.91359 0.87622 ## 8 liver_test:alc_heavy -20.988 12.824 -89.749 0.91081 0.87560 ## 9 pindex:alc_heavy -22.223 9.601 -98.678 0.90954 0.87706 ## 10 pindex:alc_mod -22.957 6.878 -107.457 0.90739 0.87729 ## 11 bcs:gender -22.981 4.865 -164.788 0.90394 0.87583 ## 12 gender:age -23.038 2.819 -108.929 0.90042 0.87434 ## 13 gender:liver_test -23.549 0.319 -196.032 0.89764 0.87383 ## 14 age -23.745 -1.866 -183.861 0.89416 0.87251 ## ------------------------------------------------------------------------------------- ## ## Final Model Output ## ------------------ ## ## Model Summary ## --------------------------------------------------------------- ## R 0.946 RMSE 0.158 ## R-Squared 0.894 MSE 0.031 ## Adj. R-Squared 0.873 Coef. Var 2.729 ## Pred R-Squared 0.841 AIC -23.745 ## MAE 0.127 SBC -1.866 ## --------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## AIC: Akaike Information Criteria ## SBC: Schwarz Bayesian Criteria ## ## ANOVA ## ------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ------------------------------------------------------------------- ## Regression 11.449 9 1.272 41.302 0.0000 ## Residual 1.355 44 0.031 ## Total 12.805 53 ## ------------------------------------------------------------------- ## ## Parameter Estimates ## ------------------------------------------------------------------------------------------------ ## model Beta Std. Error Std. Beta t Sig lower upper ## ------------------------------------------------------------------------------------------------ ## (Intercept) 3.568 0.220 16.227 0.000 3.124 4.011 ## bcs 0.070 0.022 0.230 3.161 0.003 0.026 0.115 ## gender 0.829 0.196 0.849 4.234 0.000 0.435 1.224 ## pindex 0.014 0.002 0.475 7.952 0.000 0.010 0.017 ## enzyme_test 0.022 0.002 0.947 9.262 0.000 0.017 0.027 ## liver_test -0.082 0.059 -0.178 -1.396 0.170 -0.200 0.036 ## gender:enzyme_test -0.009 0.002 -0.823 -3.894 0.000 -0.014 -0.005 ## enzyme_test:alc_mod -0.004 0.002 -0.348 -2.156 0.037 -0.008 0.000 ## liver_test:alc_mod 0.123 0.055 0.396 2.230 0.031 0.012 0.235 ## bcs:alc_heavy 0.062 0.012 0.338 5.201 0.000 0.038 0.085 ## ------------------------------------------------------------------------------------------------ Por lo tanto, el modelo seleccionado es: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{gender}_{i}\\beta_{4} + \\mbox{liver}_{i}\\beta_{5} + \\\\ &amp; \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{6} + \\mbox{enzyme}_{i}\\mbox{alc_mod}_{i}\\beta_{7} + \\mbox{liver}_{i}\\mbox{alc_mod}_{i}\\beta_{8} + \\mbox{liver}_{i}\\mbox{gender}_{i}\\beta_{9} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Con este algoritmo no se considera la edad del paciente pero si la función hepática y otras interacciones. Selección por segmentos. Aquí tenemos la función ols_step_both_aic de la librería olsrr: ols_step_both_aic(mod.surgical.completo2,details = F) ## ## ## Stepwise Summary ## ------------------------------------------------------------------------------------- ## Step Variable AIC SBC SBIC R2 Adj. R2 ## ------------------------------------------------------------------------------------- ## 0 Base Model 79.529 83.507 -77.331 0.00000 0.00000 ## 1 enzyme_test (+) 51.434 57.401 -106.921 0.42725 0.41624 ## 2 pindex (+) 24.767 32.723 -134.607 0.66318 0.64997 ## 3 bcs:alc_heavy (+) -3.014 6.931 -162.432 0.80596 0.79432 ## 4 bcs (+) -9.430 2.504 -169.458 0.83396 0.82041 ## 5 gender:pindex (+) -10.781 3.142 -171.655 0.84395 0.82770 ## 6 gender:enzyme_test (+) -19.676 -3.764 -180.194 0.87246 0.85618 ## 7 gender (+) -23.040 -5.139 -183.572 0.88452 0.86695 ## 8 gender:pindex (-) -23.631 -7.719 -183.529 0.88147 0.86634 ## 9 age (+) -25.088 -7.187 -185.232 0.88882 0.87190 ## ------------------------------------------------------------------------------------- ## ## Final Model Output ## ------------------ ## ## Model Summary ## --------------------------------------------------------------- ## R 0.943 RMSE 0.162 ## R-Squared 0.889 MSE 0.031 ## Adj. R-Squared 0.872 Coef. Var 2.736 ## Pred R-Squared 0.846 AIC -25.088 ## MAE 0.131 SBC -7.187 ## --------------------------------------------------------------- ## RMSE: Root Mean Square Error ## MSE: Mean Square Error ## MAE: Mean Absolute Error ## AIC: Akaike Information Criteria ## SBC: Schwarz Bayesian Criteria ## ## ANOVA ## ------------------------------------------------------------------- ## Sum of ## Squares DF Mean Square F Sig. ## ------------------------------------------------------------------- ## Regression 11.381 7 1.626 52.535 0.0000 ## Residual 1.424 46 0.031 ## Total 12.805 53 ## ------------------------------------------------------------------- ## ## Parameter Estimates ## ----------------------------------------------------------------------------------------------- ## model Beta Std. Error Std. Beta t Sig lower upper ## ----------------------------------------------------------------------------------------------- ## (Intercept) 3.864 0.219 17.667 0.000 3.423 4.304 ## enzyme_test 0.019 0.002 0.828 12.157 0.000 0.016 0.022 ## pindex 0.013 0.001 0.437 8.697 0.000 0.010 0.016 ## bcs 0.067 0.017 0.220 3.946 0.000 0.033 0.102 ## gender 0.839 0.191 0.859 4.381 0.000 0.453 1.224 ## age -0.004 0.002 -0.086 -1.744 0.088 -0.008 0.001 ## bcs:alc_heavy 0.054 0.010 0.294 5.392 0.000 0.034 0.074 ## enzyme_test:gender -0.010 0.002 -0.838 -4.063 0.000 -0.014 -0.005 ## ----------------------------------------------------------------------------------------------- Note que este método sigue los mismos pasos que la selección hacia delante hasta el paso 8 donde se elimina la interacción entre el índice de pronostico y el genero. Por lo que aquí obtenemos el siguiente modelo: \\[\\begin{equation} \\begin{split} \\log y_{i} =&amp; \\beta_{0} + \\mbox{bcs}_{i}\\beta_{1} + \\mbox{pindex}_{i}\\beta_{2} + \\mbox{enzyme}_{i}\\beta_{3} + \\mbox{age}_{i}\\beta_{4} + \\mbox{gender}_{i}\\beta_{5} + \\\\ &amp; \\mbox{gender}_{i}\\mbox{enzyme}_{i}\\beta_{6} + \\mbox{bcs}_{i}\\mbox{alc_heavy}_{i}\\beta_{7} + \\varepsilon_{i}. \\end{split} \\nonumber \\end{equation}\\] Dado que los algoritmos hacen la busqueda del modelo “óptimo” evaluando diferentes subconjuntos de covariables, se obtuvieron diferentes ajustes. Si observamos el AIC de las tres opciones, el modelo obtenido con el algoritmo stepwise presenta el mejor resultado. 4.5 Regresión de LASSO Otra alternativa para encontrar las variables relevantes es a través del estimador de LASSO (Least Absolute Selection and Shrinkage Operator). Este es un método de regularización que se implementa se tiene cientos de covariables disponibles y se cree que pocas tienen un aporte relevante (sparsity principle). Por ejemplo, en estudios de genética se tienen miles de genes disponibles pero solo unos pocos son activos para una mutación de interés. Aquí se asume el modelo de regresión usual, donde: \\[ E(y | \\boldsymbol x) = \\boldsymbol x&#39;\\boldsymbol \\beta, \\mbox{ y } V(y | \\boldsymbol x) = \\sigma^2, \\] pero muchos de los elementos de \\(\\boldsymbol \\beta\\) son iguales a cero. El objetivo del estimador de LASSO seleccionar los coeficientes que tienen valores diferentes de cero. Esto se obtiene minimizando la siguiente expresión: \\[\\begin{equation} S_{lasso}(\\beta)= \\sum_{i=1}^{n}(y_{i}-x_{i}&#39;\\boldsymbol \\beta)^{2}+ \\lambda\\sum_{j=1}^{p-1}|\\beta_{j}|, \\tag{4.3} \\end{equation}\\] con \\(\\lambda \\geq 0\\). Note que (4.3) es la suma de cuadrados del estimador por MCO más una penalización (dada por \\(\\lambda\\)) a la suma del valor absoluto de los coeficientes (exceptuando el intercepto). Si \\(\\lambda=0\\), se obtiene el estimador por MCO. A medida que \\(\\lambda\\) aumenta la penalización tendrá mas peso sobre la estimación de los coeficientes. Por lo que si este parámetro es suficientemente grande, todas las estimaciones serán iguales a cero (con excepción del intercepto). También se puede probar que, cuando \\(\\lambda \\rightarrow \\infty\\), la varianza de \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\) disminuye, pero el sesgo aumenta. De forma de equivalente, el estimador de LASSO minimiza: \\[ \\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\\boldsymbol \\beta)^{2} \\quad \\mbox{ sujeto a } \\quad \\sum_{j=1}^{p-1}|\\beta_{j} | \\leq t. \\] Mientras que, en la regresión de ridge minimiza: \\[ \\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\\boldsymbol \\beta)^{2} \\quad \\mbox{ sujeto a } \\quad \\sum_{j=1}^{p-1}\\beta_{j}^2\\leq t. \\] La Figura 4.3 muestra como intervienen las restricciones del estimador de LASSO (izquierda) y ridge (derecha) en un modelo con dos covariables. Los contornos muestras la suma de cuadrado de los errores, mientras que, las restricciones están dadas por el diamante \\((|\\beta_1|+|\\beta_2| \\leq t)\\) y la circunferencia \\((\\beta_1^2+\\beta_2^2 \\leq t)\\) rojas para el estimador LASSO y ridge, respectivamente. La solución de ambos estimadores se encuentra en el punto donde el contorno elíptico intercepta la región de la restricción. Para la regresión LASSO, si la solución ocurre en una esquina, entonces la estimación del parámetro es igual a cero. Figure 4.3: Estimación del estimador de LASSO (izquierda) y ridge (derecha). Las elipses rojas son los contornos de la suma de cuadrados de los errores. Mientras que, la región roja son las restricciones para cada estimador). Dado que (4.3) es una función no lineal en \\(y\\), no hay solución analítica para \\(\\widehat{\\boldsymbol \\beta}_{LASSO}\\). Sin embargo, pero hay algoritmos eficientes para su estimación. De igual forma, se recomienda escalar las variables para remover el efecto de las unidades de medida. En R, la estimación del modelo por medio del estimador de LASSO se hace por medio de la función glmnet de la librería glmnet. 4.5.1 Datos de grasa corporal La estimación de los coeficientes de regresión del modelo (4.1) por medio del estimador de LASSO para diferentes valores de \\(\\lambda\\) se puede observar en la Figura 4.4. Se puede observar que a medida que \\(\\lambda\\) aumenta, mas coeficientes de regresión son iguales a cero. #se debe especificar alpha=1 X = model.matrix(mod.fat)[,-1] lasso.mod &lt;- glmnet(X, fat$brozek, alpha = 1,nlambda = 100) plot(lasso.mod,xvar=&#39;lambda&#39;,label=T,lwd=2,ylab=&#39;coeficientes de regresión&#39;) abline(h=0,lty=2) Figure 4.4: Datos de grasa corporal. Estimación de los coeficientes de regresión vs el logaritmo de \\(\\lambda\\) La pregunta sería que valor de \\(\\lambda\\) se debe seleccionar para determinar los coeficientes que son diferentes de cero. Una alternativa que tenemos para ello es por medio de validación cruzada. 4.5.2 Validación cruzada La validación cruzada (CV) es un método general para evaluar que tan bueno es un modelo para predecir observaciones futuras de la población objetivo del estudio. La idea es dividir la muestra en dos grupos: (1) Entrenamiento: se usa para ajusta el modelo. (2) Validación: se utiliza para validar el modelo ajustado. Para no perder información, en la CV se realiza en \\(K\\) iteraciones dividiendo los datos en \\(K\\) subconjuntos. En cada iteración, uno de los subconjutos es utilizado como datos de validación y el resto de subgrupos \\((K-1)\\) como datos de entrenamiento. Para cada división,\\(k = 1, . . . , K\\), y para cada valor de \\(\\lambda\\), se estima el modelo basado en la muestra de entrenamiento. Mientras que con cada muestra de validación, y para cada valor de \\(\\lambda\\), se utiliza para calcular el error cuadrático medio: \\[ EMC_{k}(\\lambda) = \\sum_{i=1}^{n_k} \\frac{[y_{i}^{(k)}-\\boldsymbol x_{i}^{(k)}\\widehat{\\boldsymbol \\beta}_{lasso}^{(k)}(\\lambda)]^2}{n} \\] donde \\(y_{i}^{(k)}\\) son las observaciones de la muestra de validación \\(k\\), y \\(\\widehat{\\boldsymbol \\beta}_{lasso}^{(k)}(\\lambda)\\) es la estimación utilizando la muestra de entrenamiento \\(k\\). Finalmente, para cada \\(\\lambda\\), se calcula: \\[ CV(\\lambda) = \\frac{1}{K}\\sum_{i=1}^{K}EMC_{k}(\\lambda) \\] y la desviación estándar: \\[ SD(\\lambda) = \\sqrt{\\sum_{i=1}^{K} \\frac{[EMC_{k}(\\lambda)-CV(\\lambda)]^2}{K-1}} \\] Luego, el valor de \\(\\lambda\\) seleccionado corresponde al valor que minimiza \\(CV(\\lambda)\\): \\[ \\hat{\\lambda}_{cv}=arg\\quad mín_{\\lambda}-CV(\\lambda) \\] También, se puede aplicar la regla de una desviación estándar: \\[ \\hat{\\lambda}_{cv1sd}=máx \\{\\lambda:CV(\\hat{\\lambda})&lt;CV(\\hat{\\lambda}_{cv})+SD(\\hat{\\lambda}_{cv})\\} \\] El segundo método es preferible dado que tiene en cuenta la variabilidad debida a la selección de las submuestras. 4.5.3 Ejemplo grasa corporal La estimación de \\(\\lambda\\) por medio de CV se puede hacer con la función cv.glmnet determinando del número de subconjuntos con el argumento nfolds. Por ejemplo, si consideramos \\(K=10\\), se obtienen los siguientes resultados: set.seed(123) lasso.cv &lt;-cv.glmnet(X, fat$brozek, nfolds = 10, alpha = 1,nlambda = 100) plot(lasso.cv) Figure 4.5: Datos de grasa corporal. Valiación cruzada (puntos rojos) para diferentes valores de \\(\\lambda\\). Las lineas cortadas verticales representan los valores seleccionados para \\(\\lambda\\) por mínimo valores de CV y la regla de una desviación estándar. Las covariables seleccionadas al utlizar el estimador de LASSO con el \\(\\lambda\\) óptimo (regla una desviación estándar) son: est = glmnet(X, fat$brozek, alpha = 1,lambda = lasso.cv$lambda.1se) est$beta ## 13 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## age 0.04428910 ## weight . ## height -0.14861287 ## neck -0.01394232 ## chest . ## abdom 0.61111812 ## hip . ## thigh . ## knee . ## ankle . ## biceps . ## forearm . ## wrist -1.08873009 La selección de variables por medio del estimador LASSO son age, height, neck,abdom y wrist. Ahora ajustamos el modelo con estas variables por medio de MCO: mod.lasso = lm(brozek ~ age+height+neck+abdom+wrist,data=fat) summary(mod.lasso) ## ## Call: ## lm(formula = brozek ~ age + height + neck + abdom + wrist, data = fat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.9618 -2.9130 -0.1425 2.9188 10.1956 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.23156 6.31879 0.353 0.72427 ## age 0.06064 0.02206 2.749 0.00642 ** ## height -0.15037 0.07806 -1.926 0.05520 . ## neck -0.39169 0.19570 -2.002 0.04644 * ## abdom 0.71970 0.03802 18.931 &lt; 2e-16 *** ## wrist -1.49192 0.44127 -3.381 0.00084 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.093 on 246 degrees of freedom ## Multiple R-squared: 0.7267, Adjusted R-squared: 0.7212 ## F-statistic: 130.8 on 5 and 246 DF, p-value: &lt; 2.2e-16 Las covariables selecciondas en este modelo son todas significativas, a excepción de la circunferencia del cuello. Además, \\(R^2=0.7267\\) y \\(R^2_{adj}=0.7178\\). Comparado con el modelo completo, estás cantidades han disminuido ligeramente. La Tabla 4.1 muestra las estimaciones de los modelos ajustados utilizando diferentes métodos de selección de variables. En los tres modelos coinciden con la inclusión de la circunferencia del abdomen y de la muñeca. Las variables peso y estatura se encuentran en algunos de estos modelos, pero no de forma simultanea. Esto sugiere que proporcionan la misma información y no es necesario incluirlas juntas. Finalmente, se puede observar en la Tabla 4.2 que los tres modelos proporcionan similar ajuste. Table 4.1: datos de grasa corporal. Estimación de coeficientes de los modelos seleccionados Método de selección AIC y PRESS BIC LASSO Est er. std. valor-p Est er. std. valor-p Est er. std. valor-p int. -20.062 10.8465 0.0656 -31.297 6.7089 0.0000 2.234 6.3188 0.7243 age 0.059 0.0285 0.0388 0.061 0.0221 0.0064 weight -0.084 0.037 0.0237 0.126 0.0229 0.0000 height -0.15 0.0781 0.0552 neck -0.432 0.208 0.0389 -0.392 0.1957 0.0464 abdom 0.877 0.0666 0.0000 0.921 0.0519 0.0000 0.72 0.038 0.0000 hip -0.186 0.1282 0.1473 thigh 0.286 0.1195 0.1473 forearm 0.4825 0.1725 0.0056 0.446 0.1682 0.0085 wrist -1.4049 0.4717 0.0032 -1.392 0.4099 8e-04 -1.492 0.4413 0.0000 Table 4.2: datos de grasa corporal. Valores de los criterios de selección para los modelos seleccionados. Modelo \\(R^2\\) PRESS AIC BIC AIC y PRESS 0.7467 4139.682 1420.225 1455.520 BIC 0.7351 4209.140 1423.471 1444.647 LASSO 0.7267 4416.440 1433.318 1458.024 La selección del modelo más adecuado no solo debe depender de un análisis estadístico, sino también de la opinión de expertos en el tema. "],["modelos-no-lineales.html", "Capítulo 5 Modelos no lineales 5.1 Ejemplos 5.2 Modelos no lineales 5.3 Método de Bootstrap", " Capítulo 5 Modelos no lineales 5.1 Ejemplos 5.1.1 Crecimiento de pavos Considere los datos de crecimiento de pavos (data(turk0)) de la librería alr4. A 60 corrales de pavos se les alimentó con una dieta similar, complementada con dosis de metionina diferente. Luego de un tiempo determnado, se observó el peso ganado por cada corral. Las variables son: A: Cantidad de suplemento de metionina ( % de la dieta). Gain: Peso medio ganado por corral (gramos) después de 3 semanas. El objetivo del estudio es evaluar la metionina como suplemento alimenticio para pavos. En la Figura 5.1 podemos observar que hay una relación positiva, a mayor cantidad de metionina, mayor peso ganado. Sin embargo, la relación no es lineal. Por lo que sería necesario hacer transformaciones sobre las variables o considerar modelos polinomiales para ajustar un modelo lineal a los datos. Otra alternativa es considerar modelos no lineales. plot(Gain~A,data=turk0,xlab=&#39;cantidad de metionina (% dieta)&#39;,ylab=&#39;Peso ganado por corral (gramos)&#39;) Figure 5.1: Datos de crecimiento de pavos. Relación entre el peso ganado por corral y el % de metionina en la dieta. Para estos datos se puede considerar el siguiente modelo de crecimiento para la media: \\[ E(Gain|A)=\\theta_{1}+\\theta_{2}[1-exp(-\\theta_{3}A)]. \\] Dado que este modelo no es una combinación lineal de los parámetros, \\(\\boldsymbol \\theta= (\\theta_1,\\theta_2,\\theta_3)\\), se considera como un modelo lineal. Este modelo es comunmente utilizado para ajustar crecimiento dado que sus parámetros tienen interpretación: Si \\(A=0\\), entonces \\(E(Gain|A)=\\theta_{1}\\). Es decir, \\(\\theta_{1}\\) determina el peso ganado medio sin suplemento de metionina. Si \\(\\theta_{3}&gt;0\\), \\(\\theta_{1}+\\theta_{2}\\) es la asíntota. Es decir, la ganancia máxima de peso que se puede lograr. \\(\\theta_{2}\\) es el máximo crecimiento adicional debido al suplemento. \\(\\theta_{3}\\) representa la tasa de crecimiento. A valores de \\(\\theta_3\\) mas grandes, el crecimiento esperado se acerca a su máximo más rápidamente. 5.1.2 Puromicina Los datos Puromycin de la librería datasets contienen información de la reacción enzimática (qué tan rápido ésta cataliza la reacción que convierte un sustrato en producto) de 23 encimas, donde 12 de ellas fueron tratadas con Puromicina (un antibiótico). Las variables son: conc :concentración de sustrato (ppm). rate:velocidad de reacción instantáneas (conteo/min2) state:tratado y no tratado. El objetivo es evaluar la velocidad de reacción y el efecto de la Puromicina. La relación de la velocidad de reacción en función de la concentración de sustraro se puede observar en la Figura 5.2. plot(Puromycin$conc,Puromycin$rate,col=Puromycin$state, xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) Figure 5.2: Datos de Puromicina. Diagrama de dispersión: Relación entre la velocidad de reacción y la concentración de sustrato. Encimas tratadas(puntos negros), Encimas no tratadas(puntos rojos) Aquí vamos a considerar el modelo Michaelis-Menten (modelo comunmente utilizado en bioquímica) para la media: \\[ E(y | x )=\\frac{x_{1}\\theta_{1}}{\\theta_{2}+x_{1}}. \\] En este caso, \\(\\theta_1\\) es la asíntota. Es decir la velocidad de reacción máxima. Mientras que \\(\\theta_2\\) determina la concentración en la cual se obtiene la mitad de la velocidad máxima. Esto lo podemos observar en la Figura 5.3. Figure 5.3: Modelo Michaelis-Menten Dado que se tienen células tratadas y no tratadas, se propone el siguiente modelo para la media: \\[ E(\\mbox{rate} | \\mbox{conc},\\mbox{state})=\\frac{\\mbox{conc}\\theta_{1}+\\mbox{state}\\times \\mbox{conc}\\theta_{3}} {\\theta_{2}+\\mbox{state}\\theta_{3}+\\mbox{conc}}. \\] Por lo que se tiene una curva diferente para las enzimas tratadas y no tratadas. Para las primeras la meida es: \\[ E(\\mbox{rate}|\\mbox{conc},\\mbox{state}=0)=\\frac{\\mbox{conc}\\theta_{1}}{\\theta_{2}+\\mbox{conc}}. \\] Mientras que, para las segundas la media está dada por: \\[ E(\\mbox{rate}|\\mbox{conc},\\mbox{state}=1)=\\frac{\\mbox{conc}(\\theta_{1}+\\theta_{3})}{(\\theta_{2}+\\theta_{4})+\\mbox{conc}}. \\] De aquí, \\(\\theta_3\\) determina la velocidad máxima entre las células tratadas y no las no tratadas. De igual forma, \\(\\theta_{4}\\) indica la diferencia en la concentración en la cual se obtiene la mitad de la velocidad máxima. 5.2 Modelos no lineales De forma general, en los modelos de regresión asumimos que: \\[ y_{i}=m(\\boldsymbol x_{i},\\boldsymbol \\theta)+\\varepsilon_{i}, \\] donde \\(\\varepsilon_{i} \\sim N(0, \\sigma^2)\\) y \\(cov(\\varepsilon_{i},\\varepsilon_{j}) = 0\\), para todo \\(i \\neq j\\). Particularmente, en el caso de modelos lineales, se asume que: \\[ m(x_{i},\\theta)=\\boldsymbol x_i^{*&#39;}\\boldsymbol \\beta= \\beta_{0}+x_{1i}^{*}\\beta_{1}+x_{2i}^{*}\\beta_{2}+\\ldots +x_{p-1,i}^{*}\\beta_{p-1}, \\] donde \\(x_j^{*}\\) representa cualquier transformación de las covariables originales, por ejemplo, \\(x_j^{*}= \\log x_j\\) o \\(x_j^{*}= x_j^{2}\\). Estos modelos se consideran lineales porque son combinaciones lineales de los parámetros \\(\\boldsymbol \\beta\\). Modelos de la forma: \\[ y_{i}=m(x_{i},\\theta)+\\varepsilon_{i}=\\theta_{1}+\\theta_{1}[1+exp(-\\theta_{3}x_{i})]+\\varepsilon_{i}, \\] o \\[ y_{i}=m(x_{i},\\theta)+\\varepsilon_{i}=\\frac{x_{i}\\theta_{i}}{\\theta_{2}+x_{i}}+\\varepsilon_{i}, \\] son ejemplos de modelos no lineales. Es decir, no son combinación lineal de los parámetros. 5.2.1 Modelos no-lineales linealizables Dentro de los modelos no lineales se encuentran algunos que pueden ser linealizables a través de transformaciones sobre las variables. Por ejemplo, el modelo de regresión exponencial: \\[ E(y)=\\theta_{0}+\\theta_{1}\\exp(\\theta_{2}+x_{i}\\theta_{3}), \\] Si \\(\\theta_{0}=0\\), es linealizable: \\[ \\log y_{i}=(\\log\\theta_{1}+\\theta_{2})+x_{i}\\theta_{3}+\\varepsilon_{i}^* = \\beta_0+x_{i}\\beta_1+\\varepsilon_{i}^*. \\] 5.2.2 Estimación de los parámetros Al igual que en los modelos lineales, la estimación de \\(\\boldsymbol \\theta\\) se hace minimizando la suma de cuadrados de los residuos: \\[ S(\\boldsymbol \\theta)=\\sum_{i=1}^n[y_{i}-m(\\boldsymbol x_{i},\\boldsymbol \\theta)]^2. \\] Por lo que, para encontrar el mínimo, primero calculamos la derivada de \\(S(\\boldsymbol \\theta)\\) con respecto a \\(\\boldsymbol \\theta\\): \\[ \\frac{\\partial}{\\partial\\boldsymbol \\theta}S(\\boldsymbol \\theta)=-2\\sum_{i=1}^n[y_{i}-m(\\boldsymbol x_{i},\\boldsymbol \\theta)] \\begin{bmatrix}\\frac{\\partial }{\\partial\\boldsymbol \\theta}m(\\boldsymbol x_{i},\\boldsymbol \\theta)\\end{bmatrix}, \\] luego igualamos a \\(0\\), y finalmente, resolvemos la ecuación para \\(\\boldsymbol \\theta\\). Sin embargo, en la mayoría de los casos, \\(\\frac{\\partial}{\\partial\\boldsymbol \\theta}S(\\boldsymbol \\theta)\\) es una función no-lineal \\(\\boldsymbol \\theta\\). Por lo que no es posible encontrar una solución analítica y necesitamos encontrarla de forma iterativa. Las técnicas iterativas de estimación de los parámetros están basadas en expansiones de series de Taylor. Cualquier función \\(f(\\theta)\\) puede expandirse como una serie de Taylor de la forma: \\[ f(\\theta)=\\sum_{n=0}^\\infty \\frac{1}{n!}(\\theta-\\theta^*)^n \\frac{\\partial^nf(\\theta^*)}{\\partial\\theta^n}. \\] Una aproximación de la función \\(f(\\theta)\\) en los valores alrededor del punto \\(\\theta^*\\) se puede hacer usando series de Taylor de orden dos. Esto es: \\[ f(\\theta)\\approx f(\\theta^*)+(\\theta-\\theta^*) \\frac{\\partial f(\\theta^*)}{\\partial\\theta}+\\frac{1}{2}(\\theta-\\theta^*)^2\\frac{\\partial^2 f(\\theta^*)}{\\partial\\theta^2}. \\] En caso de que \\(\\boldsymbol \\theta\\) sea un vector de dimensión \\(p\\), la aproximación por series de Taylor de orden dos se expresa de la siguiente manera: \\[ f(\\boldsymbol \\theta)\\approx f(\\boldsymbol \\theta^*)+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u(\\boldsymbol \\theta^*)+\\frac{1}{2}(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}H(\\boldsymbol \\theta^*)(\\boldsymbol \\theta-\\boldsymbol \\theta^*), \\] donde: \\[ u(\\boldsymbol \\theta)=\\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta}=\\begin{pmatrix} \\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\theta_1} \\\\ \\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\theta_2} \\\\ \\vdots \\\\ \\frac{\\partial S(\\boldsymbol \\theta)}{\\partial\\theta_p} \\\\ \\end{pmatrix} \\qquad u(\\widehat{\\boldsymbol \\theta})=u(\\boldsymbol \\theta)|_{\\boldsymbol \\theta=\\widehat{\\boldsymbol \\theta}}, \\] es el vector score, y \\[ \\boldsymbol H(\\boldsymbol \\theta)= \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta^{&#39;} \\partial\\boldsymbol \\theta}=\\begin{pmatrix} \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial^2 \\theta_{1}} &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\theta_{1} \\partial \\theta_{2} } &amp; \\dots &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\theta_{1} \\partial \\theta_{p}} \\\\ \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\theta_{2} \\partial \\theta_{1}} &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial^2 \\theta_{2}} &amp; \\dots &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\theta_{2} \\partial \\theta_{p}}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\theta_{p} \\partial \\theta_{1}} &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial \\theta_{p} \\partial \\theta_{2}} &amp; \\dots &amp; \\frac{\\partial^2 S(\\boldsymbol \\theta)}{\\partial^2 \\theta_{p}} \\end{pmatrix} \\quad \\quad \\quad \\boldsymbol H(\\widehat{\\boldsymbol \\theta})=\\boldsymbol H(\\boldsymbol \\theta)|_{\\boldsymbol \\theta=\\widehat{\\boldsymbol \\theta}}, \\] es la matriz hessiana. Algunos métodos iterativos de estimación son: Gauss-Newton (aproximación de series de Taylor a la función de la media) y Newton-Raphson (aproximación de series de Taylor a la función score). Para ajustar modelos ni lineales, generalmente, se opta por la primera. La idea es aproximar \\(m(x_{i},\\boldsymbol \\theta^*)\\) usando series de Taylor de orden uno alrededor de \\(\\boldsymbol \\theta^*\\), esto es: \\[\\begin{equation} m(x_{i},\\boldsymbol \\theta)\\approx m(x_{i},\\boldsymbol \\theta^*)+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u_{i}(\\boldsymbol \\theta^*). \\tag{4.1} \\end{equation}\\] Note que (4.1) tiene la forma de un modelo lineal, donde \\(u_{i}(\\boldsymbol \\theta^*)\\) juega el papel de vector de covariables. La diferencia radica en que \\(u_{i}(\\boldsymbol \\theta^*)\\) puede depender de los parámetros. La aproximación de la suma de cuadrados de los residuos es: \\[\\begin{equation} \\begin{split} S(\\boldsymbol \\theta)&amp;=\\sum_{i=1}^n[y_{i}-m(x_{i},\\boldsymbol \\theta)]^2 \\approx \\sum_{i=1}^n[y_{i}-m(x_{i},\\boldsymbol \\theta^*)+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u_{i}(\\boldsymbol \\theta^*)]^2 \\\\ &amp;= \\sum_{i=1}^n[\\widehat e_{i}^*+(\\boldsymbol \\theta-\\boldsymbol \\theta^*)^{&#39;}u_{i}(\\boldsymbol \\theta^*)]^2. \\end{split} \\nonumber \\end{equation}\\] Esta aproximación de \\(S(\\boldsymbol \\theta)\\) es equivalente a una suma de cuadrados de un modelo lineal con \\(\\widehat e_{i}^*\\) (llamados residuos de trabajo) como variable respuesta y \\(u_{i}(\\boldsymbol \\theta^*)\\) como vector de covariables. Por lo tanto, la solución es: \\[ (\\widehat{\\boldsymbol \\theta}-\\boldsymbol \\theta^*)=[U(\\boldsymbol \\theta^*)^{&#39;}U(\\boldsymbol \\theta^*)]^{-1}U(\\boldsymbol \\theta^*)^{&#39;} \\widehat e^*, \\] \\[\\begin{equation} \\hat{\\boldsymbol \\theta}=\\boldsymbol \\theta^*+[U(\\boldsymbol \\theta^*)^{&#39;}U(\\boldsymbol \\theta^*)]^{-1}U(\\boldsymbol \\theta^*)^{&#39;} \\widehat e^*, \\tag{5.1} \\end{equation}\\] donde \\(\\widehat e_{i}^*=(\\widehat e_{1}^*,\\ldots,\\widehat e_{n}^*)^{&#39;}\\), \\(U(\\boldsymbol \\theta^*)\\) es una matriz con la \\(i\\)-ésima fila igual a \\(u_{i}(\\boldsymbol \\theta^*)\\). A partir de estas ecuaciones se propone el siguiente algoritmo (de Gauss-Newton): seleccione unos valores iniciales \\(\\boldsymbol \\theta_{0}\\) y calcule \\(S(\\boldsymbol \\theta_{0})\\), establezca el contador de iteraciones en \\(j=0\\), calcule \\(U(\\boldsymbol \\theta^{(j)})\\) y \\(\\widehat e^{(j)}\\) y encuentre \\(\\boldsymbol \\theta^{(j+1)}\\) usando (5.1). Esto es: \\[\\begin{equation} \\begin{split} &amp; \\boldsymbol \\theta^{(j+1)}=\\boldsymbol \\theta^{(j)}+[U(\\boldsymbol \\theta^{(j)})^{&#39;}U(\\boldsymbol \\theta^{(j)})]^{-1}U(\\boldsymbol \\theta^{(j)})^{&#39;} \\widehat e^{(j)}, \\end{split} \\end{equation}\\] y calcule \\(S(\\boldsymbol \\theta^{(j+1)})\\). Pare si \\(\\delta=|S(\\boldsymbol \\theta^{(j)})-S(\\boldsymbol \\theta^{(j+1)})|\\) es suficientemente pequeño (convergencia). De otra forma, \\(j=j+1\\) y vaya al paso 3. Si hay demasiadas iteraciones (\\(j\\) es muy grande), se dice que hay divergencia. La estimación de \\(\\sigma^2\\) es: \\[ \\widehat{\\sigma}^2=\\frac{1}{n-p}\\sum_{i=1}^n[y_{i}-m(\\boldsymbol x_{i},\\widehat{\\boldsymbol \\theta})]^2. \\] Todos los métodos iterativos de selección requieren de la selección de valores iniciales. Para evitar problemas de divergencia, es preferible escoger valores que estén cercanos a los parámetros. Esto se puede lograr basándose en conocimiento previo del problema, significado físico de los coeficientes, evaluación gráfica o linealización de los datos. Otro aspecto que se debe tener en cuenta es que este tipo de algoritmos pueden caer en un máximo local. Por lo que es recomendable ejecutar el algoritmo para diferentes valores iniciales. En el caso del peso de los pavos, se pueden utilizar los siguientes valores iniciales: \\(\\theta_{1}^0=620\\) (promedio del peso ganado sin suplemento). \\(\\theta_{2}^0+\\theta_{1}^0=800\\) (valor cercano al máximo peso obtenido). Por lo tanto \\(\\theta_{2}^0=180\\) \\(\\theta_{3}^0\\) se puede obtener resolviendo la siguiente ecuación \\(E(y) = \\theta_1^{0}+\\theta_2^{0}[1-\\exp(-\\theta_{3} A)]\\) para un posible punto. Por ejemplo, si \\(A=0.16\\), un valor posible para \\(E(y)\\) es 750. Por lo que: \\[ 750=620+180[1-\\exp(-\\theta_{3}^00.16)]. \\] Resolviendo la ecuación \\(\\theta_{3}^0\\approx8\\). En R, la estimación del modelo lineal se hace usando la función ´nls()´. Para el caso de crecimiento de los pavos: mod.turk = nls(Gain ~ th1 + th2*(1-exp(-th3*A)),data = turk0, start = list(th1=620,th2=180,th3=8),trace=F) mod.turk ## Nonlinear regression model ## model: Gain ~ th1 + th2 * (1 - exp(-th3 * A)) ## data: turk0 ## th1 th2 th3 ## 622.958 178.252 7.122 ## residual sum-of-squares: 12367 ## ## Number of iterations to convergence: 4 ## Achieved convergence tolerance: 6.719e-06 Por lo que la estimación de la media del peso ganado por corral es: \\[ E(\\mbox{Gain}|A)=622.958 + 178.252 [1 - \\exp(-7.122A)]. \\] La figura 5.4 muestra el ajuste de forma gráfica. plot(Gain~A,data=turk0,xlab=&#39;cantidad de metionina (% dieta)&#39;,ylab=&#39;Peso ganado (gramos)&#39;) th.turk = coef(mod.turk) x = seq(0,0.5,length.out=100) lines(x, th.turk[1]+th.turk[2]*(1-exp(-th.turk[3]*x)),col=2, lwd = 2) Figure 5.4: Datos de crecimiento de pavos. Ajuste del modelo para el peso medio ganado por corral en función de la cantidad de suplemento de metionina. 5.2.3 Inferencia sobre los parámetros Las propiedades del \\(\\widehat{\\boldsymbol \\theta}\\) se basan en teoría asintótica, es decir que son válidas si se tiene muestras grandes. Si \\(\\varepsilon \\sim N(\\boldsymbol 0,\\sigma^2 \\boldsymbol I)\\), tenemos que asintóticamente \\((n \\to \\infty)\\): \\[ \\widehat{\\boldsymbol \\theta}\\sim N\\left(\\boldsymbol \\theta,\\sigma^2\\left[U(\\boldsymbol \\theta^*)U(\\boldsymbol \\theta^*)&#39;\\right]^{-1}\\right). \\] Esto quiere decir que, \\(\\widehat{\\boldsymbol \\theta}\\) es un estimador asintoticamente insesgado de \\(\\boldsymbol \\theta\\) y se distribuye de forma normal. Además, se puede probar que es un estimador eficiente (es decir, de mínima varianza). A partir de estos resultados se pueden hacer inferencias sobre los coeficientes \\(\\boldsymbol \\theta\\). Por ejemplo, pruebas de hipótesis sobre algún coeficiente \\(\\theta_j\\) del modelo: \\[ \\qquad H_{0}:\\theta_{j}=\\theta_{0}, \\qquad H_{1}:\\theta_{j}\\ne\\theta_{0}, \\] se puede hacer a través del siguiente estadístico de prueba: \\[ t_{0}:\\frac{\\widehat{\\theta_{j}}-\\theta_{0}}{\\sqrt{V(\\widehat{\\theta_{j}})}}, \\quad \\text{donde} \\quad t_{0} \\sim t_{n-p}. \\] También, para pruebas de hipótesis sobre un subconjuntos de parámetros de \\(\\boldsymbol \\theta=(\\boldsymbol \\theta_{1}^{&#39;},\\boldsymbol \\theta_{2}^{&#39;})^{&#39;}\\): \\[ H_{0}:\\boldsymbol \\theta_{2}=\\boldsymbol 0\\qquad H_{1}:\\boldsymbol \\theta_{2}\\ne\\boldsymbol 0, \\] el estadístico de prueba que se utiliza es: \\[ F_{0}=\\frac{[SS_{res}(\\boldsymbol \\theta)-SS_{res}(\\boldsymbol \\theta_{1})]/r}{MS_{res}(\\boldsymbol \\theta)}\\sim F_{r,n-p}. \\] De igual forma, la estimación por intervalos del \\((1-\\alpha)\\)% de confianza para \\(\\theta_{j}\\) se puede calcular como: \\[ \\hat{\\theta_{j}}\\pm t_{1-\\alpha/2,n-p}\\sqrt{V(\\hat{\\theta_{j}})} , \\] Con la función confint de R se pueden obtener los IC de los coeficientes a través de los perfiles de verosimilitud. Hay que hacer énfasis que estas son propiedades para muestras grandes, para muestras pequeñas, estas propiedades pueden ser inadecuadas y se puede recurrir a otras alternativas, como el bootstrap. Para el modelo de crecimiento de los pavos, tenemos que: summary(mod.turk) ## ## Formula: Gain ~ th1 + th2 * (1 - exp(-th3 * A)) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## th1 622.958 5.901 105.57 &lt; 2e-16 *** ## th2 178.252 11.636 15.32 2.74e-16 *** ## th3 7.122 1.205 5.91 1.41e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 32 degrees of freedom ## ## Number of iterations to convergence: 4 ## Achieved convergence tolerance: 6.719e-06 A partir de este ajuste se puede concluir que el peso medio obtenido sin suplemento es de \\(622\\). Mientras que la ganancia máxima que se obtiene con el suplemento es de \\(178\\). A partir de las pruebas \\(t\\) se puede determinar que la ganancia máxima es significativamente mayor que cero. Los intervalos de confianza para \\(\\boldsymbol \\theta\\) se obtienen de la siguiente forma: confint(mod.turk) ## Waiting for profiling to be done... ## 2.5% 97.5% ## th1 610.970918 634.855196 ## th2 156.322676 204.476710 ## th3 4.966356 9.777723 Aquí podemos ver que, por ejemplo, la ganancia máxima que se puede obtener con el suplemento está entre \\(156\\) y \\(204\\) gramos con un nivel de confianza del 95%. Para calcular los intervalos de confianza para la media podemos utilizar la función predFit de la librería investr. La Figura 5.5 muestra el intervalo del 95% del peso ganado medio en función del % de metionina en la dieta. library(investr) predx = data.frame(A=seq(from=0,to=0.5,length.out=100)) predGain = predFit(mod.turk,predx,interval=&#39;confidence&#39;) plot(Gain~A,data=turk0,xlab=&#39;cantidad de metionina (% dieta)&#39;,ylab=&#39;Peso ganado (gramos)&#39;) th = coef(mod.turk) x = seq(0,0.5,length.out=100) lines(x, th.turk[1]+th.turk[2]*(1-exp(-th.turk[3]*x)),col=2, lwd = 2) lines(predx$A,predGain[,2],col=2,lty=2,lwd=2) lines(predx$A,predGain[,3],col=2,lty=2,lwd=2) Figure 5.5: Datos pavos. Intervalo del 95% de confianza para la media del peso ganado en función de la dieta. Es posible considerar otros modelos de crecimiento para este conjunto de datos. Algunos de estos son el modelo logístico: \\[ y_{i}=\\frac{\\theta_{1}}{1+\\theta_{2}\\exp(-\\theta_{3}x_{i})}+\\varepsilon_{i}, \\] o el modelo Weibull: \\[ y_{i}=\\theta_{1}+\\theta_{2}[1-\\exp(-\\theta_{3}x_{i}^{\\theta_{4}})]+\\varepsilon_{i}. \\] Para compararlos, podemos ajustarlos y verificar cuál de los tres proporciona mejor ajuste usando criterios de información (AIC o BIC). En la Figura 5.6, y a través del BIC, podemos ver que los tres modelos proporcionan casi el mismo ajuste. Así que, cualquiera puede ser usado para hacer inferencias y sacar conclusiones. Figure 5.6: Datos de crecimiento de pavos. Ajuste del modelo de crecimiento propuesto (rojo), modelo logístico (verde) y Modelo Weibull (azul). ## [1] 318.9086 ## [1] 318.6193 ## [1] 322.3054 5.2.3.1 Puromicina-Estimación Los valores iniciales del modelo Michaelis-Menten se puede hacer por linealización. Tenemos que: \\[ m(\\boldsymbol x,\\boldsymbol \\theta) = \\frac{\\theta_1 x}{x + \\theta_2}, \\] es linealizable aplicando la inversa a ambos lados de la ecuación: \\[ \\frac{1}{m(\\boldsymbol x,\\boldsymbol \\theta)} = \\frac{x + \\theta_2}{\\theta_1 x} = \\frac{1}{\\theta_1} + \\frac{1}{x}\\frac{\\theta_2}{\\theta_1}. \\] En la Figura 5.7 podemos ver que aplicando inversas a ambas variables se obtiene una relación aproximadamente lineal. Figure 5.7: Datos de puromicina. Diagrama de dispersión. Datos linealizados(derecha). A través del ajuste por MCO para: \\[ 1/y_{i}=\\beta_{0}+1/\\mbox{conc}_{i}\\beta_{1}+\\varepsilon_{i}, \\] se puede obtener valores iniciales. Luego de ajustar el modelo se obtiene que: \\[ \\theta_{1}^{(0)}=\\frac{1}{\\hat{\\beta}_{0}}=167.408 \\mbox{ y } \\theta_{2}^{(0)}=\\frac{\\hat{\\beta}_{1}}{\\hat{\\beta}_{0}}=0.039. \\] Inicialmente, se podríamos asumir que \\(\\beta_{3}^{(0)}=\\beta_{4}^{(0)}=0\\) (es decir, no hay diferencias entre las células tratadas y no tratadas). Usando la función ´nls()´ de R obtenemos los siguientes resultados: Puromycin$state2 = as.double(Puromycin$state == &#39;treated&#39;) mod.puromicyn = nls(rate ~ (th1*conc + th3*conc*state2)/(th2+th4*state2+conc),data = Puromycin, start = list(th1=168,th2=0.4,th3=0,th4=0),trace=F ) summary(mod.puromicyn ) ## ## Formula: rate ~ (th1 * conc + th3 * conc * state2)/(th2 + th4 * state2 + ## conc) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## th1 1.603e+02 6.896e+00 23.242 2.04e-15 *** ## th2 4.771e-02 8.281e-03 5.761 1.50e-05 *** ## th3 5.240e+01 9.551e+00 5.487 2.71e-05 *** ## th4 1.641e-02 1.143e-02 1.436 0.167 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.4 on 19 degrees of freedom ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 3.021e-06 Con estos resultados podemos concluir que la tasa de crecimiento máxima es significativamente diferente si la enzima es tratada o no (\\(\\theta_3\\) es significativamente diferente de cero). Sin embargo, el punto en que se alcanza la mitad del máximo es el mismo (\\(\\theta_4\\) no es significativamente diferente de cero). El ajuste se puede observar de forma gráfica en la Figura 5.8. Las líneas horizontales muestran la diferencia entre las asíntotas entre las céludas tratadas y las no tratadas. Mientras que, las líneas verticales la concentración donde se obtiene la mitad de la velocidad de reacción máxima. Figure 5.8: Datos de puromicina. Estimación de la velocidad media de reacción para enzimas tratadas(línea negra) y enzimas no tratadas(línea roja). 5.3 Método de Bootstrap Los métodos de inferencia que se muestran anteriormente se basan en la distribución asintótica de los estimadores. Por lo que pueden ser inexactas y/o engañosas si tenemos muestras pequeñas. En esos casos, una alternativa preferible es usar ténicas de remuestreo (bootstrap). El bootstrap es una técnica que se puede usar para calcular intervalos de confianza and pruebas de hipótesis cuando el cumplimiento de los supuestos distribucionales asumidos sobre los estimadores están en duda (por ejemplo, normalidad de \\(\\widehat{\\boldsymbol \\theta}\\)). Por ejemplo, si se quiere calcular un intervalo de confianza para la mediana de \\(Y\\) no sabemos la distribución del estimador. Por lo tanto, podemos calcularlo utilizando bootstrap. Para esto, seguimos los siguientes pasos: Obtener una muestra aleatoria con remuestreo de \\(\\boldsymbol y=(y_{1}^{*},...,y_{n}^{*})\\), calcular la mediana usando la muestra el paso 1 \\((Me_{1}^*)\\). repetir los pasos 1 y 2 un número grande de veces \\((B)\\). De aquí, la distribución muestral de \\(Me\\) se obtiene a partir de los remuestreos. Esto es \\((Me_{1}^*,\\ldots,Me_{B}^*)\\). El intervalo de confianza del 95% se puede calcular a partir de los percentiles muestrales 2.5% y 97.5 % de \\((Me_{1},\\ldots,Me_{B})\\). 5.3.1 Bootstrap en regresión Para un modelo de regresión, el remuestreo se hace sobre los observaciones, considerando la respuesta y las covariables, \\((y_i,\\boldsymbol x_i)\\), o sobre los residuos del ajuste por MCO. El segundo método es llamado residual resampling y se realiza de las siguiente forma: Con la muestra dada, \\((x_{i},y_{i}),i=1,...,n\\), ajustar el modelo y obtener los residuos: \\(e_{i}=y_{i}-x_{i}^{&#39;}\\hat{\\beta}\\), obtener una muestra aleatoria con reemplazo de los residuos, \\(e^*=(e_{1}^*,...,e_{n}^*)\\), 3.crear una variable respuesta bootstrap \\(y_{i}^*=\\boldsymbol x_{i}^{&#39;}\\widehat{\\boldsymbol \\beta}+e_{1}^*\\) y estimar \\(\\widehat{\\boldsymbol \\beta}^*_1\\), 4. repetir los pasos 1-3 un número grande de veces \\((B)\\), 5. obtener el intervalo de confianza para \\(\\beta_{j}\\) usando los percentiles de \\((\\widehat{\\beta}^*_{j,1},\\ldots,\\widehat{\\beta}^*_{j,B})\\). En la literatura se pueden encontrar muchas modificaciones y extensiones para calcular los intervalos de confianza y para realizar el remuestreo. Para los datos de la Puromicina, el bootstrap en R se puede implementar utilizando la función Boot de la librería car: set.seed(310) mod.boot = Boot(mod.puromicyn,method=&#39;residual&#39;,R=1000) mod.boot ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot::boot(data = data.frame(update(object, model = TRUE)$model), ## statistic = boot.f, R = R, .fn = f, parallel = p_type, ncpus = ncores, ## cl = cl2) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 160.27996673 8.913060e-01 6.33925160 ## t2* 0.04770806 -1.031726e-05 0.00748367 ## t3* 52.40375557 8.813317e-02 8.90147827 ## t4* 0.01641319 -3.563328e-05 0.01053056 En el objeto mod.boot se pueden observar todas las estimaciones calculadas en los \\(1000\\) remuestreos. Por ejemplo, la relaciones entre las estimaciones se pueden observar a través de un diagrama de dispersión (Figura 5.9). Aquí vemos una correlaciones altas en algunas de las estimaciones. pairs(mod.boot$t,labels=c(expression(hat(theta)[1]),expression(hat(theta)[2]),expression(hat(theta)[3]), expression(hat(theta)[4]))) Figure 5.9: Datos de puromicina. Diagrama de dispersión para las estimaciones bootstrap de los coeficientes del modelo. Igualmente, la distribución muestral de los estimadores de los coeficientes se pueden obtener a partir de histogramas (ver Figura 5.10). Estas parecen aproximarse a distribuciones normales, sin embargo, hay leves asimetrías en \\(\\widehat{\\theta}_1\\) y \\(\\widehat{\\theta}_2\\). par(mfrow=c(1,4)) hist(mod.boot$t[,1],breaks = 20,xlab=expression(hat(theta)[1]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[1],lty=2,lwd=2) abline(v=mean(mod.boot$t[,1]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,1],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) hist(mod.boot$t[,2],breaks = 20,xlab=expression(hat(theta)[2]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[2],lty=2,lwd=2) abline(v=mean(mod.boot$t[,2]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,2],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) hist(mod.boot$t[,3],breaks = 20,xlab=expression(theta[3]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[3],lty=2,lwd=2) abline(v=mean(mod.boot$t[,3]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,3],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) hist(mod.boot$t[,4],breaks = 20,xlab=expression(theta[4]),main = &#39;&#39;) abline(v=coef(mod.puromicyn)[4],lty=2,lwd=2) abline(v=mean(mod.boot$t[,4]),lty=2,lwd=2,col=2) abline(v=quantile(mod.boot$t[,4],c(0.025,0.975),na.rm = T),lty=2,lwd=2,col=2) Figure 5.10: Datos de puromicina. Histograma de las estimaciones de los coeficientes del modelo por bootstrap. la linea negra indica la estimación por mínimos cuadrados.La lineas rojas representa los percentiles 2.5 y 97.5 de los remuestreos. Finalmente, los intervalos del 95% de confianza usando bootstrap se pueden obtener a partir de los percentiles 2.5% y 97.5% de las distribuciones remuestreadas: quantile(mod.boot$t[,1],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## 150.2787 175.1420 quantile(mod.boot$t[,2],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## 0.03538989 0.06328547 quantile(mod.boot$t[,3],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## 34.81826 70.20826 quantile(mod.boot$t[,4],c(0.025,0.975),na.rm = T) ## 2.5% 97.5% ## -0.006628693 0.035401146 En la Figura 5.11 se pueden observar todas las estimaciones de la media obtenidas a partir de los 1000 remuestreos. Se puede observar que en todos los casos, la máxima velocidad de obtuvo para las células tratadas. fit.rate= function(theta,x){ cbind(theta[1]*x/(theta[2]+x), (theta[1]+theta[3])*x/(theta[2]+theta[4]+x)) } x= seq(from=0,to=1.1,length.out = 100) plot(NULL,NULL,xlim=c(0,1.1),ylim=c(0,210),xlab=&#39;concentración de sustrato (ppm)&#39;, ylab=&#39;velocidad de reacción&#39;) Fit = mapply(function(i){ pred = fit.rate(mod.boot$t[i,],x) lines(x,pred[,1],col=&#39;lightgray&#39;) lines(x,pred[,2],col=&#39;gray&#39;) },i=1:999) lines(x, thP[1]*x/(thP[2]+x),col=2) lines(x, (thP[1]+thP[3])*x/(thP[2]+thP[4]+x)) points(Puromycin$conc,Puromycin$rate,col=Puromycin$state) Figure 5.11: Datos de puromicina. Curvas medias estimadas por bootstrap. La estimación puntual de la media para las enzimas tratadas es la línea negro y para las enzimas no tratadas es la línea roja. 5.3.2 Algunas consideraciones Finalmente, algunas consideraciones que se debe de tener a la hora de estimar un modelo no lineal: Lo ideal es que el algoritmo llegue a la solución en pocas iteraciones. Siempre es bueno evaluar la solución con diferentes puntos iniciales. Es posible que caigamos en un máximo local. Si el tamaño de muestra no es grande, las propiedades asintóticas pueden no ser adecuadas. Por lo tanto, es mas conveniente usar bootstrap para hacer inferencias. "],["modelo-lineal-generalizado.html", "Capítulo 6 Modelo lineal generalizado 6.1 Introducción 6.2 Casos de estudio 6.3 Preambulo 6.4 Modelo lineal generalizado (GLM) 6.5 Ajuste de un GLM 6.6 Pruebas de hipótesis 6.7 intervalos de confianza 6.8 Devianza 6.9 Bondad de ajuste 6.10 Selección de variables", " Capítulo 6 Modelo lineal generalizado 6.1 Introducción Asumiendo independencia entre las observaciones \\(y_1,\\ldots,y_n\\), los modelos lineales se expresan como: \\[ \\boldsymbol y=\\boldsymbol X\\boldsymbol \\beta+\\boldsymbol \\varepsilon, \\quad \\text{donde}\\quad \\boldsymbol \\varepsilon\\sim N(\\boldsymbol 0,\\sigma^2\\boldsymbol I), \\] donde \\(\\boldsymbol y\\) es el vector de variable respuesta, \\(\\boldsymbol X\\) es la matriz \\(n \\times p\\) de covariables, \\(\\boldsymbol \\beta\\) el vector de coeficientes, y \\(\\boldsymbol \\varepsilon\\) el vector de los errores. De aquí podemos derivar que \\(\\boldsymbol y\\sim N(\\boldsymbol X\\boldsymbol \\beta, \\sigma^{2}\\boldsymbol I)\\). Esto implica, entre otras cosas, que: hay una relación lineal entre el valor esperado de la variable respuesta y el conjunto de covariables, la varianza de la variable respuesta es constante, la inferencia asume que la variable respuesta sigue una distribución normal. Sin embargo, hay situaciones en donde estas propiedades no son factibles, incluso luego de hacer transformaciones sobre las variables. Por ejemplo, si la variable de respuesta es dicotómica \\((y_{i}\\in\\{0,1\\})\\) no podríamos representar \\(E(y|\\boldsymbol x)\\) como una función lineal. Además, la distribución asociada dificilmente serían normal. Aquí es mas conveniente es proponer una distribución de probabilidad diferente, por ejemplo, una Bernoulli, y ajustar un modelo lineal generalizado (GLM). 6.2 Casos de estudio 6.2.1 Mortalidad de escarabajos En un estudio de toxicología, se está interesado en la tasa de mortalidad de escarabajos expuestos a disulfuro de carbono gaseoso. La Tabla 6.1 muestra el número de escarabajos muertos después de cinco horas de exposición de este tóxico a diferentes concentraciones del químico. Podemos ver que a medida que aumenta la dosis, la mortalidad de escarabajos va aumentando. Table 6.1: Número de escarabajos expuestos y muertos a diferentes dosis (escala logarítmica) de disulfuro de carbono gaseoso. log. dosis expuestos muertos 1.6907 59 6 1.7242 60 13 1.7552 62 18 1.7842 56 28 1.8113 63 52 1.8369 59 53 1.8610 62 61 1.8839 60 60 Lectura de datos en R: En la Figura 6.1 podemos observar como es el aumento de la tasa de mortalidad con respecto a la dosis del tóxico. Note que la relación no es lineal, si no que tiene una forma de ‘S’. Esto hace que un modelo de regresión lineal sea inadecuado (además una línea recta permitiría estimaciones por debajo de cero y encima de 1). Figure 6.1: Datos de mortalidad de escarabajos. Relación entre la concentración de disulfuro de carbono gaseoso (en escala logarítmica) y la proporción de muertes (muertos/expuestos) de los escarabajos. Una alternativa es utilizar una transformación que describa la relación de forma adecuada y que garantice que la respuesta se encuentre dentro del espacio del parámetro. Es decir, que la estimación de la proporción de escarabajos muertos esté entre \\(0\\) y \\(1\\). 6.2.2 ataques de epilepsia Los datos data(epilepsy) de la librería HSAUR2 corresponden a un ensayo clínico para evaluar el efecto del fármaco Progabida sobre los ataques epilépticos. Al inicio del estudio, los \\(59\\) pacientes epilépticos fueron observados durante 8 semanas, y se registró el número de convulsiones. Luego, fueron aleatorizados al tratamiento con el fármaco Progabide (\\(31\\) pacientes) o al grupo de placebo (\\(28\\) pacientes). A los pacientes de ambos grupos se les observo durante cuatro períodos de dos semanas y se registró el número de convulsiones. Estos datos son de naturaleza longitudinal (cada paciente cuenta con 4 observaciones tomadas en el tiempo), lo que requiere metodologías que tengan en cuenta la correlación que hay en los datos. En es caso, no consideraremos las observaciones intermedias de cada paciente, sino solamente las mediciones tomadas luego de las cuatro semanas de observación. Las variables que se tendrán en cuenta son las siguientes: * age: edad del paciente, base: número de ataques epilépticos en las 8 semanas antes de iniciar los tratamientos, treatment: tratamiento (placebo, Progabida), seizure.rate(variable respuesta): número de ataques epilépticos en las últimas dos semanas al final de estudio (luego cuatro semanas de tratamiento). Figure 6.2: Datos de epilepsia. Relación entre el número de ataques epilépticos con la edad, ataques previos (izquierda) y el tratamiento (derecha). La Figura 6.2 muestra la relación del número de ataques epiléticos al final del estudio con las posibles covariables. Aquí vemos que no parece haber relación con la edad, hay una relación positiva fuerte con el número de ataques pre-tratamiento, y que los pacientes del grupo placebo parecen presentar un poco más de convulsiones luego de las 8 semanas de tratamiento. 6.3 Preambulo Antes de introducir los GLM, vamos a recordar algunos conceptos sobre distribuciones de probabilidad. 6.3.1 Familia exponencial La distribución de probabilidad de una variable aleatoria \\(Y\\) pertenece a la familia exponencial si la función de densidad (o masa) de \\(Y\\) se puede expresar de la siguiente forma: \\[ f(y;\\theta,\\phi)=\\exp\\{[y\\theta-b(\\theta)]/a(\\phi)+c(y,\\phi)\\}, \\] donde \\(\\theta\\) es llamado parámetro natural, y \\(\\phi&gt;0\\) es un parámetro de dispersión. Además se tiene que: \\[ E[Y]=b&#39;(\\theta) \\quad \\text{y} \\quad V[Y]=b&#39;&#39;(\\theta)a(\\phi). \\] Por ejemplo, la distribución Poisson pertenece a la familia exponencial. Para demostrarlo, la función de densidad de la distribución Poisson: \\[ f(y;\\mu)=\\frac{\\lambda^y\\exp(-\\lambda)}{y!}, \\] se puede reescribir como: \\[ f(y;\\lambda)=\\exp(-\\lambda+y\\log\\lambda-\\log y!). \\] Por lo tanto, la distribución Poisson pertenece a la familia exponencial con \\(\\theta=\\log\\mu\\), \\(b(\\theta)=\\exp(\\theta)\\), \\(a(\\phi)=1\\) y \\(c(y,\\phi)=-\\log y!\\). La distribución normal también pertenece la familia exponencial. Su función de densidad, \\[ f(y;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left[-\\frac{(y-\\mu )^2}{2\\sigma^2}\\right], \\] se puede reescribir como: \\[ f(y;\\mu,\\sigma^2)=\\exp\\left[\\frac{y\\mu-\\frac{1}{2}\\mu^2}{\\sigma^2}-\\frac{1}{2}\\log(2\\pi\\sigma^2)-\\frac{y^2}{2\\sigma^2}\\right], \\] donde \\(\\theta=\\mu\\), \\(b(\\theta)=\\frac{1}{2}\\theta^2\\), \\(a(\\phi)=\\sigma^2\\) y \\(c(y,\\phi)=-\\frac{1}{2}log(2\\pi\\sigma^2)-\\frac{y^2}{2\\sigma^2}\\). Finalmente, también la distribución binomial pertenece a la familia exponencial. Su función densidad, \\[ f(y;\\pi)=\\left(\\begin{array}{c}n\\\\y\\end{array}\\right)\\pi^y(1-\\pi)^{n-y}. \\] se puede reescribir como: \\[ f(y;\\pi)=\\exp\\left\\{y[\\log\\pi-\\log(1-\\pi)]+n\\log(1-\\pi)+\\log\\left(\\begin{array}{c}n\\\\y\\end{array}\\right)\\right\\}, \\] donde \\(\\theta=log\\left(\\frac{\\pi}{1-\\pi}\\right)\\), \\(b(\\theta)=-n\\log\\left[1+\\exp(\\theta)\\right]\\), \\(c(\\phi)=1\\) y \\(d(y,\\phi)=\\log\\left(\\begin{array}{c}n\\\\y\\end{array}\\right)\\). 6.3.2 Relación media-varianza: Para algunas distribuciones de probabilidad hay una relación directa entre la media y la varianza. Por ejemplo, para la distribución binomial tenemos que: \\[ E(Y) = \\mu = n\\pi \\mbox{ y } V(Y) = n\\pi(1-\\pi), \\] por lo que \\(V(Y) = \\nu(\\mu) = \\mu(1-\\mu)/n\\). Para la distribución Poisson, tenemos que \\(V(Y) = \\nu(\\mu) = \\mu\\). Es decir, la varianza es igual a la media. Mientras que, para la distribución normal, la media y la varianza son parámetros independientes. 6.3.3 Estimador de máxima verosimilitud Se asume que \\(Y \\sim f(y,\\boldsymbol \\theta)\\) y estamos interesados en estimar \\(\\boldsymbol \\theta\\). Para ello tomamos una muestra independiente \\((y_{1},y_{2},...,y_{n})\\). Por lo que, la función de verosimilitud está definida como: \\[ L(\\boldsymbol \\theta)=\\prod_{i=1}^nf(y_{i};\\boldsymbol \\theta), \\] y la función de log-verosimilitud es: \\[ \\ell(\\boldsymbol \\theta)=\\sum_{i=1}^n\\log f(y_{i},\\boldsymbol \\theta). \\] El objetivo del estimador por máxima verosimilitud (MLE) es encontrar el \\(\\widehat{\\boldsymbol \\theta}\\) que maximiza \\(L(\\boldsymbol \\theta)\\) (o \\(\\ell(\\boldsymbol \\theta)\\)). Para esto, tenemos que calcular las derivadas de \\(\\ell(\\boldsymbol \\theta)\\) con respecto a cada elemento de \\(\\boldsymbol \\theta\\) y resolver las ecuaciones score: \\[ \\boldsymbol u(\\boldsymbol \\theta)=\\frac{\\partial\\ell(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta}=\\boldsymbol 0, \\] para \\(\\boldsymbol \\theta\\). Es necesario verificar si la solución corresponde a un máximo de \\(\\ell(\\boldsymbol \\theta)\\), evaluando si la matriz de segundas derivadas (matriz hessiana): \\[ \\boldsymbol H(\\boldsymbol \\theta)=\\frac{\\partial^2\\ell(\\boldsymbol \\theta)}{\\partial\\boldsymbol \\theta\\partial\\boldsymbol \\theta^{&#39;}}, \\] evaluada en \\(\\boldsymbol \\theta=\\widehat{\\boldsymbol \\theta}\\), es negativa-definida. 6.3.3.1 Propiedades asintóticas de los MLEs Las propiedades de los MLEs están basadas en teoría asintótica. Es decir, se cumplen cuando el tamaño de muestra es grande \\((n\\to \\infty)\\). \\(\\widehat{\\boldsymbol \\theta}\\) es asintóticamente insesgado. Esto es, \\(E(\\widehat{\\boldsymbol \\theta})=\\boldsymbol \\theta\\) cuando \\(n\\to \\infty\\). La varianza asintótica de \\(\\widehat{\\boldsymbol \\theta}\\) se calcula como la inversa de la matriz de información: \\[ V(\\widehat{\\boldsymbol \\theta})=I(\\boldsymbol \\theta)^{-1},\\quad \\text{donde} \\quad I(\\boldsymbol \\theta)=-E[H(\\boldsymbol \\theta)]. \\] Por lo cual, es MLE es un estimador eficiente. Además, \\(\\widehat{\\boldsymbol \\theta}\\) es asintoticamente normal. Esto es, \\(\\widehat{\\boldsymbol \\theta}\\sim N[\\boldsymbol \\theta,I(\\boldsymbol \\theta)^{-1}]\\). Finalmente, el MLE cumple con la propiedad de invarianza. Si \\(\\widehat{\\boldsymbol \\theta}\\) es el MLE de \\(\\boldsymbol \\theta\\), entonces \\(g(\\widehat{\\boldsymbol \\theta})\\) es el MLE de \\(g(\\boldsymbol \\theta)\\). 6.3.4 Métodos iterativos de maximización En algunos casos no es posible encontrar los MLEs de forma analítica, por lo que debemos hacerlo de forma iterativa. Los métodos que vamos a utilizar están basados en expansiones de series de Taylor. Considere una expansión de series de Taylor de orden 1 para la función score, alrededor de \\(\\boldsymbol \\theta=\\boldsymbol \\theta^{(t)}\\): \\[ \\boldsymbol u\\left(\\boldsymbol \\theta\\right)\\approx \\boldsymbol u\\left(\\boldsymbol \\theta^{(t)}\\right)+\\boldsymbol H\\left(\\boldsymbol \\theta^{(t)}\\right)\\left(\\boldsymbol \\theta-\\boldsymbol \\theta^{(t)}\\right). \\] Igualando la expresión anterior a cero tenemos que: \\[ \\boldsymbol u\\left(\\boldsymbol \\theta^{(t)}\\right)+\\boldsymbol H\\left(\\boldsymbol \\theta^{(t)}\\right)\\left(\\boldsymbol \\theta-\\boldsymbol \\theta^{(t)}\\right)=\\boldsymbol 0, \\] Ahora, encontramos la solución para \\(\\boldsymbol \\theta^{(t+1)}\\) de la siguiente forma: \\[ \\boldsymbol \\theta^{(t+1)}=\\boldsymbol \\theta^{t}-\\boldsymbol H\\left(\\boldsymbol \\theta^{t}\\right)^{-1}\\boldsymbol u\\left(\\boldsymbol \\theta^{t}\\right). \\] Podemos encontrar la estimación de \\(\\boldsymbol \\theta\\) repitiendo los pasos anteriores hasta que se cumpla un criterio de convergencia. Si reemplazamos \\(\\boldsymbol H(\\boldsymbol \\theta)\\) por \\(\\boldsymbol I(\\boldsymbol \\theta)\\), el algoritmo recibe el nombre de Fisher’s scoring. 6.4 Modelo lineal generalizado (GLM) Un GLM está definido por tres componentes: Componente aleatorio: variable respuesta \\(Y\\) y su distribución de probabilidad. Predictor lineal: \\(\\eta=\\boldsymbol x^{&#39;}\\boldsymbol \\beta\\), donde \\(\\boldsymbol \\beta\\) es un vector de parámetros y \\(\\boldsymbol x\\) un vector de covariables. Función de enlace: una función \\(g\\) que conecta la media de la variable respuesta, \\(E(Y)\\), con el predictor lineal, \\(\\eta\\), \\(g[E(Y|\\boldsymbol x)]=\\boldsymbol x&#39;\\boldsymbol \\beta\\). 6.4.1 Componente aleatorio Se asume que se cuenta con \\(n\\) observaciones independientes \\((y_{i},...,y_{n})\\) de una variable aleatoria \\(Y\\) cuya distribución de probabilidad pertenece a la familia exponencial. Restringir un GLM a esta familia permite tener expresiones generales para la función de verosimilitud y funciones score, la distribución asintótica de los estimadores de los parámetros del modelo, y el algoritmo para ajustar el modelo. Por ejemplo, para los datos de los escarabajos podemos asumir que el número de escarabajos que mueren, para cada dosis del químico, sigue una distribución binomial. Mientras que en el caso del ensayo clínico en epilepsia, se podría asumir que el número de ataques epilépticos, condicionado a las covariables, sigue una distribución Poisson. 6.4.2 Predictor lineal Para cada observación \\(i\\), tenemos un conjunto de covariables observadas, \\[ \\boldsymbol x_{i}=(1,x_{i1},...,x_{i,p-1})^{&#39;}, \\] donde \\(x_{ij}\\) es la \\(j\\)-ésima covariable asociada al individuo \\(i\\). Entonces, el predictor lineal está definido como: \\[ \\eta_{i}=\\beta_{0}+\\sum_{j=1}^{p-1}\\beta_{j}x_{ij}=\\boldsymbol x_{i}^{&#39;}\\boldsymbol \\beta. \\] 6.4.3 Función de enlace En algunas distribuciones,\\(E(Y|\\boldsymbol x)\\) está acotada en un intervalo. Por ejemplo, en la distribución binomial, \\(0\\leq \\pi_{i}\\leq 1\\), o en la Poisson, \\(\\lambda_{i}&gt;0\\). Esto hace que no siempre sea razonable asumir una relación lineal entre \\(E(Y | \\boldsymbol x)\\) y \\(\\boldsymbol x\\). Por lo que la función de enlace conecta \\(E(Y|\\boldsymbol x)\\) con el predictor lineal \\(\\eta\\) a través de una función \\(g()\\): \\[ E(Y|\\boldsymbol x)=g^{-1}(\\eta)=g^{-1}(\\boldsymbol x^{&#39;}\\boldsymbol \\beta), \\] donde, la función \\(g(\\cdot)\\) es monótona y diferenciable. Generalmente, \\(g(\\cdot)\\) está determinada por la distribución que se asume para \\(Y\\). 6.4.4 Ejemplos de GLM 6.4.4.1 modelo logístico para la mortalidad de escarabajos Sea: \\[ y_{ij} = \\begin{cases} 1 &amp; \\mbox{ si el escarabajo }j \\mbox{ expuesto a la dosis }i\\mbox{ muere,} \\\\ 0 &amp; \\mbox{ si lo contrario.} \\end{cases} \\] Por lo que, para \\(i\\)-ésima dosis, el número de escarabajos que muere es igual a \\(y^*_i=\\sum_{j=1}^{n_i} y_{ij}\\). Además, asumiendo independencia, podemos decir que: \\[ y^*_i \\sim \\mbox{binomial} (n_{i},\\pi_{i}), i=1,\\ldots,n_i, \\] donde \\(0 &lt; \\pi_i &lt; 1\\) depende de la dosis. Definiendo \\(y_{i}=y^*_i/n_{i}\\) (la proporción de escarabajos muertos para la dosis \\(i\\)), tenemos que: \\[ E(y_{i}| \\mbox{dosis}_{i}) = \\pi_{i} \\mbox{ y }V(y_{i}| \\mbox{dosis}_{i}) = \\pi_{i}(1-\\pi_{i})/n_{i}. \\] Dado que \\(0 \\leq \\pi_i \\leq 1\\), se podría proponer que: \\[ \\pi_{i} = g^{-1}(\\mbox{dosis},\\boldsymbol \\beta) = \\frac{\\exp(\\beta_0 + \\log \\ \\mbox{dosis}_i\\beta_1)}{1+\\exp(\\beta_0 + \\log \\ \\mbox{dosis}_i\\beta_1)} =\\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}. \\] Por lo que \\(g(\\pi_{i}) = \\log \\left( \\frac{\\pi_{i}}{1-\\pi_{i}}\\right) = \\eta_i\\). Esta función recibe el nombre de función logit y su forma la podemos ver gráficamente en la Figura 6.3. Como vemos, está función está acotada entre \\(0\\) y \\(1\\), garantizando que las probabilidades estimadas siempre estén en este rango, y tiene forma de ‘S’. Figure 6.3: Función logística 6.4.4.2 modelo Poisson para el número de ataque epilépticos Definiendo \\(y_i\\) como el número de ataques epilépticos (\\(\\times\\) dos semanas) del paciente \\(i\\) luego de 8 semanas de tratamiento, podemos suponer que: \\[ y_{i}\\sim \\mbox{Poisson}(\\lambda_{i}), \\quad i=1,...,n,\\quad \\text{donde} \\quad \\lambda_{i}=g(\\eta_i), \\] donde \\(\\eta_i = \\beta_{0}+\\beta_{1}\\text{treat}_{i}+\\beta_{2}\\text{base}_{i}+\\beta_{3}\\text{base}_{i}\\text{treat}_{i}.\\) Entonces, \\(E(y_{i}|\\boldsymbol x_{i})=V(y_{i}|\\boldsymbol x_{i})=\\lambda_{i}\\). Dado que \\(\\lambda_ i &gt; 0\\), se puede asumir que: \\[ \\lambda_{i}=g^{-1}(\\eta_i)=\\exp(\\eta_i). \\] Por lo que, \\(g(\\lambda_{i})=\\log\\lambda_{i}\\). Este modelo puede ser una opción para estimar el número medio de ataques epiléticos de los pacientes del ensayo clínico en epilepsia. 6.4.4.3 Otros ejemplos Un modelo lineal es un caso particular del GLM. Aquí asumimos que \\(y_i \\sim N(\\mu_i, \\sigma^2)\\), con \\(\\mu_i = \\boldsymbol x_i&#39;\\boldsymbol \\beta\\). Es decir, la función de enlace es la identidad. Otros ejemplos de distribuciones que se pueden utilizar son: Modelo binomial-negativo para conteo con sobredispersión. Modelo beta-binomial para ensayos Bernoulli correlacionados (sobredispersión). Modelo multinomial para variables nominales (ordinales) con más de dos categorías. Modelo beta para proporciones. Modelo gamma para variables continuas asimétricas y con varianza creciente. Modelo Weibull para tiempos de falla. 6.5 Ajuste de un GLM Al igual que en los modelos lineales, los pasos para ajustar un GLM incluyen: Especificación del modelo.Definición del componente aleatorio, predictor lineal y función de enlace. Estimación de los parámetros. Para los GLM, la estimación se hace por máxima verosimilitud. Evaluación del modelo. Verificar si el modelo estimado se ajusta bien a los datos. Inferencia. cálculo de intervalos de confianza, pruebas de hipótesis e interpretación de los resultados según los objetivos del estudio. 6.5.1 Especificación del modelo El GLM asume que: \\[ f(y_{i};\\theta_{i},\\phi)=exp\\left\\{\\frac{y_{i}\\theta_{i}-b(\\theta_{i})}{a(\\phi)} +c(y_{i},\\phi)\\right\\}, \\] donde \\(\\phi&gt;0\\). Además, \\(E(y_{i})=b&#39;(\\theta_{i})\\) y \\(V(y_{i})=b&#39;&#39;(\\theta_{i})a(\\phi).\\) El predictor lineal \\(\\eta_{i}=\\boldsymbol x&#39;_{i}\\boldsymbol \\beta\\) se relaciona con \\(\\mu_{i}\\) a través la función de enlace, \\(\\eta_{i}=g(\\mu_{i})\\). La función de enlace \\(g(\\cdot)\\) que transforma \\(\\mu_{i}\\) en el parámetro natural \\(\\theta_{i}\\) es llamada función de enlace canónica. Por ejemplo para la distribución normal, \\(\\eta_{i}=\\mu_{i}\\) (identidad), por lo tanto \\(\\mu_{i}=\\eta_{i}\\). Para la distribución Poisson, \\(\\eta_{i}=\\log\\mu_{i}\\), por lo tanto \\(\\mu_{i}=\\exp(\\eta_{i})\\). Para la distribución binomial, \\(\\eta_{i}=\\log\\left(\\frac{\\pi_{i}}{1-\\pi_{i}}\\right)\\), por lo tanto \\(\\pi_{i}=\\frac{\\exp(\\eta_{i})}{1+\\exp(\\eta_{i})}\\). 6.5.2 Estimador por máxima verosimilitud para \\(\\boldsymbol \\beta\\) dado que asumimos que la distribución de probabilidad asociada a la variable resputa pertenece a la familia exponencial, la función de log-verosimilitud está definida como: \\[ \\ell(\\boldsymbol \\beta)=\\sum_{i=1}^{n}\\ell_{i}(\\boldsymbol \\beta)=\\sum_{i=1}^{n}\\log f(y_{i};\\theta_{i},\\phi)=\\sum_{i=1}^{n}\\left[\\frac{y_{i}\\theta_{i}-b(\\theta_{i})}{a(\\phi)}+c(y_{i},\\phi)\\right]. \\] Las funciones score están definidas como: \\[ u(\\beta_{j})=\\sum_{i=1}^{n}\\frac{(y_{i}-\\mu_{i})x_{ij}}{V(y_{i})}\\frac{\\partial\\mu_{i}}{\\partial\\eta_{i}}=0, \\quad \\text{para} \\quad j=0,...,p-1, \\] donde \\(\\eta_{i}=\\boldsymbol x_{i}&#39;\\boldsymbol \\beta=g(\\mu_{i})\\). Dado que las funciones score son no-lineales, es necesario estimar \\(\\boldsymbol \\beta\\) iterativamente usando el algoritmo de Newton-Raphson. En forma matricial, tenemos que la función score es: \\[ \\b(\\boldsymbol \\beta)=\\boldsymbol X&#39;\\boldsymbol D\\boldsymbol V^{-1}(\\boldsymbol y-\\boldsymbol \\mu), \\] la función hessiana es: \\[ \\boldsymbol H(\\boldsymbol \\beta)=\\boldsymbol I(\\boldsymbol \\beta)=\\boldsymbol X&#39;\\boldsymbol W\\boldsymbol X, \\] donde \\(\\boldsymbol V\\) es una matriz diagonal con valores \\(v_{ii}=V(y_{i})\\) en la diagonal, \\(\\boldsymbol D\\) es una matriz diagonal con valores \\(d_{ii}=\\frac{\\partial\\mu_{i}}{\\partial\\eta_{i}}\\),y \\(\\boldsymbol W\\) es una matriz diagonal con \\(w_{ii}=\\frac{(\\partial\\mu_{i}/\\partial\\eta_{i})^2}{V(y_{i})}\\). A través del algoritmo de Newton-Raphson (o Fisher’s scoring) podemos encontrar la estimación de \\(\\boldsymbol \\beta\\) de forma iterativa: \\[ \\boldsymbol \\beta^{(t+1)}=\\boldsymbol \\beta^{(t)}+[\\boldsymbol I(\\boldsymbol \\beta^{(t)})]^{-1}\\boldsymbol u(\\boldsymbol \\beta^{(t)}). \\] Equivalentemente, podemos utilizar el método de mínimos cuadrados iterativamente reponderados: \\[ \\boldsymbol \\beta^{(t+1)}=(\\boldsymbol X&#39;\\boldsymbol W^{(t)}\\boldsymbol X)^{-1}\\boldsymbol X&#39;\\boldsymbol W^{(t)}\\boldsymbol z^{(t)}, \\] donde, los elementos de \\(\\boldsymbol z^{(t)}\\) son: \\[ z_{i}^{(t)}=\\boldsymbol x_{i}&#39;\\boldsymbol \\beta^{(t)}+(y_{i}-\\mu_{i}^{(t)})\\frac{\\partial\\eta_{i}^{(t)}}{\\partial \\mu_{i}^{(t)}}. \\] La distribución asintótica del MLE de \\(\\boldsymbol \\beta\\) es: \\[ \\widehat{\\boldsymbol \\beta}\\sim N\\left[\\boldsymbol \\beta,(\\boldsymbol X&#39;\\boldsymbol W\\boldsymbol X)^{-1}\\right]. \\] #### ajuste e interpretación de parámetros del modelo logístico Para los datos de mortalidad de escarabajos, tenemos el siguiente modelo: \\[ y_i^* \\sim \\mbox{binomial}(n_i,\\pi_i), \\mbox{ donde } \\pi_{i}=\\frac{\\exp(\\beta_0+\\beta_1\\log\\ \\text{dosis}_i)}{1+\\exp(\\beta_0+\\beta_0\\log\\ \\text{dosis}_i)}. \\] La estimación de los parámetros del GLM por MLE se puede hacer utilizando la función glm(). En esta función debemos determinar la distribución de probabilidad asociada a los datos y la función de enlace con el argumento family. Para el modelo logístico, usamos family=binomial (por defecto la función de enlace es logit): modBin = glm(cbind(dead,n-dead)~logdose,family=binomial) summary(modBin) ## ## Call: ## glm(formula = cbind(dead, n - dead) ~ logdose, family = binomial) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -60.717 5.181 -11.72 &lt;2e-16 *** ## logdose 34.270 2.912 11.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 284.202 on 7 degrees of freedom ## Residual deviance: 11.232 on 6 degrees of freedom ## AIC: 41.43 ## ## Number of Fisher Scoring iterations: 4 Aquí vemos que el efecto del logarítmo de la dosis es positivo y significativo. Es decir que, un incremento en la dosis significa un aumento en la probabidad de que el escarabajo muera. La relación la podemos ver de forma gráfica en la figura 6.4. pred.x = data.frame(logdose = seq(min(logdose),max(logdose),length.out = 50)) pred = predict(modBin,pred.x,type=&#39;response&#39;) plot(logdose,dead/n,xlab=&#39;log dosis de disulfuro de carbono gaseoso&#39;,ylab=&#39;proporción de escarabajos muertos&#39;,ylim=c(0,1)) lines(pred.x$logdose,pred,col=2,lwd=2) Figure 6.4: Datos de mortalidad de escarabajos. Estimación de la probabilidad de que el escarabajo muera en función del logarítmo de la dosis de disulfuro de carbono gaseoso. La interpretación de los coeficientes del modelo logístico se hacen a través de los odds ratio. Primero, un odd está definido como: \\[ \\mbox{odd}=\\frac{P(Y=1)}{P(Y=0)}. \\] Es decir, es una razón entre la probabilidad de éxito sobre la probabilidad de fracaso. Por ejemplo, si el odd es igual a dos, estaríamos diciendo que la probabilidad de éxito es el doble de la probabilidad de fracaso. Para el caso del modelo logístico simple, tenemos que: \\[ \\text{odd}(x)=\\frac{P(Y=1|x)}{P(Y=0|x)}= \\frac{\\frac{\\exp(\\beta_0+x\\beta_1)}{1+\\exp(\\beta_0+x\\beta_1)}}{1-\\frac{\\exp(\\beta_0+x\\beta_1)}{1+\\exp(\\beta_0+x\\beta_1)}} = \\exp(\\beta_0+x\\beta_1). \\] Ahora, los odds ratio están definido como \\(\\text{OR}=\\frac{\\text{odd}(x=a+1)}{\\text{odd}(x=a)}\\). Para el caso del modelo logístico simple temos que el OR es: \\[ \\mbox{OR}(x)=\\frac{\\text{odd}(x=a+1)}{\\text{odd}(x=a)}=\\frac{\\exp\\left[\\beta_{0}+(a+1)\\beta_{1}\\right]}{\\exp(\\beta_0+a\\beta_1)}=\\exp(\\beta_1). \\] Por lo tanto, por cada cambio unitario en \\(x\\), los odds de morir incrementan por un factor de \\(exp(\\beta_1)\\). Por un cambio en \\(x\\) de \\(a\\) a \\(a+\\delta\\), tenemos que \\(OR=\\exp(\\delta\\beta_1)\\). Para los datos de los escarabajos un cambio unitario en el log dosis es muy grande, por lo que podríamos interpetra \\(\\beta_1\\) usando un aumento en la log dosis mas pequeño, por ejemplo de \\(\\delta=0.02\\): \\[ \\exp ( 0.01\\times 34.270) = 1.984. 5. \\] Es decir, si el log de la concentración de aumenta en \\(0.02\\), entonces la probabilidad de que el escarabajo muera aumenta casi el doble. 6.5.2.1 ajuste e interpretación de parámetros del modelo Poisson Para el ensayo clínico en epilepsia, el modelo propuesto es: \\[ y_i \\sim \\mbox{Poisson}(\\lambda_i), \\mbox{ donde } \\lambda_i=\\exp(\\beta_0+\\beta_1\\text{treat}_i+\\beta_2\\text{base}_i+\\beta_2\\text{base}_i\\text{treat}_i). \\] Para el ajuste del modelo usamos la función glm() con el argumento family=poisson: modPois=glm(seizure.rate~treatment+base+base:treatment,family=poisson, data =epilepsy4) summary(modPois) ## ## Call: ## glm(formula = seizure.rate ~ treatment + base + base:treatment, ## family = poisson, data = epilepsy4) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.2453159 0.1176157 10.588 &lt;2e-16 *** ## treatmentProgabide -0.3635857 0.1624995 -2.237 0.0253 * ## base 0.0209356 0.0019144 10.936 &lt;2e-16 *** ## treatmentProgabide:base 0.0008523 0.0022778 0.374 0.7083 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 476.25 on 58 degrees of freedom ## Residual deviance: 149.54 on 55 degrees of freedom ## AIC: 345.3 ## ## Number of Fisher Scoring iterations: 5 Los resultados del ajuste muestran que el número de ataques pre-tratamiento tiene un efecto positivo sobre la media del número de ataques epilépticos. Es decir, esta aumenta entre más ataques previos tenga el paciente. Mientras que, el tratamiento parece tener un efecto negativo. Es decir, el prograbide reduce la media del número de ataques epilépticos con respecto al placebo. Finalmente, el efecto interacción es muy pequeño sugiriendo que el efecto del tratamiento no se ve afectado por la frecuencia de los ataques epilépticos que tiene el paciente. La estimación de la media se puede observa de forma gráfica en la Figura 6.5. y = data.frame(base=seq(min(epilepsy4$base),max(epilepsy4$base),length.out=100), treatment = factor(1:2, levels = 1:2, labels = levels(epilepsy4$treatment))) predpois = predict(modPois,y,type=&#39;response&#39;) x=seq(min(epilepsy4$base),max(epilepsy4$base),length.out=50) plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy4, xlab=&quot;ataques pre-tratamiento (x 4 semanas)&quot;, ylab = &quot;ataques post-tratamiento (x 2 semanas)&quot;) lines(x,predpois[order(y$treatment, y$base)[1:50]], lwd = 2) lines(x,predpois[order(y$treatment, y$base)[51:100]],col=2, lwd = 2) Figure 6.5: Datos de epilepsia. Estimación de la media de ataques epilépticos (por dos semanas) luego de cuatro semanas de tratamiento. La línea negra representa la media de ataques para el grupo placebo, mientras que, la linea roja la media de ataques para el tratamiento con Progabide. En el modelo Poisson simple, tenemos que \\(E(Y|x)=\\exp(\\beta_0+x\\beta_1)\\). Si aumentamos a \\(x\\) en \\(\\delta\\) unidades, tenemos que \\(E(Y|x+\\delta)=\\exp(\\beta_0+(x+\\delta)\\beta_1)\\). Ahora si calculamos el logaritmo de la razón de los valores esperados obtenemos que: \\[ \\log\\left[\\frac{E(Y|x=a+\\delta)}{E(Y|x=a)}\\right]=\\delta\\beta_1. \\] Por lo tanto, \\(\\exp(\\delta\\beta_1)=\\frac{E(Y|x=a+\\delta)}{E(Y|x=a)}\\). Es decir que, \\(\\exp(\\delta\\beta_1)\\) es una tasa de crecimiento del valor esperado de \\(Y\\) por un aumento en \\(x\\) de \\(\\delta\\) unidades. Por ejemplo, si el número de ataques epilépticos pre-tratamiento aumenta \\(20\\) casos, entonces el valor esperado de ataques epilépticos post-tramiento aumenta un 52% \\((\\exp(20 \\times0.021)=1.522)\\). Además, el tramiento con Progabida reduce en un poco más del 40% el número de episodios de epilepsia \\((\\exp(-0.3635857) = 0.695)\\). Finalmente, el efecto de los casos pre-tramiento es el mismo para los pacientes en el grupo tratamiento y el control (\\(\\exp(0.0009) \\approx 1\\)). 6.6 Pruebas de hipótesis Al igual que en los modelos lineales, uno puede estar interesado en realizar pruebas hipótesis sobre los coeficientes del modelo. Por ejemplo, en el estudio sobre epilépsia estamos interesados en evaluar si hay un efecto significativo del tratamiento sobre los ataques epilépticos. Esto es: \\[ \\qquad H_0:\\beta_1=\\beta_3=0. \\] Si rechazamos \\(H_0\\) podemos concluir que el tratamiento con Progabida tiene un efecto sobre el número de ataques epilépticos. Particularmente, si \\(\\beta_1 &lt; 0\\), este fármaco reduce los episodios. Suponga que tenemos un modelo con el siguiente predictor lineal: \\[ \\eta = \\boldsymbol x_{1}&#39;\\boldsymbol \\beta_1+\\boldsymbol x_{2}&#39;\\boldsymbol \\beta_2, \\] donde \\(\\boldsymbol x_{1 }= (1,x_{11},\\ldots,x_{1,q-1})&#39;\\) y \\(\\boldsymbol x_{2}= (1,x_{21},\\ldots,x_{2,p-q})&#39;\\) son vectores de covariables; \\(\\boldsymbol \\beta_1\\) y \\(\\boldsymbol \\beta_2\\) son vectores de coeficientes de dimensiones \\(q\\) y \\(p-q\\), respetivamente. Ahora, planteamos el siguiente sistema de hipótesis: \\[ \\qquad H_0:\\boldsymbol \\beta_2=\\boldsymbol 0\\qquad H_1:\\boldsymbol \\beta_2 \\ne \\boldsymbol 0. \\] Para estimadores basados en verosimilitud, podemos utilizar tres métodos diferentes: la prueba de razón de verosimilitud, la prueba del score o la prueba de Wald. 6.6.1 Método de razón de verosimilitud La idea de este método es comparar la verosimilitud bajo dos condiciones: (\\(L_0\\)) la máxima alcanzada bajo \\(H_0\\) (es decir, asumiendo que \\(\\boldsymbol \\beta_2=\\boldsymbol 0\\)) y (\\(L_1\\)) la evaluada en la estimación maximo verosimil, es decir \\(L(\\widehat{\\boldsymbol \\beta})\\). El estadístico de prueba usado es: \\[ LR = 2\\log \\left( \\frac{L(\\widehat{\\boldsymbol \\beta})}{L(\\widehat{\\boldsymbol \\beta}_0)} \\right) = 2 (\\ell_1 - \\ell_0). \\] Asumiendo que \\(H_0\\) es cierta, asintóticamente tenemos que \\(LR \\sim \\chi^{2}_{p-q}\\). Por lo que, rechazamos \\(H_0\\) con un nivel de significancia de \\(\\alpha\\), si \\(LR &gt; \\chi^{2}_{1-\\alpha,p-q}\\). 6.6.2 Método de Wald Asintóticamente, tenemos que \\(\\widehat{\\boldsymbol \\beta}\\sim N[\\boldsymbol \\beta,\\boldsymbol I(\\boldsymbol \\beta)^{-1}]\\). por lo tanto se tiene que: \\[ (\\hat{\\boldsymbol \\beta}-\\boldsymbol \\beta)&#39;\\boldsymbol I(\\boldsymbol \\beta)(\\hat{\\boldsymbol \\beta}-\\boldsymbol \\beta)\\sim\\chi^2_{p}. \\] Entonces, para probar \\(H_0: \\boldsymbol \\beta_2 = \\boldsymbol \\beta_2^{0}\\), se puede utilizar el siguiente estadístico de prueba: \\[ W = \\left(\\widehat{\\boldsymbol \\beta}_2-\\boldsymbol \\beta_2^{0}\\right)&#39;\\left[V\\left(\\widehat{\\boldsymbol \\beta}_2\\right)\\right]^{-1}\\left(\\widehat{\\boldsymbol \\beta}_2-\\boldsymbol \\beta_2^0\\right). \\] Si \\(H_0\\) es cierta, entonces \\(W \\sim \\chi^2_{p-q}\\). Por lo que, rechazamos \\(H_0\\) con un nivel de significancia de \\(\\alpha\\), si \\(W &gt; \\chi^{2}_{1-\\alpha,p-q}\\). 6.6.2.1 Ejemplo ataques epilépticos En este caso, podríamos plantear el siguiente sistema de hipótesis: \\[ \\qquad H_0:\\beta_1=\\beta_3=0 \\qquad H_1: \\beta_1=0 \\mbox{ o } \\beta_3=0. \\] Es decir, \\(H_0\\) indica que no hay ningún efecto del tratamiento para la epilépsia (no es estadísticamente diferente al placebo). La prueba de razón de verosimilitud se puede implementar de las siguiente forma: modPois0=glm(seizure.rate~base,family=poisson, data =epilepsy4) anova(modPois0,modPois,test=&#39;LRT&#39;) ## Analysis of Deviance Table ## ## Model 1: seizure.rate ~ base ## Model 2: seizure.rate ~ treatment + base + base:treatment ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 57 159.94 ## 2 55 149.54 2 10.405 0.005502 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Este resultado, complementado con los anteriores, sugiere que hay un efecto del tratamiento. Es decir, el Progabida tiene un efecto significativo para reducir los ataques epilépticos. La prueba de Wald la podemos implementar de la siguiente forma: library(aod) VarMat = vcov(modPois) Coef = coefficients(modPois) wald.test(VarMat,Coef,Terms=c(2,4)) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 10.3, df = 2, P(&gt; X2) = 0.0057 Estos resultados son similares a los observados con la prueba de razón de verosimilitud y llegamos a la misma conclusión. 6.7 intervalos de confianza A partir de los estadísticos de prueba anteriores se pueden encontrar intervalos de confianza para \\(\\boldsymbol \\beta\\). Por ejemplo usando el estadístico de Wald: \\[ \\widehat{\\beta}_j\\pm z_{\\alpha/2}\\sqrt{V(\\widehat{\\beta}_j)}. \\] Otra alternativa es a partir de los perfiles de log-verosimilitud, el intervalo del \\((1-\\alpha)\\)% de confianza para \\(\\beta_j\\) está definido a partir de los valores de \\(\\beta^0_j\\) que satisfacen: \\[ -2[\\ell(\\widehat{\\boldsymbol \\beta})-\\ell(\\widehat{\\boldsymbol \\beta}^{0})]&lt;\\chi^2_{1-\\alpha,1}, \\] donde \\(\\ell(\\widehat{\\boldsymbol \\beta}^{0})\\) es la log-verosimilitud donde \\(\\beta_j\\) está restringido a los valores de \\(\\beta_j^0\\). 6.7.1 Intervalo de confianza para la media Dado que \\(\\widehat{\\eta}_0=\\boldsymbol x_0&#39;\\widehat{\\boldsymbol \\beta}\\) ,por lo tanto (asintóticamente): \\[ \\widehat{\\eta}_0\\sim N[\\boldsymbol x_0&#39;\\boldsymbol \\beta,\\boldsymbol x_0&#39;\\boldsymbol I(\\boldsymbol \\beta)^{-1}\\boldsymbol x_0]. \\] Entonces un intervalo de confianza para \\(\\eta_0\\) es: \\[ \\widehat{\\eta}_0\\pm z_{\\alpha/2}\\sqrt{\\boldsymbol x_0&#39;I(\\widehat{\\boldsymbol \\beta})^{-1}\\boldsymbol x_0}. \\] Para encontrar el intervalo de confianza para \\(\\mu_i\\), se hace la transformación \\(g^{-1}\\) a los límites de confianza. 6.7.1.1 Ejemplo mortalidad de escarabajos La estimación de la probabilidad de que el escarabajo muera se puede observar en la Figura 6.6. pred.link = predict(modBin,newdata = pred.x,type=&#39;link&#39;,se.fit = T) lim.sup = faraway::ilogit(pred.link$fit + qnorm(0.975)*pred.link$se.fit) lim.inf = faraway::ilogit(pred.link$fit - qnorm(0.975)*pred.link$se.fit) plot(logdose,dead/n,xlab=&#39;log dosis&#39;,ylab=&#39;proporción de escarabajos muertos&#39;,ylim=c(0,1)) lines(pred.x$logdose,pred) lines(pred.x$logdose,lim.sup, col=2) lines(pred.x$logdose,lim.inf, col=2) Figure 6.6: Datos de mortalidad de escarabajos. Intervalo del 95% de confianza para la probabilidad de que el escarabajo muera. 6.7.1.2 Ataques epilépticos Los intervalos del 95% de confianza para los parámetros del modelo son: confint(modPois) ## 2.5 % 97.5 % ## (Intercept) 1.009325667 1.470661654 ## treatmentProgabide -0.682237759 -0.044653101 ## base 0.017137251 0.024649428 ## treatmentProgabide:base -0.003594654 0.005341111 Aquí vemos que el efecto del tratamiento está entre \\(-0.4\\) y \\(-0.09\\). Es decir, que la reducción del número de ataques epilépticos está entre el 10% y el 49%. Mientras que, como ya se mencionó antes, el efecto interacción no tiene un aporte significativo. La media del número de ataques epiléptico por dos semanas se puede observar en la Figura 6.7. Se puede observar que las estimaciones de la media para el grupo tratamiento es inferior que para el grupo control. predx1 = data.frame(base=seq(6,151,length.out = 100),treatment=&#39;placebo&#39;) predx2 = data.frame(base=seq(6,151,length.out = 100),treatment=&#39;Progabide&#39;) predy1 = predict(modPois,newdata = predx1,type=&#39;response&#39;) predy2 = predict(modPois,newdata = predx2,type=&#39;response&#39;) predy1Link = predict(modPois,newdata = predx1,type=&#39;link&#39;,se.fit = T) predy2Link = predict(modPois,newdata = predx2,type=&#39;link&#39;,se.fit = T) lwr1 = exp(predy1Link$fit - qnorm(0.975)*predy1Link$se.fit) upr1 = exp(predy1Link$fit + qnorm(0.975)*predy1Link$se.fit) lwr2 = exp(predy2Link$fit - qnorm(0.975)*predy2Link$se.fit) upr2 = exp(predy2Link$fit + qnorm(0.975)*predy2Link$se.fit) plot(epilepsy4$base,epilepsy4$seizure.rate, ylab=&#39;# ataques epilépticos (4ta semana)&#39;, xlab=&#39;# ataques epilépticos (pre tratamiento)&#39;, col=as.double(epilepsy4$treatment)) lines(predx1$base,predy1,lwd=2) lines(predx1$base,predy2,col=2,lwd=2) lines(predx1$base,lwr1,lty=2) lines(predx1$base,upr1,lty=2) lines(predx2$base,lwr2,lty=2,col=2) lines(predx2$base,upr2,lty=2,col=2) Figure 6.7: Datos de epilépsia. Estimación de la media de ataques epilépticos por dos semanas en el grupo placebo (línea negra) y el grupo tratamiento (línea roja). 6.8 Devianza Considere un GLM con observaciones \\(\\boldsymbol y_i=(y_1,...,y_n)\\). Sea \\(ℓ(\\widehat{\\boldsymbol \\mu})\\) la log-verosimilitud (expresada en función de \\(\\boldsymbol \\mu\\)) evaluada en el MLE. La máxima verosimilitud que se puede alcanzar corresponde a \\(ℓ(\\boldsymbol y)\\) (un modelo con ajuste perfecto \\(y_i=\\mu_i\\)). A este último se le conoce como el modelo saturado. La Devianza compara el modelo propuesto contra el modelo saturado de la siguiente manera: \\[\\begin{equation} \\begin{split} D &amp;=- 2 \\log \\left[ \\frac{L(\\widehat{\\boldsymbol \\mu})}{L(\\boldsymbol y)} \\right] = -2[\\ell(\\hat{\\boldsymbol \\mu})-\\ell(\\boldsymbol y)] \\\\ &amp;= 2\\sum_{i=1}^n[y_i\\widetilde{\\theta}_i-b(\\widetilde{\\theta}_i)]/a(\\phi) - \\sum_{i=1}^n[y_i\\widehat{\\theta_i}-b(\\widehat{\\theta_i})]/a(\\phi), \\end{split} \\end{equation}\\] donde \\(\\widetilde{\\theta}_i\\) corresponde a la estimación de \\(\\theta_i\\) en el modelo saturado \\((\\widetilde{\\mu}_i=y_i)\\). Dado que, \\(\\ell(\\boldsymbol y) \\geq \\ell(\\hat{\\boldsymbol \\mu})\\), entonces \\(D \\geq 0\\). Por lo que, si el ajuste propuesto es pobre, se espera que \\(D\\) sea grande. Modelo binomial: Para el modelo binomial, la devianza está determinada por: \\[ D=2\\sum_{i=1}^n\\left[y_i\\log \\left(\\frac{y_i}{\\widehat{\\mu}_i}\\right)+(n_i-y_i)\\log\\left(\\frac{n_i-y_i}{n_i-\\widehat{\\mu}_i}\\right)\\right], \\] donde \\(\\widehat{\\mu}_i=n_i\\widehat{\\pi}_i.\\) Modelo Poisson: Para el modelo Poisson, la devianza está definida como: \\[ D=2\\sum_{i=1}^ny_i\\log\\left(\\frac{y_i}{\\widehat{\\mu}_i}\\right), \\quad \\text{donde} \\quad \\widehat{\\mu}_i=\\widehat{\\lambda}_i. \\] Modelo normal: Mientras que, para el modelo lineal es: \\[ D=2\\sum_{i=1}^n\\left(y_i-\\widehat{\\mu}_i\\right)^2. \\] 6.8.1 El estadístico chi-cuadrado de Pearson Otro indicador de calidad del modelo es el estadístico chi-cuadrado de Pearson, definido como: \\[ X^2=\\sum_{i=1}^n\\frac{\\left(y_i-\\widehat{\\mu}_i\\right)^2}{v(\\widehat{\\mu}_i)}. \\] Para el modelo binomial, tenemos que: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-n_i\\hat{\\pi}_i)^2}{n_i\\hat{\\pi}_i(1-\\hat{\\pi})}. \\] Para el modelo Poisson es: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-\\widehat{\\mu}_i)^2}{\\widehat{\\mu}_i}. \\] 6.9 Bondad de ajuste Tanto \\(D\\) como \\(X^2\\) se pueden utilizar para evaluar la bondad del ajuste. Por lo que se podría plantear las siguientes hipótesis: \\(H_0\\) indica que el modelo ajusta bien a los datos, y \\(H_1\\) lo contrario. Por ejemplo, en el caso de un modelo logístico: \\[ H_0:\\mbox{logit}(\\pi_i)=\\x_i\\boldsymbol \\beta. \\] Si \\(H_0\\) es cierta, entonces \\(D\\) y \\(X_2\\) siguen una distribución \\(\\chi_2\\) con \\((n - p)\\) grados de libertadad. Sin embargo, está aproximación es buena solo si se cuenta con datos agrupados. Mortalidad de escarabajos Para el modelo de mortalidad de escarabajos, los valores de la devianza y el \\(\\chi^2\\) de Pearson son: # devianza D = deviance(modBin) 1-pchisq(D,6) # valor p ## [1] 0.08145881 # chi-cuadrado de Pearson X2 = sum(residuals(modBin,type=&#39;pearson&#39;)^2) 1-pchisq(X2,6) # valor p ## [1] 0.1235272 Los datos de epilepsia no están agrupados, por lo tanto esta prueba de bondad de ajuste no se puede utilizar. 6.10 Selección de variables La selección de variables se puede realizar de la misma forma que en los LMs basándose en indicadores como el AIC y BIC (o alguna modificación de ellos). El primero está definido como: \\[ \\text{AIC}=-2\\ell_M+2p, \\] donde \\(\\ell_M\\) es la verosimilitud alcanzada con el modelo propuesto y \\(p\\) el número de parámetros. El segundo indicador como: \\[ \\text{BIC}=-2\\ell_M+p\\log(n). \\] En ambos casos, entre mas pequeños sean estos indicadores mejor es el modelo. "],["modelo-logístico.html", "Capítulo 7 Modelo logístico 7.1 Casos de estudio 7.2 Datos agrupados o datos no agrupados 7.3 Funciones de enlace 7.4 Curva característica operativa del receptor(ROC) 7.5 Sobredispersión 7.6 Distribución beta-binomial", " Capítulo 7 Modelo logístico 7.1 Casos de estudio 7.1.1 Mortalidad de escarabajos Vamos a continuar con los datos del estudio sobre los escarabajos del capítulo anterior. logdose &lt;- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839) dead &lt;- c(6, 13, 18, 28, 52, 53, 61, 60) n &lt;- c(59, 60, 62, 56, 63, 59, 62, 60) Datos.esc=data.frame(logdose,n,dead) 7.1.2 Bajo peso al nacer Con los datos de la librería , se quiere indentificar factores de riesgo asociados con el nacimiento de niños con bajo peso (es decir, un peso al nacer menor de \\(2.5\\)kgs). Se tiene información de 189 recién nacidos y sus madres atendidos en una hospital (Baystate Medical Center, Springfield, Mass en 1986). Los posibles factores que pueden afectar el bajo al peso son: age: edad de la madre (años), lwt: peso de la madre antes del embarazo (libras), smoke: estado de tabaquismo durante el embarazo (0 no, 1 si), ptl: historia de parto prematuro (número de casos). La variable de respuesta es si el bebé nace con bajo peso (1 si peso \\(&lt;\\) 2.5 kgs, 0 si peso \\(\\geq\\) 2.5 kgs, low). Para este caso, se puede ajustar modelo logístico. Sea \\(y_i=1\\) si el \\(i\\)-ésimo bebé nace con bajo peso, \\(y_i=0\\) si lo contrario. Por lo que la densidad de \\(y_i\\) está definida como: \\[ f(y_i;\\pi_i)=\\pi_i^{y_i}(1-\\pi_i)^{(1-y_i)}, \\] donde \\(\\pi_i\\) es la probabilidad de que el \\(i\\)-ésimo bebé nazca con bajo peso. La función de enlace puede definirse como: \\[ \\log\\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\beta_0+\\beta_1\\text{age}_i+\\beta_2\\text{lwt}_i+\\beta_3\\text{smoke}_i+\\beta_4\\text{ptl}_i. \\] Sin embargo, se puede considerar alguna función de enlace diferente, como veremos adelante. 7.1.3 Estudio de teratología Los datos de la librería son de un estudio para investigar los efectos de unos régimenes alimenticio en el desarrollo fetal de ratas. En este experimento, 58 ratas hembras con dietas deficientes en hierro se dividieron en cuatro grupos. Tres grupos recibieron inyecciones semanales de suplementos de hierro, a diferentes dosis, para normalizar sus niveles de hierro. Mientras que, el grupo restante no recibieron ningún suplemento (placebo). Luego, las ratas fueron preñadas, sacrificadas luego de 3 semanas, y se contó el número de fetos muertos en cada camada. En la Figura 7.1 se puede observar la relación entre el número de fetos muertos por tratamiento, y también, la relación con los niveles de hemoglobina de las ratas hembras. data(lirat,package = &#39;VGAM&#39;) par(mfrow=c(1,2)) #Proporcion de fetos muestos vs tratamiento plot(lirat$grp,lirat$R/lirat$N,xlab=&quot;Tratamiento&quot;,ylab=&quot;Proporción de fetos muertos&quot;) #Proporcion de fetos muestos vs Hemoglobina plot(lirat$hb,lirat$R/lirat$N,xlab=&quot;Hemoglobina&quot;,ylab=&quot;Proporción de fetos muertos&quot;) Figure 7.1: Datos de teratoligía. Relación entre la proporción de fetos muertos por camada con los tratamientos (izquierda) y los niveles de hemoglobina de la madra (derecha) Si consideramos: \\[ y_{ij} = \\begin{cases} 1 &amp; \\mbox{ si el feto } j \\mbox{ de la camada }i\\mbox{ está muerto}, \\\\ 0 &amp; \\mbox{ si lo contrario}, \\\\ \\end{cases} \\] la variable número de fetos muertos se puede asociar a una distribución binomial: \\[ y_i = \\sum_{j=1}^{n_i} y_{ij} \\sim \\mbox{binomial}(n_i, \\pi_i), \\] donde \\(n_i\\) es el tamaño de la camada y \\(\\pi_i\\) es la probabilidad de que el feto muera. Dado que \\(\\pi_i\\) depende del tratamiento y de los niveles de hemoglobina de la rata hembra, se propone que: \\[ \\log\\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\beta_0+\\beta_1\\text{grp1}_{i}+\\beta_2\\text{grp2}_{i}+\\beta_3\\text{grp3}_{i}+\\beta_4\\text{hb}_i, \\] donde: \\(\\text{grp1}_{i}=1\\) si la \\(i\\)-ésima hembra pertenece al grupo 1, \\(\\text{grp1}_{i}=0\\) si lo contrario; \\(\\text{grp2}_{i}=1\\) si la \\(i\\)-ésima hembra pertenece al grupo 2, \\(\\text{grp2}_{i}=0\\) si lo contrario; \\(\\text{grp3}_{i}=1\\) si la \\(i\\)-ésima hembra pertenece al grupo 3, \\(\\text{grp3}_{i}=0\\) si lo contrario; y hb_i es el nivel de hemoglobina de la hembra \\(i\\). 7.2 Datos agrupados o datos no agrupados Los datos binarios tienen dos tipos de formatos: Datos agrupados: Hay \\(n_i\\) observaciones que tienen los mismos valores de las covariables \\(x_i\\). Aquí tenemos que para agrupación, \\(y_i \\sim\\)binomial\\((n_i,\\pi_i)\\). Datos no agrupados: Hay \\(n_i=1\\) (o muy pocas) observaciones por cada \\(x_i\\). Es decir que, cada \\(y_i\\) sigue una distribución Bernoulli\\((\\pi_i)\\). Las propiedades asintóticas de las inferencias para los datos no agrupados aplican cuando \\(N \\to \\infty\\). Mientras que para datos agrupados, aplican cuando \\(\\sum_{i=1}^nn_i\\to\\infty\\). 7.2.1 Datos agrupados Para datos agrupados, \\(D\\) y \\(X^2\\) sirven para evaluar si el ajuste del modelo es bueno o no. Aquí podemos plantear las siguientes hipótesis: \\(H_0\\) indica que el modelo se ajusta bien a los datos, \\(H_1\\) lo contrario. Si \\(H_0\\) es cierta (y \\(\\sum_{i=1}^nn_i\\to\\infty\\)), entonces \\(D\\) y \\(X^2\\) siguen una distribución \\(\\chi^2\\) con \\((n-p)\\) grados de libertad. 7.2.2 Datos no agrupados Las distribuciones límite para \\(D\\) y \\(X^2\\) no aplican para datos no agrupados. Tampoco para datos agrupados con \\(N\\) grande y algunos \\(n_i\\) muy pequeños. Se puede aproximar \\(D\\) y \\(X^2\\) agrupando \\((\\boldsymbol x_i,\\hat{\\boldsymbol y})\\) por particiones del espacio de covariables o por particiones de \\(\\hat{\\pi}\\). En estos casos, es preferible evaluar la falta de ajuste comparando el modelo propuesto contra modelos más generales. 7.3 Funciones de enlace En los modelos anteriores hemos propuesto funciones de enlace logit: \\(log\\begin{pmatrix} \\frac{\\pi_i}{1-\\pi_i} \\end{pmatrix}=\\boldsymbol x_i&#39;\\boldsymbol \\beta\\).Lo que implica que: \\[ \\pi_i=g^{-1}(\\boldsymbol x_i&#39;\\boldsymbol \\beta)=\\frac{\\exp(\\boldsymbol x_i&#39;\\boldsymbol \\beta)}{1+\\exp(\\boldsymbol x_i&#39;\\boldsymbol \\beta)}=\\frac{1}{1+\\exp(-\\boldsymbol x_i&#39;\\boldsymbol \\beta)} \\] Sin embargo, podemos utilizar otras funciones de enlace. Por ejemplo, la función Probit: \\[ \\Phi^{-1}(\\pi_i)=\\boldsymbol x&#39;_i\\boldsymbol \\beta\\quad \\pi_i=\\Phi(\\boldsymbol x&#39;_i\\boldsymbol \\beta), \\] donde \\(\\Phi(\\cdot)\\) es función acumulativa de la distribución normal estándar. La función Log-log complementaria: \\[ \\pi_i=1-\\exp[-\\exp(\\boldsymbol x&#39;_i\\boldsymbol \\beta)] \\quad \\log[-\\log(1-\\pi_i)]=\\boldsymbol x&#39;_i\\boldsymbol \\beta. \\] En la Figura 7.2 podemos observar la estimación de la probabilidad de que el escarabajo muera utilizando diferentes funciones de enlace (logit, probit, log-log complementaria). Graficamente, la función log-log complementaria parece mostrar mejores estimaciones que las otras. Figure 7.2: Datos de mortalidad de escarabajos. Estimaciones de la probabilidad de que el escarabajo muera en función del logaritmo de la dosis de disulfuro de carbono gaseoso utilizando diferentes funciones de enlace: logit(negro), probit(rojo) y cloglog(verde). Este resultado se puede comprobar utilizando criterios de información: AIC(modEsc.logit) ## [1] 41.43027 AIC(modEsc.probit) ## [1] 40.3178 AIC(modEsc.cloglog) ## [1] 33.64448 7.4 Curva característica operativa del receptor(ROC) Para evaluar el poder predictivo del modelo se puede construir una tabla de contingencia comparando \\(y_i\\) con la predicción \\(\\widehat{y}_i\\) a través del modelo. Para datos no agrupados, la predicción \\(\\widehat{y}_i\\) se define como: \\[ \\widehat{y}_i = \\begin{cases} 1 &amp; \\mbox{si } \\widehat{\\pi}_i &gt; \\pi_0, \\\\ 0 &amp; \\mbox{si } \\widehat{\\pi}_i \\leq \\pi_0, \\\\ \\end{cases} \\] para un punto de corte \\(\\pi_0\\) definido por el investigador, por ejemplo \\(\\pi_0 = 0.5\\). Con esto podemos contruir la siguiente tabla: \\(\\hat{y}=0\\) \\(\\hat{y}=1\\) \\(y=0\\) a c \\(y=1\\) b d donde \\(a\\) y \\(b\\) corresponde al conteo de predicciones negativas \\((\\widehat{y}= 0)\\) correctas (cuando \\(y=0\\)) e incorrectas (cuando \\(y=1\\)), respectivamente. Mientras que \\(c\\) y \\(d\\) son los contenos de predicciones positivas \\((\\widehat{y}= 1)\\) incorrectas (cuando \\(y=0\\)) y correctas (cuando \\(y=1\\)), respectivamente. El recuento de casillas en esta tabla permite estimar la sensibilidad, definida como la probabilidad de identificar correctamente un caso positivo, esto es: \\[ P(\\widehat{y}=1|y=1) = \\frac{d}{b+d}, \\] y la especificidad, definida como la probabilidad de identificar correctamente un caso negativo, \\[ P(\\widehat{y}=0|y=0) = \\frac{a}{a+c}. \\] Si el objetivo del modelo es realizar predicciones es preferible obtener valores altos para estas métricas. Sin embargo, estos valores dependen de \\(\\pi_0\\). Consideremos los datos de peso al nacer. Se propone el siguiente modelo para la probabilidad de nacer con bajo peso: \\[ \\mbox{logit } \\pi_i = \\beta_0+ \\mbox{age}_i\\beta_1+ \\mbox{lwt}_i\\beta_2 + \\mbox{smoke}_i\\beta_3 + \\mbox{ptl}_i\\beta_4, \\] El ajuste del modelo es: ## ## Call: ## glm(formula = low ~ age + lwt + smoke + ptl, family = binomial, ## data = birthwt) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.275961 1.031063 1.238 0.2159 ## age -0.048859 0.033796 -1.446 0.1483 ## lwt -0.010412 0.006219 -1.674 0.0941 . ## smoke 0.550111 0.334455 1.645 0.1000 ## ptl 0.695720 0.336895 2.065 0.0389 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 234.67 on 188 degrees of freedom ## Residual deviance: 218.36 on 184 degrees of freedom ## AIC: 228.36 ## ## Number of Fisher Scoring iterations: 4 Estos resultados muestran que el estado de tabaquismo y el historial de partos prematuros son factores de riesgo para el bajo peso al nacer. Sin embargo, el primero no tiene un aporte significativo dentro del modelo. Ahora vamos a evaluar el poder predictivo del modelo. Si definimos \\(\\pi_0=0.5\\), tenemos los siguientes resultados: \\(\\hat{y}=0\\) \\(\\hat{y}=1\\) \\(y=0\\) 123 7 \\(y=1\\) 49 10 Por lo tanto, la sensibilidad es \\(10/59 = 0,169\\) y la especificidad es \\(123/130 = 0.946\\). Con este punto de corte, el modelo tiene una predictividad alta para los recién nacidos con peso adecuado (alta especificidad). Sin embargo, la probabilidad de falsos positivos es alta (baja sensibilidad). Ahora, si definimos \\(\\pi_0= 0,3\\) obtenemos lo siguiente: \\(\\hat{y}=0\\) \\(\\hat{y}=1\\) \\(y=0\\) 79 51 \\(y=1\\) 19 40 En este caso la sensibilidad aumentó \\((40/59 = 0,678)\\), pero la especificidad disminuye \\((79/130 = 0,607)\\). Dado que la sensibilidad y especificidad tiene una alta dependencia del punto de corte, lo preferible es calcular estas cantidades para muchos valores de \\(\\pi_0\\) y buscar un valor ‘óptimo’. En la Figura 7.3 podemos observar que a medida que aumenta \\(\\pi_0\\) la sensibilidad disminuye y la especificidad aumenta. Figure 7.3: Datos de bajo peso al nacer. Sensibilidad (linea negra) y especificidad (linea roja) para diferentes puntos de corte. La sensibilidad es la tasa de verdaderos positivos (tvp), mientras que, el complemento de la especificidad \\((P(\\widehat{y}= 1 | y=0))\\) es la tasa de falsos positivos (tfp). El gráfico de la tasa de verdaderos positivos en función de la tasa de falsos positivos para diferentes valores de \\(\\pi_0\\), entre \\(0\\) y \\(1\\), lleva el nombre de curva característica operativa del receptor (ROC). Cuando \\(\\pi_0\\) es muy cercano a \\(1\\), casi todas las predicciones son \\(\\widehat{y}=0\\); entonces tenemos (tvp,tfp)\\(\\approx(0,0)\\). Mientras que, cuando \\(\\pi_0\\) es muy cercano a \\(0\\), casi todas las predicciones son \\(\\widehat{y}=1\\); entonces tenemos (tvp,tfp)\\(\\approx(1,1)\\). esto lo podemos ver en la Figura 7.4. Figure 7.4: Datos de bajo peso al nacer. Sensibilidad (linea negra) y especificidad (linea roja) para diferentes puntos de corte. Si el modelo realiza buenas predicciones, para un valor de especificidad dado, se espera que la sensibilidad también sea alta. Por lo tanto, el poder predictivo del modelo se puede medir a través del área bajo la curva del gráfico ROC. Esta medida es llamada índice de concordancia. En R, las curvas ROC se puede realizar por medio de la función roc de la librería pROC de la siguiente forma: library(pROC) modbw.logit = glm(low~age+lwt+ptl+smoke,family=binomial(logit),data=birthwt) modbw.probit = glm(low~age+lwt+smoke+ptl,family=binomial(probit),data=birthwt) ROCbw.logit = roc(birthwt$low~modbw.logit$fitted.values) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases ROCbw.probit = roc(birthwt$low~modbw.probit$fitted.values) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(ROCbw.logit,xlab=&#39;especificidad&#39;,ylab=&#39;sensibilidad&#39;) lines(ROCbw.probit,col=2) Figure 7.5: Datos de bajo peso al nacer. Curva ROC para predecir bajo peso al nacer usando el modelo logístico (negro) y el probit (rojo) En la Figura 7.5 se puede observar que ambas funciones de enlace proporcionan el mismo poder predictivo. El índice de concordancia es de \\(AUC= 0,6884\\) para el modelo logístico y \\(AUC= 0,6866\\) para el modelo probit. Para datos agrupados, la base de datos se debe desagrupar para obtener la curva ROC. Por ejemplo para los datos de escarabajos: obs.morir = unlist(apply(Datos.esc,1,function(x){rep(c(1,0),c(x[3],x[2]-x[3]))})) prob.morir =rep(modEsc.logit$fitted.values,Datos.esc$n) ROCesc = roc(obs.morir~prob.morir) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(ROCesc,print.auc=T) Figure 7.6: Datos de mortalidad de escarabajos. Curva ROC para el modelo logístico 7.5 Sobredispersión En un experimento Bernoulli se asume que los \\(n_i\\) ensayos para la observación \\(i,\\quad (y_{i1},...y_{in_i})\\), son independientes. Por lo tanto, para \\(y_i = \\sum_{j=1}^{n_i}y_{ij}/n_i\\),tenemos que: \\[ E(y_i)=\\pi_i \\quad \\text{y} \\quad V(y_i)=\\pi_i(1-\\pi_i)\\frac{1}{n_i}. \\] Pero, hay casos donde las observaciones \\((y_{i1},...y_{in_i})\\) están correlacionadas, lo que hace que \\(V(y_{i}) \\neq V(y_i)=\\pi_i(1-\\pi_i)\\frac{1}{n_i}\\). Si asumimos que \\(cor(y_{is},y_{it})=\\phi\\), para todo \\(s\\ne t\\), tenemos que: \\[ V(y_{it})=\\pi_i(1-\\pi_i)\\quad \\text{y} \\quad cov(y_{it},y_{is})=\\phi\\pi_i(1-\\pi_i). \\] En este caso, la varianza de \\(y_i\\) es: \\[ V(y_i)=V\\begin{pmatrix} \\sum_{j=1}^{n_i}\\frac{y_{ij}}{n_i}\\end{pmatrix}=\\frac{1}{n_i^2} \\left[ \\sum_{j=1}^{n_i}V(y_{it})+2\\sum\\sum_{s&lt;t}cov(y_{is},y_{it}) \\right]=[1+\\phi(n_i-1)]\\frac{\\pi_i(1-\\pi_i)}{n_i}. \\] Dependiendo del valor de \\(\\phi\\), la varianza de \\(y_i\\) puede ser mayor o menor a la esperada bajo una distribución binomial. Si \\(\\phi_i &gt; 0\\), tenemos un problema de sobredispersión (los datos tienen una varianza superior a la que se asume bajo un modelo binomial). Mientras que, si \\(-(n_i-1)^{-1}&lt;\\phi&lt;0\\), hay presencia de subdispersión. El segunda caso es menos frecuenta en datos reales. 7.5.1 Evaluación de sobredispersión En el caso del modelo binomial, el estadístico de \\(\\chi^2\\) de Pearson está definido como: \\[ X^2=\\sum_{i=1}^n\\frac{(y_i-\\widehat{\\pi}_i)^2}{\\widehat{\\pi}_i(1-\\widehat{\\pi}_1)/n_i}. \\] Si el modelo binomial proporciona buen ajuste, \\(X^2\\) sigue (asintóticamente) una distribución \\(\\chi^2\\) con \\(n-p\\) grados de libertad. Por lo tanto, un indicador de posible inflación de varianza es: \\[ \\widehat{\\phi}=\\frac{X^2}{n-p}. \\] Si hay problemas de sobredispersión, entonces \\(\\widehat{\\phi} &gt; 1\\). 7.6 Distribución beta-binomial El modelo beta-binomial es una alternativa para modelar datos binomial con sobredispersión. Esta distribución parte de una mezcla de una distribución binomial con una distribución beta. Sea \\(y^{*}\\) el número de éxitos de \\(n\\) ensayos Bernoulli. Se puede suponer la siguiente distribución de \\(y^{*}\\): \\[ y^{*}|\\pi\\sim \\mbox{binomial}(n,\\pi), \\mbox{ donde } \\pi\\sim \\mbox{beta}(\\alpha_1,\\alpha_2). \\] La función de densidad de \\(\\pi\\) es: \\[ f(\\pi;\\alpha_1,\\alpha_2)=\\frac{\\Gamma(\\alpha_1+\\alpha_2)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}\\pi^{\\alpha_1-1}(1-\\pi)^{\\alpha_2-1}, \\] con \\(\\alpha_1&gt;0\\) y \\(\\alpha_2&gt;0\\). Asumiendo \\(\\mu=\\frac{\\alpha_1}{\\alpha_1+\\alpha_2}\\) y \\(\\theta=1/(\\alpha_1+\\alpha_2)\\), tenemos: \\[ E(\\pi)=\\mu\\quad\\text{y}\\quad V(\\pi)=\\mu(1-\\mu)\\frac{\\theta}{1+\\theta}. \\] La distribución beta-binomial se obtiene al marginalizar \\(y^{*}\\). Esto es: \\[ f(y^{*};n,\\mu,\\theta)=\\int f(y^{*}|\\pi)f(\\pi)d\\pi. \\] Al resolver la integral anterior, encontramos la función de densidad de la distribución beta-binomial es: \\[ f(y^{*};n,\\mu,\\theta)=\\begin{pmatrix} n \\\\ y^{*} \\end{pmatrix}\\frac{\\begin{bmatrix} \\Pi_{k=0}^{y^{*}-1}(\\mu+k\\theta)\\end{bmatrix} \\begin{bmatrix} \\Pi_{k=0}^{n-y^{*}-1}(1-\\mu+k\\theta)\\end{bmatrix}}{\\begin{bmatrix} \\Pi_{k=0}^{n-1}(1+k\\theta) \\end{bmatrix}}, \\] para \\(y^{*}=0,1,...,n.\\) El valor esperado y varianza de \\(y = \\frac{y^{*}}{n}\\) son: \\[ E(y)=\\mu\\quad\\text{y}\\quad V(y)=\\left[ 1+(n-1)\\frac{\\theta}{1+\\theta} \\right]\\frac{\\mu(1-\\mu)}{n} = \\left[ 1+(n-1)\\phi \\right]\\frac{\\mu(1-\\mu)}{n}. \\] Por lo cual, \\(\\phi &gt; 0\\) es la correlación entre ensayos Bernoulli. La distribución de probabilidad de la distribución beta-binomial se puede observar en la Figura 7.7. Aquí vemos que la dispersión de la distribución beta-binomial aumenta con el parámetro \\(\\phi\\). Figure 7.7: Distribución de probabilidad de Binomial(15,0.3) (negro), beta-binomial(15,0.3,0.1) (rojo), beta-binomial(15,0.3,0.4) (verde) 7.6.1 Modelo beta-binomial El modelo beta-binomial para \\(y_i^{*}\\) se define como: \\[ y_i^{*}|\\pi_i\\sim \\mbox{binomial}(n_i,\\pi_i), \\mbox{ donde } \\pi_i\\sim \\mbox{beta}(\\mu_i,\\phi), \\] donde \\(\\mbox{logit } \\mu_i = \\boldsymbol x_{i}&#39;\\boldsymbol \\beta\\) (si usamos una función de enlace logit). Por lo cuál, para \\(y_i = \\frac{y_i^{*}}{n_i}\\) se tiene que: \\[ E(y_i)=\\mu_i\\quad\\text{y}\\quad V(y_i)=[1+(n-1)\\phi]\\mu_i(1-\\mu_i)/n_i, \\] La estimación de los parámetros \\((\\beta,\\phi)\\) se hace por máxima verosimilitud. Dado que \\(\\phi&gt;0)\\), el modelo beta-binomial no puede modelar datos con subdispersión. 7.6.2 Estudio de teratología Para los datos del estudio de teratología se puede proponer un modelo binomial en un principio: \\[ y_i^* = n_i y_i \\sim \\mbox{binomial} (n_i, \\pi_i), \\] donde \\(n_i\\) y \\(y_i\\) es el tamaño y la proporción de fetos muertos de la camada \\(i\\). Además, se tiene que: \\[ \\mbox{logit }\\pi_i = \\beta_0 + \\mbox{hb}_i \\beta_1 + \\mbox{treat}_{2i} \\beta_2 + \\mbox{treat}_{3i}\\beta_3 + \\mbox{treat}_{4i} \\beta_4. \\] El ajuste del modelo es: data(lirat,package = &#39;VGAM&#39;) modlirat.binom = glm(cbind(R,N-R)~hb+as.factor(grp),family=binomial,data=lirat) summary(modlirat.binom) ## ## Call: ## glm(formula = cbind(R, N - R) ~ hb + as.factor(grp), family = binomial, ## data = lirat) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1795 0.5056 4.311 1.63e-05 *** ## hb -0.2190 0.1020 -2.148 0.031728 * ## as.factor(grp)2 -2.4748 0.5045 -4.906 9.30e-07 *** ## as.factor(grp)3 -3.1527 0.9433 -3.342 0.000832 *** ## as.factor(grp)4 -2.0520 1.0629 -1.931 0.053525 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 509.43 on 57 degrees of freedom ## Residual deviance: 168.91 on 53 degrees of freedom ## AIC: 250.38 ## ## Number of Fisher Scoring iterations: 5 La evaluación de sobredispersión la podemos hacer usando la razón entre el estadístico \\(\\chi^2\\) de Pearson y los grados de libertad: X2 = sum(residuals(modlirat.binom,type=&#39;pearson&#39;)^2) X2/53 ## [1] 2.93993 Dado que esta razón es aproximadamente \\(3\\), los datos presentan sobredispersión. Por lo tanto es mas conveniente proponer un modelo beta-binomial: \\[ y_i^{*}\\sim \\mbox{beta-binomial}(n_i,\\pi_i,\\phi), \\] con \\[ \\mbox{logit }\\pi_i = \\beta_0 + \\mbox{hb}_i \\beta_1 + \\mbox{treat}_{2i} \\beta_2 + \\mbox{treat}_{3i}\\beta_3 + \\mbox{treat}_{4i} \\beta_4. \\] En R el modelo beta-binomial se puede estimar utilizando la función betabin de la librería aod: library(aod) modlirat.betabinom =betabin(cbind(R,N-R)~hb+as.factor(grp),data=lirat,random=~1) modlirat.betabinom ## Beta-binomial model ## ------------------- ## betabin(formula = cbind(R, N - R) ~ hb + as.factor(grp), random = ~1, ## data = lirat) ## ## Convergence was obtained after 433 iterations. ## ## Fixed-effect coefficients: ## Estimate Std. Error z value Pr(&gt; |z|) ## (Intercept) 2.130e+00 8.676e-01 2.455e+00 1.408e-02 ## hb -1.694e-01 1.772e-01 -9.557e-01 3.392e-01 ## as.factor(grp)2 -2.441e+00 8.508e-01 -2.869e+00 4.113e-03 ## as.factor(grp)3 -2.836e+00 1.329e+00 -2.134e+00 3.287e-02 ## as.factor(grp)4 -2.285e+00 1.817e+00 -1.258e+00 2.085e-01 ## ## Overdispersion coefficients: ## Estimate Std. Error z value Pr(&gt; z) ## phi.(Intercept) 2.356e-01 6.041e-02 3.9e+00 4.813e-05 ## ## Log-likelihood statistics ## Log-lik nbpar df res. Deviance AIC AICc ## -9.302e+01 6 52 1.146e+02 1.98e+02 1.997e+02 Las estimaciones de los coeficientes del modelo beta-binomial son similares a los del modelo binomial. Sin embargo, los errores estándar son mas grandes. Esto último es debido al efecto de la sobredispersión (\\(\\widehat{\\phi}=0.236\\) indica una correlación positiva entre los fetos que pertenecen a la misma camada). Además, el efecto del tratamiento #4 no es significativamente diferente al placebo. Podemos evaluar sobredispersión de nuevo: X2 = sum(residuals(modlirat.betabinom,type=&#39;pearson&#39;)^2) ## Warning in model.matrix.default(mt, mfb, contrasts): non-list contrasts argument ignored X2/52 ## [1] 0.9287516 La razón está cercana a \\(1\\) por lo que la varianza está mejor modelada por la distribución beta-binomial. La comparación de los ajustes se puede hacer a través de criterios de información: AIC(modlirat.binom) ## [1] 250.3777 AIC(modlirat.betabinom) ## df AIC AICc ## modlirat.betabinom 6 198.0319 199.6789 Observando los valores del AIC podemos determinar que el modelo beta-binomial presenta mejor ajuste. "],["modelo-para-conteos.html", "Capítulo 8 Modelo para conteos 8.1 Casos de estudio 8.2 Modelo Poisson 8.3 Sobredispersión 8.4 Inflación de ceros", " Capítulo 8 Modelo para conteos 8.1 Casos de estudio 8.1.1 Ataques de epilepsia Vamos a considerar de nuevo los datos del ensayo clínico sobre epilepsia (data(epilepsy) de la librería HSAUR2). 8.1.2 Muertes por enfermedades cardiovasculares en doctores del Reino Unido Los datos bs1 de la librería ACSW muestran el número de doctores muertos por enfermedades coronarias entre médicos varones \\(10\\) años después de seguimiento por grupos de edad y condición de tabaquismo. De igual forma, la base de datos contiene el número de total de años-personas de observación en el momento de análisis. En la Figura 8.1 podemos observar la tasa de mortalidad, por 100,000 personas, crece por grupo de edad, y es generalmente, mayor para los fumadores. library(ACSWR) data(bs1) plot(bs1$Age_Cat, 100000*(bs1$Deaths/bs1$Person_Years),col=bs1$Smoke_Ind+1, xlab=&#39;grupo de edad&#39;,ylab=&#39;muertes por 100,000 personas-año&#39;,pch=20,xaxt=&#39;n&#39;) axis(1,1:5,bs1$Age_Group[1:5]) Figure 8.1: Datos de muertes cardiovasculares. Tasa de muertes por enfermedades cardiovasculares en doctores del Reino Unido por grupo de edad. Los puntos rojos corresponden a fumadores y los negros a no fumadores. A partir de estos datos surgen las siguientes preguntas: ¿La tasa de mortalidad es mayor para los fumadores que para los no fumadores? y ¿El efecto diferencial está relacionado con la edad? 8.1.3 Número de cangrejos satélites Los datos crabs de la librería asbio son de un estudio sobre \\(173\\) hembras de cangrejos herradura en una isla en el Golfo de México. Durante el periodo de desove, el macho se adhiere a las hembras cuando estas van a la playa a excavar un agujero en la arena para poner racimos de huevos. Esto con el fin de que puedan fecundar los huevos (fecundación externa). Dada la competencia, las hembras suelen llegar a la playa con varios machos adheridos a ella. A este grupo de cangrejos se les llama satélites. Aquí, nuestra variable de respuesta es el número de satélites que tiene cada hembra. Mientras que las posibles covariables son: color: color (1, medio-claro; 2, medio; 3, medio-oscuro; 4,oscuro). spine: condición de la espina dorsal (1, ambos bien; 2, uno gastado o roto; 3, ambos gastados o rotos). width: ancho del caparazón (cm). weight: peso (kg) En la Figura 8.2 podemos observar que hay una alta presencia de hembras que llegan a la playa sin ningún satélite. Además, vemos que hay una relación positiva entre el número de satélites y el tamaño de la hembra. Figure 8.2: Datos de cangrejos satélites. Histograma del número de satélites por hembra (izquierda) y diagrama de dispersión del número de satélites en función del peso de la hembra (derecha) 8.2 Modelo Poisson La distribución Poisson es comúnmente usada para modelar el número de eventos aleatorios que ocurren en un intervalo de tiempo o espacio determinado. Esta distribución también se aplica como una aproximación de la binomial cuando \\(n\\) es grande y \\(\\pi\\) es pequeño. En este caso, la binomial\\((n_i, \\pi_i)\\) converge hacia una Poisson\\((\\mu=n\\pi)\\). El modelo Poisson se expresa como: \\[ y_i\\sim \\mbox{Poisson}(\\lambda_i), \\quad i=1,...,n, \\text{ donde } \\log \\lambda_i=\\boldsymbol x&#39;_i\\boldsymbol \\beta, \\] donde \\(\\boldsymbol x_i\\) es el vector de covariables asociada al individuo \\(i\\). Este modelo asume que: \\[ E(y_i|\\boldsymbol x_i)=V(y_i|\\boldsymbol x_i)=\\lambda_i. \\] Es decir, la media es igual a la varianza. 8.2.1 Modelo de conteo con offset En algunos casos los conteos \\(y_i\\) son proporcionales a un índice \\(t_i\\) (intervalo de tiempo, área de espacio, tamaño de población). Cuando \\(t_i\\) no es fijo, se debe incluir un término llamado offset. En este caso, es preferible modelar la tasa \\(y_i/t_i\\) con valor esperado igual a \\(\\lambda_i/t_i\\). Por lo tanto, el predictor lineal queda de la forma: \\[ \\log\\left(\\frac{\\lambda_i}{t_i}\\right)=\\boldsymbol x&#39;_i\\boldsymbol \\beta, \\] donde el término \\(\\log t_i\\) es llamado offset. Por lo que \\(\\log\\lambda_i=\\boldsymbol x_i&#39;\\boldsymbol \\beta+\\log t_i\\). Entonces, el valor esperado de \\(y_i\\) es: \\[ \\lambda_i=t_i \\exp(\\boldsymbol x_i&#39;\\boldsymbol \\beta). \\] #### Muertes por enfermedades cardiovasculares en doctores del Reino Unido Dado que los tamaños de la población por grupo de edad son diferentes (hay menos personas en los grupos de edad mayores), debemos considerar esto en el modelo por medio del offset. En este caso el modelo propuesto para el número de doctores muertos por enfermedades cardiovasculares en la \\(i\\)-ésima categoría \\((y_i)\\) puede ser: \\[ y_i \\sim \\mbox{Poisson}(\\lambda_i), \\] donde: \\[ \\log \\lambda_i = \\beta_0 + \\mbox{smoke}_i\\beta_1 + \\mbox{ageCat1}_i\\beta_2 + \\mbox{ageCat2}_i\\beta_3 + \\mbox{ageCat3}_i\\beta_4 + \\mbox{ageCat4}_i\\beta_5 + \\log(t_i), \\] smoke\\(_i=1\\) si la \\(i\\)-ésima categoría corresponde a fumadores; smoke\\(_i=0\\) si lo contrario; \\(t_i\\) es el número de doctores en riesgo asociados al grupo \\(i\\), ageCat\\(j\\) corresponde a las variables indicadores asociadas al grupo de edad, con \\([35,44]\\) como referencia. El ajuste del modelo es poisson.doctor = glm(Deaths ~ Smoke_Ind + Age_Group + offset(log(Person_Years)), family = poisson(link = &quot;log&quot;), data = bs1) summary(poisson.doctor) ## ## Call: ## glm(formula = Deaths ~ Smoke_Ind + Age_Group + offset(log(Person_Years)), ## family = poisson(link = &quot;log&quot;), data = bs1) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.9193 0.1918 -41.298 &lt; 2e-16 *** ## Smoke_Ind 0.3545 0.1074 3.302 0.00096 *** ## Age_Group45-54 1.4840 0.1951 7.606 2.82e-14 *** ## Age_Group55-64 2.6275 0.1837 14.301 &lt; 2e-16 *** ## Age_Group65-74 3.3505 0.1848 18.131 &lt; 2e-16 *** ## Age_Group75-84 3.7001 0.1922 19.249 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 935.067 on 9 degrees of freedom ## Residual deviance: 12.132 on 4 degrees of freedom ## AIC: 79.2 ## ## Number of Fisher Scoring iterations: 4 Estos datos sugieren que, luego de tener en cuenta el efecto de la edad, los fumadores tienen una tasa de mortalidad \\(\\exp(\\widehat{\\beta}_1)=1.42\\) mayor que los no fumadores. Para simplificar el modelo, y evaluar efectos de interacción, uno podría tomar la categoría de edad de forma numérica de la siguiente forma: *age\\(=1,2,\\ldots,5\\) para las categorías \\(35-44,45-54,\\ldots,75-84\\). Así podemos proponer un modelo de la forma: \\[ \\log \\lambda_i = \\beta_0 + \\mbox{smoke}_i\\beta_1 + \\mbox{age}_i\\beta_2 + \\mbox{age}^2_i\\beta_3 + \\mbox{smoke}_i\\mbox{age}_i\\beta_4 + \\log(t_i), \\] El efecto de la interacción se incluye para evaluar si la diferencia en la tasa de muertes entre fumadores y no fumadores cambia con el aumento de la edad. El ajuste del modelo es: bs1$Age = c(1:5,1:5) poisson.doctor2 = glm(Deaths ~ Smoke_Ind+Age+I(Age^2)+Smoke_Ind*Age + offset(log(Person_Years)), family = poisson(link = &quot;log&quot;), data = bs1) summary(poisson.doctor2) ## ## Call: ## glm(formula = Deaths ~ Smoke_Ind + Age + I(Age^2) + Smoke_Ind * ## Age + offset(log(Person_Years)), family = poisson(link = &quot;log&quot;), ## data = bs1) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.79176 0.45008 -23.978 &lt; 2e-16 *** ## Smoke_Ind 1.44097 0.37220 3.872 0.000108 *** ## Age 2.37648 0.20795 11.428 &lt; 2e-16 *** ## I(Age^2) -0.19768 0.02737 -7.223 5.08e-13 *** ## Smoke_Ind:Age -0.30755 0.09704 -3.169 0.001528 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 935.0673 on 9 degrees of freedom ## Residual deviance: 1.6354 on 5 degrees of freedom ## AIC: 66.703 ## ## Number of Fisher Scoring iterations: 4 Estos resultados también muestran que, en promedio, los fumadores tienen una tasa de mortalidad \\(4\\) veces más alta que los no fumadores. Sin embargo, esta diferencia se atenúa a medida que la edad aumenta (efecto interacción negativo). Esto lo podemos ver forma gráfica en la Figura 8.3. plot(bs1$Age, 100000*(bs1$Deaths/bs1$Person_Years),col=bs1$Smoke_Ind+1, xlab=&#39;grupo de edad&#39;,ylab=&#39;muertes por 100,000 personas-año&#39;,pch=20,xaxt=&#39;n&#39;) axis(1,1:5,bs1$Age_Group[1:5]) pred.tasa = 100000*poisson.doctor2$fitted.values/bs1$Person_Years lines(1:5,pred.tasa[bs1$Smoke_Ind==0],lwd=2) lines(1:5,pred.tasa[bs1$Smoke_Ind==1],col=2,lwd=2) Figure 8.3: Datos de muertes cardiovasculares. Estimación de la tasa de muertes de doctores del Reino Unido por grupo de edad para fumadores(línea roja) y no fumadores (línea negra). 8.3 Sobredispersión El modelo Poisson asume que \\(V(y) = E(y)\\). Sin embargo, es común encontrar datos en que la varianza crece más rápido que la media, provocando sobredispersión. En este caso, el estimador por máxima verosimilitud del modelo Poisson sigue proporcionando estimaciones consistentes para la media (estimaciones insesgadas). Sin embargo, los errores estándar estarán mal calculados (generalmente más pequeños). Para modelar datos de tipo conteo en presencia de sobredispersión se puede asumir una distribución binomial negativa. Esta incluye un parámetro adicional que cuantifica la sobredispersión. 8.3.1 Distribución binomial negativa La distribución binomial negativa parte de una mezcla de una distribución Poisson con una gamma. Esto es: \\[ y | \\lambda \\sim \\mbox{Poisson}(\\lambda), \\mbox{ donde } \\lambda\\sim \\mbox{gamma}(k,\\mu). \\] La función de densidad de \\(\\lambda\\) es: \\[ f(\\lambda;k,\\mu)=\\frac{(k/\\mu)^k}{\\Gamma(k)}\\exp\\left(-\\frac{k\\lambda}{\\mu}\\right)\\lambda^{k-1}. \\] Entonces tenemos: \\[ E(\\lambda)=\\mu \\mbox{ y } V(\\lambda)=\\mu^2/k. \\] La distribución binomial negativa (también llamada Poisson-gamma) se obtiene al marginalizar \\(y\\). Esto es: \\[ f(y;\\mu,k)=\\int f(y|\\lambda)f(\\lambda)d\\lambda. \\] La función de densidad de la binomial negativa queda definida como: \\[ f(y;\\mu,k)=\\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left( \\frac{\\mu}{\\mu+k}\\right)^y \\left( \\frac{k}{\\mu+k} \\right)^k \\] para \\(y=0,1,...\\) Si definimos \\(\\gamma=1/k\\), tenemos que: \\[ E(y)=\\mu \\mbox{ y } V(y)=\\mu(1+\\gamma\\mu), \\] para \\(\\gamma&gt;0\\) (llamado parámetro de sobredispersión). La distribución de probabilidad de la distribución binomial negativa se puede observar en la Figura 8.4. Aquí vemos que a medida que aumenta \\(\\gamma\\), la probabilidad de valores extremos es más alta. Figure 8.4: Función de probabilidad de Poisson(3) (negro), binomial negativa (3,0.5) (rojo), binomial negativa (3,1.5) (verde). Dado que \\(\\gamma&gt;0\\), el modelo binomial negativo no puede modelar datos con subdispersión. 8.3.2 Modelo binomial negativo El modelo binomial negativo se expresa de la siguiente forma: \\[ y_i|\\lambda_i\\sim \\mbox{binomial negativa}(\\mu_i,\\gamma), \\] donde \\[ \\mu_i=\\exp(\\boldsymbol x&#39;_i\\boldsymbol \\beta). \\] Por lo cuál: \\[ E(y_i)=\\mu_i\\mbox{ y } V(y_i)=\\mu_i(1+\\gamma\\mu_i). \\] La estimación de los parámetros \\((\\boldsymbol \\beta,\\gamma)\\) se hace por máxima verosimilitud. 8.3.2.1 Ataques epilépticos Para los datos de los ataques epilépticos se propone el siguiente modelo: \\[ y_i \\sim \\mbox{Poisson}(\\lambda_i), \\mbox{ donde } \\log \\lambda_i = \\beta_0 + \\mbox{treatment}_i\\beta_1 + \\mbox{base}_i\\beta_2, \\] donde \\(y_i\\) es el número de ataques epiléptivos en dos semanas luego de cuatro semanas de tratamiento; treatment\\(_i=1\\) si el \\(i\\)-ésimo paciente recibió el tratamiento progabida y treatment\\(_i=0\\) si lo contrario. Los resultados del ajuste son: epilepsy4= HSAUR2::epilepsy[HSAUR2::epilepsy$period==4,] modPois = glm(seizure.rate~treatment+base,data=epilepsy4,family=poisson) summary(modPois) ## ## Call: ## glm(formula = seizure.rate ~ treatment + base, family = poisson, ## data = epilepsy4) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.214772 0.085543 14.201 &lt; 2e-16 *** ## treatmentProgabide -0.315159 0.098469 -3.201 0.00137 ** ## base 0.021536 0.001039 20.733 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 476.25 on 58 degrees of freedom ## Residual deviance: 149.68 on 56 degrees of freedom ## AIC: 343.44 ## ## Number of Fisher Scoring iterations: 5 Los resultados del ajuste del modelo Poisson muestran que el tratamiento es altamente significativo. Por lo que, se puede concluir que la progabida reduce el número de ataques epilépticos. Sin embargo, estos datos presenta sobredispersión. Esto se puede verificar por medio de la razón del estadístico \\(\\chi^2\\) de Pearson y los grados de libertad: X2.poisson = sum(residuals(modPois,type=&#39;pearson&#39;)^2) X2.poisson/modPois$df.residual ## [1] 2.525656 Por esta razón se puede estimar el modelo binomial negativo: \\[ y_i \\sim \\mbox{binomial negativa}(\\mu_i,\\gamma), \\mbox{ donde } \\log \\mu_i = \\beta_0 + \\mbox{treat}_i\\beta_1 + \\mbox{base}_i\\beta_2. \\] El ajuste en R se puede hacer utilizando la función glm.nb de la librería MASS: library(MASS) modbinNeg = glm.nb(seizure.rate~treatment+base,data=epilepsy4) summary(modbinNeg) ## ## Call: ## glm.nb(formula = seizure.rate ~ treatment + base, data = epilepsy4, ## init.theta = 4.483069947, link = log) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.080878 0.153419 7.045 1.85e-12 *** ## treatmentProgabide -0.327892 0.168163 -1.950 0.0512 . ## base 0.025017 0.002688 9.308 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(4.4831) family taken to be 1) ## ## Null deviance: 174.166 on 58 degrees of freedom ## Residual deviance: 70.124 on 56 degrees of freedom ## AIC: 313.32 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 4.48 ## Std. Err.: 1.55 ## ## 2 x log-likelihood: -305.324 Las estimaciones del modelo binomial negativo son similares a las del ajuste del modelo Poisson. Sin embargo, vemos que los errores estándar son más grandes. Además, el valor-\\(p\\) del efecto asociado al tratamiento es mayor. Si consideramos \\(\\alpha=0.05\\), el efecto del tratamiento no es significativo. Podemos evaluar si se presentan problemas de sobredispersión con el ajuste del modelo binomial negativo: ## [1] 1.036141 Este resultado sugiere que la varianza está bien estimada. Comparando criterios de información, el modelo binomial negativo presenta un mejor ajuste que el modelo Poisson: ## [1] 321.6337 ## [1] 349.6773 8.4 Inflación de ceros En ocaciones, los datos muestran mayor frecuencia de ceros de los que la distribución Poisson y binomial negativa permiten. Por ejemplo, en datos de cangrejos herradura, hay muchas hembras que llegan a la playa sin ningún satélite. La Figura 8.2 muestra que una distribución bimodal para el número de satélites. Distribución con inflación de ceros Este tipo de datos se pueden modelar con una mezcla de distribuciones donde \\(Y=0\\) cuando \\(Z=1\\), y \\(Y\\) sigue una distribución particular cuando \\(Z=0\\) (la cual permite la posibilidad de ceros). Por ejemplo, la distribución Poisson con inflación de ceros (ZIP) se puede representar como: \\[ y\\sim\\begin{cases} 0 \\qquad &amp; \\text{con probabilidad}\\quad \\phi, \\\\ \\mbox{Poisson}(\\lambda) \\quad &amp; \\text{con probabilidad}\\quad 1-\\phi. \\end{cases} \\] La probabilidad de que \\(y\\) sea igual a cero es: \\[ P(y=0) = \\phi + (1-\\phi) \\exp \\left(-\\lambda\\right). \\] Mientras que para \\(y&gt;0\\) se tiene: \\[ P(y=j) =(1-\\phi) \\exp \\frac{\\left(-\\lambda\\right)\\lambda^j}{j!}, j=1,2,\\ldots. \\] El valor esperado y la varianza de \\(y\\) son: \\[ E(y) = E(E(y|z)) = (1-\\phi)\\lambda, \\mbox{ y } V(y)= (1-\\phi)\\lambda \\left( 1+ \\phi\\lambda\\right). \\] Note que la distribución ZIP permite sobredispersión. De igual forma, se puede tener un modelo binomial negativo con inflación de ceros (ZINB): \\[ y\\sim \\begin{cases} 0 \\qquad &amp; \\text{con probabilidad } \\phi, \\\\ \\mbox{binomial negativa}(\\mu,\\gamma) \\quad &amp; \\text{con probabilidad } 1-\\phi. \\end{cases} \\] En este modelo, la probabilidad de que \\(y\\) sea igual a cero es: \\[ P(y=0) = \\phi + (1-\\phi) \\left( 1 + \\gamma \\mu \\right)^{-1/\\gamma}. \\] Mientras que el valor esperado y varianza son: \\[ E(y)= (1-\\phi)\\mu \\mbox{ y } V(y) = (1-\\phi) \\mu \\left[1+\\mu(\\gamma+\\phi)\\right]. \\] Otra alternativa para modelar exceso de ceros es a través del mdelo Hurdle. Este asume que hay un proceso Bernoulli que determina si el conteo es igual a cero o es positivo. Si ocurre lo segundo, una distribución truncada determina la probabilidad del conteo. La distribución se puede expresar como: \\[\\begin{equation} P(y = j)=\\begin{cases} \\pi &amp; j=0, \\\\ (1-\\pi)\\frac{f(y|\\theta)}{1-f(0|\\theta)} &amp; j &gt;0. \\end{cases} \\tag{8.1} \\end{equation}\\] A diferencia del modelo ZI, aquí la probabilidad de \\(y=0\\) solo está determinado por \\(\\pi\\) (no por una mezcla de distribuciones). Por ejemplo, el modelo Hurdle Poisson (HP) se define como: \\[ P(y = j)=\\begin{cases} \\pi \\qquad &amp; j=0, \\\\ (1-\\pi)\\frac{\\lambda^{j}\\exp(-\\lambda)}{j! [1-\\exp(-\\lambda)]} &amp; j &gt;0. \\end{cases} \\] El valor esperado y varianza de \\(y\\) son: \\[\\begin{equation} E(y)= (1-\\pi) E(y | y &gt; 0) \\mbox{ y } V(y) = (1-\\pi) V(y | y &gt; 0) + \\pi(1-\\pi)E(y | y&gt;0)^2, \\tag{8.2} \\end{equation}\\] donde: \\[ E(y | y &gt; 0) = \\frac{\\lambda}{1- \\exp(-\\lambda)} \\mbox{ y } V(y|y&gt;0) = \\frac{\\mu}{1-\\exp(-\\lambda)} + \\exp(-\\lambda)E(y | y &gt;0)^2. \\] También, se tiene la distribución Hurdle binomial negativa (HNB), definiendo \\(f(y)\\) como la distribución binomial negativa en (8.1). 8.4.1 Modelo de inflación de ceros El modelo Poisson con ceros inflados (ZIP) se expresa de la siguiente forma: \\[ y_i \\sim \\begin{cases} 0 \\qquad &amp; \\text{con probabilidad } \\phi_i, \\\\ \\mbox{Poisson}(\\lambda_i) \\quad &amp; \\text{con probabilidad } 1-\\phi_i. \\end{cases} \\] Los parámetros \\(\\phi_i\\) y \\(\\lambda_i\\) pueden ser modelados a través de covariables: \\[ \\mbox{logit } \\phi_i=\\boldsymbol x&#39;_{1i}\\boldsymbol \\beta_1 \\quad\\text{y}\\quad \\log\\lambda_i=\\boldsymbol x&#39;_{2i}\\boldsymbol \\beta_2. \\] En presencia de una sobredispersión mayor, el modelo de ceros inflados puede combinarse con una distribución binomial negativa. Esto es: \\[ y_i\\sim \\begin{cases} 0 &amp; \\text{con probabilidad } \\phi_i, \\\\ \\text{binomial negativa}(\\mu_i,\\gamma)&amp; \\text{con probabilidad } 1-\\phi_i. \\end{cases} \\] De igual forma \\(\\mu_i\\) y \\(\\phi_i\\) se pueden modelar por medio de covariables. 8.4.2 Modelo Hurdle El modelo Hurdle lo podemos expresar de la forma: \\[ P(Y_i=j)=\\begin{cases} \\pi_i &amp; j=0,\\\\ (1-\\pi_i)\\frac{f(j;\\theta_i)}{1-f(0;\\theta_i)} &amp; j=1,2,\\ldots, \\end{cases} \\] donde la función \\(f(\\cdot;\\theta_i)\\) puede ser una Poisson o binomial negativa. Al igual que el modelo ZI, \\(\\pi_i\\) se puede modelar usando un modelo logístico y un modelo log-lineal para \\(\\lambda_i\\) (Poisson) o \\(\\mu_i\\) (binomial negativo). 8.4.3 Número de cangrejos satélites Para los datos de los cangrejos satélites podemos estimar un modelo con inflación de ceros con: \\[ \\mbox{logit } \\phi_i=\\beta_{10} + \\mbox{weight}_i\\beta_{11} \\quad\\text{y}\\quad \\log\\lambda_i=\\beta_{20} + \\mbox{weight}_i\\beta_{21}. \\] En R, se puede utilizar la funcion zeroinfl de la librería pscl library(pscl) # modelo ZIP ZIP.sat = zeroinfl(satell~weight | weight,data=crabs,dist=&#39;poisson&#39;) # modelo ZINB ZINB.sat = zeroinfl(satell~weight | weight,data=crabs,dist=&#39;negbin&#39;) Mientras que los modelos Hurdle se pueden estimar con la función hurdle: # modelo HP HP.sat = hurdle(satell~weight | weight,data=crabs,dist=&#39;poisson&#39;) # modelo HNB HNB.sat = hurdle(satell~weight | weight,data=crabs,dist=&#39;negbin&#39;) Los ajustes los podemos comparar con criterios de información: BIC(ZIP.sat) ## [1] 747.8175 BIC(ZINB.sat) ## [1] 731.2937 BIC(HP.sat) ## [1] 747.7903 BIC(HNB.sat) ## [1] 731.0369 Aquí vemos que los modelos ZINB y HNB proporcionan mejores ajustes. Las estimaciones del modelo ZINB son: summary(ZINB.sat) ## ## Call: ## zeroinfl(formula = satell ~ weight | weight, data = crabs, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -1.3647 -0.7899 -0.3112 0.5086 3.8908 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.8979 0.3053 2.941 0.00327 ** ## weight 0.2171 0.1119 1.941 0.05229 . ## Log(theta) 1.6013 0.3553 4.507 6.57e-06 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.7565 0.9841 3.817 0.000135 *** ## weight -1.9131 0.4322 -4.426 9.59e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 4.9595 ## Number of iterations in BFGS optimization: 19 ## Log-likelihood: -352.8 on 5 Df Los resultados muestran que la probabilidad de que no hayan satélites disminuye cuando la hembra tiene mayor peso. De igual forma, la media incrementa con el peso. "],["modelo-lineal-mixto.html", "Capítulo 9 Modelo lineal mixto 9.1 Casos de estudio 9.2 Datos por cluster (correlacionados) 9.3 Notación 9.4 Modelos con intercepto aleatorios 9.5 Modelo lineal mixto 9.6 Estimación de los parámetros", " Capítulo 9 Modelo lineal mixto 9.1 Casos de estudio 9.1.1 Peso al nacer de ratas Los datos ratpup de la librería WWGbook corresponden a un estudio donde se asignaron aleatoriamente 30 ratas hembra para recibir una de tres dosis (Alta, Baja o Control) de un compuesto experimental. El objetivo es determinar como este compuesto afecta el peso al nacer de las crías. En la Figura 9.1 se puede observar que el peso de los machos tiende a ser un poco mayor que el de las hembras. En término de los tratamientos, las crías que pertenecen a las madres que recibieron las dosis del compuesto experimental tienen pesos levemente inferiores. Además, vemos que las medidas del peso de las crías que pertenecen a la misma camada son más parecidas entre ellas que las observaciones entre camadas. Esto puede ser un indicio de que las observaciones dentro de la misma camada estén correlacionadas, sin embargo, se podría asumir que las observaciones entre camadas sean independientes. library(WWGbook) data(ratpup) plot(ratpup$litter,ratpup$weight,col=as.double(ratpup$sex),pch=19,xlab=&#39;camada&#39;, ylab=&#39;peso al nacer (gramos)&#39;) abline(v=c(10.5,20.5),lty=2) Figure 9.1: Datos del peso al nacer de ratas. Diagrama de dispersión de peso de la cría por camada. Los puntos negros corresponden a las hembras y los rojos a los machos. Las primeras diez camadas corresponden al control, las siguientes diez a la dosis baja y las últimas diez a la dosis alta. 9.1.2 Capacidad respiratoria El volumen espiratorio forzado (FEV) es la cantidad de aire que una personal puede exhalar (en litros por segundo) durante una respiración forzada . Los datos fev.txt (Disponible aquí) pertenece a un estudio que tiene como objetivo determinar como dos medicamentos experimentales afecta la capacidad respiratoria de los pacientes, la cual se cree es un efecto secundario a corto plazo del tratamiento. Para esto, se asignó de forma aleatoria \\(72\\) pacientes hombres en tres grupos, droga A, droga B y control (placebo). A cada paciente, el FEV fue medido antes administrar los tratamientos (baseline) y luego cada hora durante 8 horas consecutivas. La Figura 9.2 muestra las trayectorias individuales de cada individuo. Se puede observar una tendencia negativa del FEV en el tiempo para los pacientes que se encuentran en los tratamientos. Mientras que, para los pacientes del grupo placebo se observa que el FEV es constante en el tiempo. Además, se observa que hay poca variabilidad en las mediciones que pertenecen al mismo individuo. Por el contrario, hay mucha variabilidad entre individuos. library(ggplot2) fev = read.table(&#39;fev.txt&#39;,header = T) fev$drug = as.factor(fev$drug) levels(fev$drug) = c(&#39;A&#39;,&#39;B&#39;,&#39;Placebo&#39;) p &lt;- ggplot(data = fev, aes(x = hour, y = fev, group = patient)) p + geom_line() + facet_grid(. ~ drug) + ylab(&#39;Volumen espiratorio forzado (litros por segundo)&#39;) + xlab(&#39;tiempo (horas)&#39;) Figure 9.2: Datos de capacidad respiratoria. Mediciones del volumen espiratorio forzado por hora de los pacientes en cada uno de los grupos. 9.2 Datos por cluster (correlacionados) En ambos estudios se tienen datos que pueden estar correlacionados. Este tipo de datos son llamados por cluster (jerárquicos o multinivel) dado que pueden ordenarse jerárquicamente. Por ejemplo: En el estudio del peso de las ratas, se espera que las crías que pertenecen a la misma camada (madre) estén correlacionados. Por lo que el cluster está definido por la camada. En el estudio sobre capacidad pulmonar, las mediciones que pertenecen al mismo individuo pueden estar correlacionadas. Por lo que el paciente define el cluster. En un estudio sobre resultados de estudiantes de una muestra de escuelas se puede tener correlaciones entre estudiantes de la misma clase, o de estudiantes que pertenecen a la misma escuela. Por lo que se tienen tres niveles: escuelas - salones - estudiantes. 9.3 Notación Para el cluster \\(i\\), se tienen \\(n_i\\) observaciones. Por lo que ahora tenemos un vector respuesta para el \\(i\\)-ésimo cluster: \\(\\boldsymbol y_i = (y_{i1},\\ldots,y_{in_i})&#39;\\), para \\(i=1,\\ldots,N\\). Sea \\(\\boldsymbol x_{ij}\\) el vector \\(p\\)-dimensional de covariables asociadas a la observación \\(y_{ij}\\). 9.4 Modelos con intercepto aleatorios Definimos a \\(y_{ij}\\) como el peso de la \\(j\\)-ésima cría de la camada \\(i\\). Por lo que, \\(\\boldsymbol y_i\\) corresponde al vector de pesos de las crías de la madre \\(i\\). Para inducir correlación entre las observaciones (pesos de las crías) del mismo cluster (madre) se puede incluir un efecto aleatorio en el modelo lineal: \\[ y_{ij} = \\beta_0 + b_i + \\mbox{sex}_{ij}\\beta_1+ \\mbox{treat1}_{i}\\beta_2 + \\mbox{treat2}_{i}\\beta_3 + \\varepsilon_{ij}, \\] donde sex\\(_{ij}=1\\) si la \\(j\\)-ésima cría de la camada \\(i\\) es macho, sex\\(_{ij}=1\\) si lo contrario; treat1\\(_{i}=1\\) si la madre \\(i\\) recibió la dosis baja, treat1\\(_{i}=0\\) si lo contrario; treat2\\(_{i}=1\\) si la madre \\(i\\) recibió la dosis alta, treat2\\(_{i}=0\\) si lo contrario. Además, asumimos que \\(b_i\\) y \\(\\varepsilon_{ij}\\) son independientes y \\(b_i \\sim N(0,d)\\) y \\(\\varepsilon_{ij} \\sim N(0,\\sigma^2)\\). Por lo que condicionalmente a \\(b_i\\), se tiene que: \\[ E(y_{ij} | b_i)= \\beta_0 + b_i + \\mbox{sex}_{ij}\\beta_1+ \\mbox{treat}_{1i}\\beta_2 + \\mbox{treat}_{2i}\\beta_3 \\mbox{ y } v(y_{ij} | b_i)= \\sigma^{2}. \\] Mientras que, marginalmente: \\[ E(y_{ij}) = E\\left[ E(y_{ij} | b_i) \\right] = \\beta_0 + \\mbox{sex}_{ij}\\beta_1+ \\mbox{treat}_{1i}\\beta_2 + \\mbox{treat}_{2i}\\beta_3, \\] y \\[ V(y_{ij}) = E\\left[ V(y_{ij} | b_i) \\right] + V\\left[ E(y_{ij} | b_i) \\right] = \\sigma^{2} + d. \\] Además, \\[ cov(y_{ij},y_{ik}) = d \\mbox{ y } cov(y_{i^{*}j},y_{ik}) = 0. \\] Esto quiere decir que los pesos de las crías que pertenecen a la misma camada están correlacionadas, con \\(cor(y_{ij},y_{ik}) = d/(\\sigma^2+d)\\). La matriz de covarianza y correlación de \\(y_{ij}\\) son: \\[ \\boldsymbol V_{i} = \\begin{pmatrix} \\sigma^{2} + d &amp; d &amp; d &amp; \\ldots &amp; d \\\\ d &amp; \\sigma^{2} + d &amp; d &amp; \\ldots &amp; d \\\\ d &amp; d &amp; \\sigma^{2} + d &amp; \\ldots &amp; d \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ d &amp; d &amp; d &amp; \\ldots &amp; \\sigma^{2} + d \\\\ \\end{pmatrix} \\mbox{ y } \\boldsymbol R_{i} = \\begin{pmatrix} 1 &amp; \\frac{d}{\\sigma^{2} + d} &amp; \\frac{d}{\\sigma^{2} + d} &amp; \\ldots &amp; \\frac{d}{\\sigma^{2} + d} \\\\ \\frac{d}{\\sigma^{2} + d} &amp; 1 &amp; \\frac{d}{\\sigma^{2} + d} &amp; \\ldots &amp; \\frac{d}{\\sigma^{2} + d} \\\\ \\frac{d}{\\sigma^{2} + d} &amp; \\frac{d}{\\sigma^{2} + d} &amp; 1 &amp; \\ldots &amp; \\frac{d}{\\sigma^{2} + d} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{d}{\\sigma^{2} + d} &amp; \\frac{d}{\\sigma^{2} + d} &amp; \\frac{d}{\\sigma^{2} + d} &amp; \\ldots &amp; 1 \\\\ \\end{pmatrix}. \\] Mientras que se asume que hay incorrelación entre los pesos de crías de diferente camada. 9.4.1 Modelo con pendiente aleatoria Sea \\(y_{ij}\\) la diferencia del FEV del \\(j\\)-ésimo paciente antes de iniciar el tratamiento y en la hora \\(t_{j}\\), para \\(t_{j}= \\{1,2,\\ldots,8\\}\\). Por lo tanto, \\(\\boldsymbol y_i\\) corresponde al vector de mediciones del FEV del paciente \\(i\\) a lo largo del tiempo. En este caso el modelo de intercepto aleatorio no es conveniente, puesto que este asume que todas las correlaciones de las observaciones del mismo cluster son iguales. Con datos de longitudinales lo que se espera que la correlación dependa de la distancia en el tiempo en que se hayan tomado las observaciones. Por ejemplo, \\(cor(y_{i1}, y_{i2}) &gt; cor(y_{i1}, y_{i3}) &gt; cor(y_{i1}, y_{i4})\\). En este caso, se puede proponer un modelo con pendiente aleatoria: \\[ y_{ij} = \\beta_{0} + b_{0i} + \\mbox{drugA}_{i}\\beta_{2} + \\mbox{drugB}_{i}\\beta_{3} + t_j \\beta_4 + t_jb_{1i} + \\mbox{drugA}_{i}t_j\\beta_{5} + \\mbox{drugB}_{i}t_j\\beta_{6} + \\varepsilon_{ij}, \\] donde drugA\\(_{i}=1\\) si el paciente \\(i\\) recibió la droga A, drugA\\(_{i}=0\\) si lo contrario; donde drugB\\(_{i}=1\\) si el paciente \\(i\\) recibió la droga B, drugB\\(_{i}=0\\) si lo contrario; \\(\\boldsymbol b_i = (b_{0i},b_{1i}) \\sim N(\\boldsymbol 0,\\boldsymbol D)\\) y \\(\\varepsilon_{ij} \\sim N(0,\\sigma^2)\\). Además, asumimos que \\(\\boldsymbol b_i\\) y \\(\\varepsilon_{ij}\\) son independientes. En este caso, se tiene que el valor esperado marginal es: \\[ E(y_{ij}) = \\beta_{0} + \\mbox{drugA}_{i}\\beta_{2} + \\mbox{drugB}_{i}\\beta_{3} + t_j \\beta_4 + \\mbox{drugA}_{i}t_j\\beta_{5} + \\mbox{drugB}_{i}t_j\\beta_{6}. \\] La varianza marginal es: \\[ V(y_{ij}) = V(b_{0i} + t_{j}b_{1i} + \\varepsilon_{ij}) = d_{11}t_{j}^2 + 2d_{01}t_{j} + d_{00} + \\sigma^{2}. \\] Finalmente, la covarianza es: \\[ Cov(y_{ij},y_{ik}) = Cov(b_{0i} + t_{j}b_{1i} + \\varepsilon_{ij},b_{0i} + t_{j}b_{1i} + \\varepsilon_{ik}) = d_{11}t_{j}t_{k} + d_{01}(t_{j}+t_{k}) + d_{00}. \\] Aquí vemos que al incluir una pendiente aleatoria, el modelo permite heterogeneidad de varianza y la correlación depende de los tiempos en que se tomaron las mediciones. 9.5 Modelo lineal mixto De forma general, el modelo lineal mixto se puede expresar de la siguiente forma: \\[ \\boldsymbol y_{i} = \\boldsymbol X_{i}\\boldsymbol \\beta+ \\boldsymbol Z_{i}\\boldsymbol b_{i} + \\boldsymbol \\varepsilon_i. \\] donde \\(\\boldsymbol X_i\\) y \\(\\boldsymbol Z_i\\) son matrices de dimensión \\(n \\times p\\) y \\(n \\times q\\), \\(\\boldsymbol b_i\\) es el vector \\(q\\)-dimensional de efectos aleatorios, y \\(\\boldsymbol \\varepsilon_i\\) es el vector \\(n\\)-dimensional de errores asociados al \\(i\\)-ésimo cluster. Además, se asume que: \\[ \\boldsymbol b_{i}\\sim N(\\boldsymbol 0,\\boldsymbol D) \\mbox{ y }\\boldsymbol \\varepsilon_{i} \\sim N(\\boldsymbol 0,\\sigma^{2}\\boldsymbol I), \\] y que \\(\\boldsymbol b_{i}\\) y \\(\\boldsymbol \\varepsilon_{i}\\) son independientes. Por lo tanto, marginalmente tenemos que: \\[ E(\\boldsymbol y_{i}) = \\boldsymbol X_{i}\\boldsymbol \\beta\\mbox{ y } V(\\boldsymbol y_{i}) = \\boldsymbol V_{i} = \\boldsymbol Z_{i}\\boldsymbol D\\boldsymbol Z_{i}&#39; + \\sigma^{2}\\boldsymbol I. \\] Aquí estamos asumiendo independencia condicional. Esto es, \\(V(\\boldsymbol \\varepsilon_{i}) = \\sigma^{2}\\boldsymbol I\\). Sin embargo, se puede agregar una estructura a la componente de varianza del error, \\(V(\\boldsymbol \\varepsilon_{i}) =\\boldsymbol \\Sigma_i\\). 9.6 Estimación de los parámetros La estimación de los parámetros se hace a través de máxima verosimilitud. Sea \\(\\boldsymbol \\theta= (\\boldsymbol \\beta&#39;,\\boldsymbol \\alpha)&#39;\\), donde \\(\\boldsymbol \\alpha\\) son los parámetros asociados a \\(\\boldsymbol D\\) y \\(\\boldsymbol \\Sigma_i\\). La función de log-verosimilitud está defina como: \\[\\begin{equation} \\ell(\\boldsymbol \\theta) \\propto \\sum_{i=1}^{N} \\left[ -\\frac{1}{2}\\log |\\boldsymbol V_i(\\boldsymbol \\alpha)| - \\frac{1}{2}(\\boldsymbol y_{i} - \\boldsymbol X_{i}\\boldsymbol \\beta)&#39;\\boldsymbol V_i(\\boldsymbol \\alpha)^{-1}(\\boldsymbol y_{i} - \\boldsymbol X_{i}\\boldsymbol \\beta) \\right]. \\tag{9.1} \\end{equation}\\] Se define \\(\\boldsymbol V_i=\\boldsymbol V_i(\\boldsymbol \\alpha)\\) para enfatizar que la matriz de varianza marginal depende de los parámetros contenidos en \\(\\boldsymbol \\alpha\\). No hay solución analítica para maximizar (9.1), por lo que la estimación se hace por métodos iterativos (algoritmos basados en Newton-Raphson). Otra alternativa es a través de máxima verosimilitud restringida (REML). Este método proporciona estimaciones para \\(\\boldsymbol \\alpha\\) con menor sesgo. De todas formas, tanto las estimaciones por ML y REML son consistentes, eficientes y asintóticamente normales. Para ajustar un modelo lineal mixto en R podemos utilizar la función lme de la librería nlme. 9.6.1 Estimación del modelo con intercepto aleatorio Para los datos del peso de las ratas, el ajuste del modelo por REML lo podemos hacer de la siguiente forma: library(nlme) mod.ratpup = lme(weight~sex+treatment, random=~1|litter,data=ratpup) summary(mod.ratpup) ## Linear mixed-effects model fit by REML ## Data: ratpup ## AIC BIC logLik ## 431.6577 454.23 -209.8288 ## ## Random effects: ## Formula: ~1 | litter ## (Intercept) Residual ## StdDev: 0.5708847 0.4044791 ## ## Fixed effects: weight ~ sex + treatment ## Value Std.Error DF t-value p-value ## (Intercept) 6.244974 0.18668230 294 33.45241 0.0000 ## sexMale 0.361273 0.04779857 294 7.55823 0.0000 ## treatmentHigh -0.354683 0.28930612 24 -1.22598 0.2321 ## treatmentLow -0.374705 0.26172392 24 -1.43168 0.1651 ## Correlation: ## (Intr) sexMal trtmnH ## sexMale -0.150 ## treatmentHigh -0.633 0.017 ## treatmentLow -0.701 0.026 0.450 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -7.46481761 -0.45010149 0.01750892 0.54197117 3.13826976 ## ## Number of Observations: 322 ## Number of Groups: 27 A través del argumento random=~1|litter estamos indicando que hay un intercepto aleatorio asociado a la variable litter (camada). La varianza total del peso de las crías es \\(0.164+0.326=0.49\\). Mientras que, la correlación entre crías de la misma camada es: \\[ \\rho_{I} = \\frac{0.326}{0.326+0.164} = 0.665. \\] Ahora, los resultado de los efectos fijos muestran que el peso de las crías machos tienen un peso significativamente mayor que el de las crías hembras. Evaluando los coeficientes asociados a los tratamiento, se observa que son negativos. Sin embargo, estos efectos parecen ser no significativos (se tiene que confirma con una prueba conjunta). Este último resultado es contrario al que se obtiene con el modelo lineal (sin asumir correlación entre las observaciones, usando la función lm). Para evaluar si los tratamientos tienen un efecto significativo, se plantea el siguiente sistema de hipótesis: \\[ H_0: \\beta_2 = \\beta_3 = 0. \\] La prueba de hipótesis la podemos realizar a través de la prueba de razón de verosimilud de la siguiente forma: mod.ratpup0 = lme(weight~sex, random=~1|litter,data=ratpup,method=&#39;ML&#39;) mod.ratpup1 = lme(weight~sex+treatment, random=~1|litter,data=ratpup,method=&#39;ML&#39;) anova(mod.ratpup0,mod.ratpup1) ## Model df AIC BIC logLik Test L.Ratio p-value ## mod.ratpup0 1 4 421.6434 436.7416 -206.8217 ## mod.ratpup1 2 6 422.9754 445.6227 -205.4877 1 vs 2 2.667947 0.2634 Estos resultados confirmar que no hay un efecto de los tratamientos sobre el peso de las crías. Hay que tener en cuenta que para realizar una prueba de razón de verosimilitud sobre los efectos fijos (\\(\\boldsymbol \\beta\\)) es necesario estimar los parámetros por máxima verosimilitud. 9.6.2 Estimación del modelo con pendiente aleatoria Ahora, podemos estimar el modelo de pendiente aleatoria para los datos de la capacidad respiratoria de la siguiente forma: fev$fevdiff = fev$fev - fev$base fev$drug=relevel(fev$drug,&#39;Placebo&#39;) lmm.fev = lme(fevdiff~hour*drug, random=~hour|patient,data=fev) summary(lmm.fev) ## Linear mixed-effects model fit by REML ## Data: fev ## AIC BIC logLik ## 237.5878 281.0442 -108.7939 ## ## Random effects: ## Formula: ~hour | patient ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 0.51220391 (Intr) ## hour 0.06040257 -0.459 ## Residual 0.20700812 ## ## Fixed effects: fevdiff ~ hour * drug ## Value Std.Error DF t-value p-value ## (Intercept) 0.2592411 0.10961492 501 2.365016 0.0184 ## hour -0.0184772 0.01394747 501 -1.324770 0.1859 ## drugA 0.5999851 0.15501891 69 3.870400 0.0002 ## drugB 0.9422619 0.15501891 69 6.078367 0.0000 ## hour:drugA -0.0714782 0.01972470 501 -3.623791 0.0003 ## hour:drugB -0.0968105 0.01972470 501 -4.908086 0.0000 ## Correlation: ## (Intr) hour drugA drugB hr:drA ## hour -0.512 ## drugA -0.707 0.362 ## drugB -0.707 0.362 0.500 ## hour:drugA 0.362 -0.707 -0.512 -0.256 ## hour:drugB 0.362 -0.707 -0.256 -0.512 0.500 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.366026703 -0.428767946 0.009979273 0.517082034 3.142966988 ## ## Number of Observations: 576 ## Number of Groups: 72 Estos resultados muestran que, para los pacientes del grupo control, el FEV no cambia con el tiempo. Esto es esperado dado que estos pacientes no recibieron ningún medicamento que pudiera modificar su capacidad espiratoria. Mientras que, los tratamientos tienen un efecto negativo significativo sobre la capacidad espiratoria. Esto se puede observar de forma gráfica en la Figura 9.3. betaEst = lmm.fev$coefficients$fixed mean.line = data.frame(a = c(betaEst[1],betaEst[1] + betaEst[3], betaEst[1] + betaEst[4]), b= c(betaEst[2],betaEst[2] + betaEst[5], betaEst[2] + betaEst[5]), drug = c(&#39;Placebo&#39;,&#39;A&#39;,&#39;B&#39;)) p &lt;- ggplot(data = fev, aes(x = hour, y = fevdiff, group = patient)) p + geom_line() + facet_grid(. ~ drug) + ylab(&#39;diferencia del FEV (litros por segundo)&#39;) + xlab(&#39;tiempo (horas)&#39;) + geom_abline(data=mean.line,aes(intercept = a, slope = b),col=2,lwd=2) Figure 9.3: Datos de capacidad respiratoria. Estimación de la media de la diferencia del Volumen espiratorio forzado por hora (línea roja) para los pacientes en cada uno de los grupos. La estimación de la varianza y correlacion marginal se puede obtener a través de la función getVarCov. Por ejemplo, la estimación de la matriz de varianza y correlación marginal para el individuo uno es: Vest = getVarCov(lmm.fev,type=&#39;marginal&#39;) Vest ## patient 1 ## Marginal variance covariance matrix ## 1 2 3 4 5 6 7 8 ## 1 0.28045 0.22705 0.21650 0.20595 0.19539 0.18484 0.17429 0.16374 ## 2 0.22705 0.26300 0.21324 0.20634 0.19944 0.19253 0.18563 0.17873 ## 3 0.21650 0.21324 0.25284 0.20673 0.20348 0.20022 0.19697 0.19372 ## 4 0.20595 0.20634 0.20673 0.24998 0.20752 0.20792 0.20831 0.20870 ## 5 0.19539 0.19944 0.20348 0.20752 0.25442 0.21561 0.21965 0.22369 ## 6 0.18484 0.19253 0.20022 0.20792 0.21561 0.26615 0.23099 0.23868 ## 7 0.17429 0.18563 0.19697 0.20831 0.21965 0.23099 0.28518 0.25367 ## 8 0.16374 0.17873 0.19372 0.20870 0.22369 0.23868 0.25367 0.31151 ## Standard Deviations: 0.52958 0.51283 0.50283 0.49998 0.5044 0.5159 0.53402 0.55813 cov2cor(as.matrix(Vest$&#39;1&#39;)) ## 1 2 3 4 5 6 7 8 ## 1 1.0000000 0.8360136 0.8130169 0.7778053 0.7314927 0.6765661 0.6162925 0.5539758 ## 2 0.8360136 1.0000000 0.8269414 0.8047373 0.7710037 0.7277256 0.6778195 0.6244266 ## 3 0.8130169 0.8269414 1.0000000 0.8223095 0.8022765 0.7718468 0.7335290 0.6902507 ## 4 0.7778053 0.8047373 0.8223095 1.0000000 0.8228830 0.8060672 0.7801834 0.7478984 ## 5 0.7314927 0.7710037 0.8022765 0.8228830 1.0000000 0.8285640 0.8154488 0.7945883 ## 6 0.6765661 0.7277256 0.7718468 0.8060672 0.8285640 1.0000000 0.8384306 0.8289286 ## 7 0.6162925 0.6778195 0.7335290 0.7801834 0.8154488 0.8384306 1.0000000 0.8510795 ## 8 0.5539758 0.6244266 0.6902507 0.7478984 0.7945883 0.8289286 0.8510795 1.0000000 Aquí vemos que la varianza de la diferencia del FEV no tiene un cambio notable en tiempo. Con respecto a las correlaciones, estas son mayores cuando las observaciones están más cercanas en el tiempo. Este último resultado es común con datos de tipo longitudinal. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
