# Selección de variables
```{r preamble4, include=FALSE}
library(glmnet)
library(olsrr)
library(kableExtra)
library("plotrix") 

mathy.df <- data.frame(b0=c("int.","age","weight","height","neck",
     "abdom","hip","thigh","forearm","wrist"), 
b1=c(-20.062,0.059,-0.084,"",-0.432,0.877,-0.186,0.286,0.4825,-1.4049),
b2=c(10.8465,0.0285,0.0370,"",0.2080,0.0666,0.1282,0.1195,0.1725,0.4717),
b3=c(0.0656, 0.0388,0.0237,"", 0.0389, 0.0000, 0.1473, 0.1473, 0.0056, 0.0032),
b4=c(-31.297,"",0.126,"","",0.921,"","",0.446,-1.392),
b5=c( 6.7089,"",0.0229,"","",0.0519,"","",0.1682,0.4099),
b6=c(0.0000,"",0.0000,"","",0.0000,"","",0.0085,0.0008),
b7=c(0.249,0.067,"",-0.165,"",0.676,"","","",-1.937),
b8=c(6.2785, 0.0220,"", 0.0782,"",0.0312,"","","",0.3834),
b9=c( 0.9685,0.0025,"",0.0363,"",0.0000,"","","",0.0000)
)
colnames(mathy.df)<-c("","Est"," er. std.","valor-p","Est"," er. std.","valor-p","Est"," er. std.","valor-p")


```

## Ejemplos
### Unidad quirúrgica
Una unidad quirúrgica de un hospital está interesada en predecir la supervivencia de los pacientes sometidos a un tipo particular de operación hepática. Se dispuso de una selección aleatoria de $108$ pacientes para el análisis. De cada registro del paciente, se extrajo la siguiente información de la evaluación preoperatoria:

- ``bcs``: coagulación sanguínea.

- ``pindex``: índice de pronóstico.

- ``enzyme``: función enzimática.

- ``liver_test``: función hepática.

- ``age``: edad.

- ``gender``: genero (0 = masculino, 1 = femenino).

- ``alc_mod``: historial de consumo de alcohol (0 = Ninguno, 1 = Moderado).

- ``alc_heavy``: & historial de consumo de alcohol (0 = Ninguno, 1 = Fuerte).

- ``y``: tiempo de supervivencia.

El objetivo del estudio es determinar los factores que influyen sobre el tiempo de supervivencia (que se determinó posteriormente) en función de las demás variables. 

El modelo propuesto es el siguiente:
\begin{equation}
\begin{split}
\log y_{i} =& \beta_{0}+\mbox{bcs}_{i}\beta_{1} + \mbox{pindex}_{i}\beta_{2}+ \mbox{enzyme}_{i}\beta_{3} +  \mbox{liver}_{i}\beta_{4} + \mbox{age}_{i}\beta_{5} +  \mbox{gender}_{i}\beta_{6}+ \\ & \mbox{alc_mod}_{i}\beta_{7} + \mbox{alc_heavy}_{i}\beta_{8} + \varepsilon_{i}
\end{split}
\nonumber
\end{equation}

El ajuste del modelo es:
```{r surgicalFit, echo=T, warning=FALSE,message = FALSE}
library(olsrr)
data(surgical)
mod.surgical.completo = lm(log(y)~.,data=surgical)
summary(mod.surgical.completo)
```


### Grasa corporal
La medición de la grasa corporal es un proceso complejo. Dado que los músculos y los huesos son más densos, el calculo de \% de grasa corporal se basa, entre otros aspectos, en la medición de la densidad corporal la cuál requiere sumergir a las personas en el agua. Por esta razón, se quiere buscar un método más sencillo para determinarlo. Para esto, se registraron la edad, el peso, la altura y $10$ medidas de la circunferencia corporal de $252$ hombres. De igual forma, a cada uno de estos hombres se les midió el \% de grasa corporal de forma precisa (usando la ecuación de Brozek, medición a partir de la densidad).

Cómo variable respuesta se utiliza la medición por el método de Brozek, y las posibles covariables son:

- ``age``: edad (en años).

- ``weight``: peso (en libras).

- ``height``: altura (en pulgadas).

- ``neck``: circunferencia del cuello (en centímetros).

- ``chest``: circunferencia del pecho (en centímetros).

- ``abdom``: circunferencia del abdomen (en centímetros).

- ``hip``: circunferencia de la cadera (en centímetros).

- ``thigh``:circunferencia del muslo (en centímetros).

- ``knee``:circunferencia de la rodilla (en centímetros).

- ``ankle``:circunferencia del tobillo (en centímetros).

- ``biceps``: circunferencia del bíceps extendido (en centímetros).

- ``forearm``: circunferencia del antebrazo (en centímetros).

- ``wrist``: circunferencia de la muñeca (en centímetros).

El modelo propuesto es el siguiente:
\begin{equation}
\mbox{brozek}_i = \beta_{0} + \mbox{age}_i\beta_1+ \mbox{weight}_i\beta_2 + \ldots + \mbox{wrist}_i\beta_{13}  + \varepsilon_i.
(\#eq:modeloFat)
\end{equation}
El ajuste del modelo es:
```{r fatFit, echo=T, warning=FALSE,message = FALSE}
library(faraway)
data(fat)
mod.fat <- lm(brozek ~ age + weight + height + neck + chest + abdom +
             hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat)
summary(mod.fat)
```

## Problema de selección de variables
En problemas de regresión se tiene un conjunto grande de potenciales covariables. Si ajustamos un modelo considerandolas todas podemos estar incluyendo covariables que son irrelevante. Por el otro lado, si no las incluimos todas es posible que estemos omitiendo covariables importantes. En ambos casos hay consecuencias negativas.

Para illustrar esto, considere el siguiente modelo:
\begin{equation}
\begin{split}
y_{i} &= \beta_{0} + \sum_{j=1}^{p-1}\beta_{j}x_{ij} + \varepsilon_{i} \\
&= \beta_{0} + \sum_{j=1}^{r}\beta_{j}x_{ij} + \sum_{j=r+1}^{p-1}\beta_{j}x_{ij} + \varepsilon_{i} \\
&= \bx_{1i}'\bbeta_1 + \bx_{2i}'\bbeta_2 + \varepsilon_i,
\end{split}
(\#eq:modelogral)
\end{equation}
donde $\bx_{1i} = (1,x_{1i},x_{2i},\ldots,x_{ri})$, $\bx_{2i} = (x_{r+1,i},x_{r+2,i},\ldots,x_{p-1,i})$, $\bbeta_1 = (\beta_0,\beta_1,\beta_2,\ldots,\beta_r)'$, $\bbeta_2 = (\beta_{r+1},\beta_{r+2},\ldots,\beta_{p-1})'$, y $\varepsilon_i \sim N(0,\sigma^{2})$. Es decir, se hace una partición de las covariables y los coeficientes de regresión en dos componentes. 

En forma matricial, el modelo es:
\[
\by = \bX_{1}\bbeta_{1} + \bX_{2}\bbeta_{2}+ \bvarepsi,
\]
donde $\bX_{1}$ es una matriz $n \times r$ con la $i$-ésima fila igual a $\bx_{1i}$ y $\bX_{2}$ es una matriz $n \times (p-r-1)$ con la $i$-ésima fila igual a $\bx_{2i}$.

### ¿Qué pasa si ignoramos covariables importantes?
Ahora, considere que el modelo de regresión real es \@ref(eq:modelogral), pero decidimos estimar:
$$
y_{i} = \bx_{1i}'\bbeta_1 + \varepsilon_i.
$$
Por lo tanto, estamos omitiendo las covariables $\bx_{2i}$ del modelo (puesto que $\bbeta_2 \neq 0$). El estimador por MCO de $\bbeta_1$ es: 
$$
\hatbbeta_{1} = (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\by.
$$
De aquí tenemos que $E(\hatbbeta_{1}) = \bbeta_{1} + (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\bX_{2}\bbeta_{2}$. Es decir que $\hatbbeta_{1}$ es un estimador sesgado, a menos que  $\bX_{1}'\bX_{2} = \bZERO$ (las columnas de $X_{1}$ son ortogonales a las columnas de $X_{2}$). 

De igual forma, las predicciones también serán sesgadas. La predicción en el punto $\bx_{01}$ es:
$$
\haty_{0} = \bx_{01}'\hatbbeta_{1}.
$$
Su valor esperado es:
$$
E(\haty_{0}) = \bx_{01}'\bbeta_{1} + \bx_{01}'(\bX_{1}'\bX_{1})^{-1}\bX_{1}'\bX_{2}\bbeta_{2} \neq \bx_{01}'\beta_{1} + \bx_{02}'\beta_{2}.
$$
Por lo tanto, si omitimos variables relevantes obtenemos sesgo en las estimaciones.

### ¿Que pasa si incluimos covariables irrelevantes?
Ahora, consideremos el caso en que $\bbeta_2=0$, es decir, las covariables $\bx_{2}$ no tienen un aporte significativo en el modelo. Pero decidimos estimar el modelo completo.

En este caso, el estimador de $\bbeta$ es:
$$
\hatbbeta = (\bX'\bX)^{-1}\bX'\by = \begin{pmatrix}
\bX_{1}'\bX_{1} & \bX_{1}\bX_{2} \\ \bX_{2}'\bX_{1} & \bX_{2}'\bX_{2}
\end{pmatrix}^{-1} \begin{pmatrix}
\bX_{1}' \\ \bX_{2}'
\end{pmatrix}\by.
$$
El Valor esperado de $\hatbbeta$ es:
\begin{equation}
\begin{split}
E(\hatbbeta) =& (\bX'\bX)^{-1}\bX'E(\by) = (\bX'\bX)^{-1}\bX'\bX_{1}\bbeta_1 \\
 = & (\bX'\bX)^{-1}\bX'(\bX_{1} \ \bX_{2}) \begin{pmatrix}
 \bbeta_1 \\ \bZERO
 \end{pmatrix} = \begin{pmatrix}
 \bbeta_1 \\ \bZERO
 \end{pmatrix}.
\end{split}
\nonumber
\end{equation}
Es decir que $\hatbbeta$ es un estimador insesgado. 

La varianza de $\hatbbeta$ es:
\begin{equation}
\begin{split}
V(\hatbbeta) =& \sigma^{2}(\bX'\bX)^{-1} = \sigma^{2}\begin{pmatrix}
\bX_{1}'\bX_{1} & \bX_{1}\bX_{2} \\ \bX_{2}'\bX_{1} & \bX_{2}'\bX_{2}
\end{pmatrix}^{-1} \\
=& \sigma^{2} \begin{pmatrix}
(\bX_{1}'\bX_{1})^{-1} + \bL\bM\bL & - \bL\bM \\
-\bM\bL' & \bM
\end{pmatrix},
\end{split}
\nonumber
\end{equation}
donde $\bL= (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\bX_{2}$ y $\bM = \bX_{2}'(\bI - \bH_{1})\bX_{2}$. Particularmente, para $\hatbbeta_1$ tenemos que:
$$
V(\hatbbeta_{1}) = \sigma^{2} \left[ (\bX_{1}'\bX_{1})^{-1} + \bL\bM\bL \right].
$$
Dado que $\bM$ (y por lo tanto $\bL\bM\bL$) es positiva-definida, la varianza de $\hatbbeta_{1}$ se infla al incluir las covariables irrelevantes al modelo. La única excepción es cuando $\bX_{1}$ y $\bX_{2}$ son ortogonales ($\bX_{1}'\bX_{2} = \bZERO$).

De igual forma, las predicciones en el punto $\bx_{0}' = (\bx_{01}' \ \bx_{02}')$ son insesgadas:
$$
E(\haty_{0}) = E(\bx_{0}'\hatbbeta) = (\bx_{01}' \ \bx_{02}')\begin{pmatrix}
\bbeta_{1} \\ \bZERO
\end{pmatrix} = \bx_{01}'\bbeta_{1}.
$$
En conclusión:

- Cuando omitimos covariables relevantes, obtenemos sesgos en las estimaciones.

- Cuando incluimos covariables irrelevantes, se inflan las varianzas de las estimaciones. Adicionalmente, incluir más covariables puede llevar a problemas de multicolinealidad.


## Métodos para la selección de variables
Si tenemos $(p-1)$ covariables, entonces tenemos $(p-1)^2$ potenciales modelos. Por lo que podemos ajustar todos los posibles modelos y hacer una comparación entre ellos usando algunos criterios de decisión.

Existen varios criterios para determinar que modelo es "mejor" que otro y este debe escogerse teniendo en cuenta cuál es el objetivo que se tiene al ajustar el modelo (descripción de la relación, predicción, control, etc.). Algunos de estos criterios son el coeficiente de determinación ($R^{2}$ y $R^{2}_{adj}$), el estadístico $C_{p}$ de Mallows, el estadístico PRESS y el $R^{2}$ de predicción, los criterios de información (AIC y BIC).

### Coeficiente de determinación
Esté indicador está definido como:
$$
R^{2} = \frac{\Sreg}{\Stotal} = 1 - \frac{\Sres}{\Stotal}.
$$
El $R^{2}$ cuantifica la cantidad de variabilidad de la variable respuesta que es explicada por el modelo. Se tiene que $0 \leq R^{2} \leq 1$. Valores más cercanos a $1$ implican que el modelo explica gran parte de la variabilidad de $y$. Hay que tener en cuenta que el $R^{2}$ siempre crece a medida que se adicionan más covariables al modelo. Por lo tanto, se puede puede agregar regresores hasta el punto en que una covariable adicional no propociona un aumento considerable en el $R^{2}$.

### Coeficiente de determinación ajustado
Para evitar el incoviente del $R^{2}$, se puede utlizar el el coeficiente de determinación ajustado definido como:
$$
R^{2}_{adj} = 1 - \frac{n-1}{n-p}\frac{\Sres}{\Stotal} = 1- \frac{\MSres}{\Stotal/(n-1)} = 1- \frac{n-1}{n-p}(1-R^{2}).
$$
El $R^{2}_{adj}$ no necesariamente aumenta al adicionar nuevos términos al modelo. Este solo aumenta si hay una disminución del $\MSres$.

### C$_p$ de Mallows
Mallows propone un criterio basado en el error cuadrático medio (ECM) de $\haty_i$, esto es:
$$
E[\haty_{i}- E(y_{i})]^2 = [E(y_{i}) - E(\haty_{i})]^2 + V(\haty_{i}),
$$
donde $E(y_{i})$ es el valor esperado de la respuesta ('modelo real'), y $E(\haty_{i})$ es el valor esperado de la respuesta basado en el modelo propuesto (basado en $p$ covariables).

El ECM total estandarizado está definido como:
\begin{equation}
\begin{split}
\Gamma_{p} =& \frac{1}{\sigma^{2}}\left\{\sum_{i=1}^{n}[E(y_{i}) - E(\haty_{i})]^2 + \sum_{i=1}^{n}  V(\haty_{i}) \right\} \\
=&  \frac{1}{\sigma^{2}}\left\{SS_{B}(p)  + \sum_{i=1}^{n}  V(\haty_{i}) \right\} = \frac{1}{\sigma^{2}}\left\{SS_{B}(p)  + p\sigma^{2} \right\} \\
=& \frac{1}{\sigma^{2}}\left\{ E[\Sres(p)] - (n-p)\sigma^{2} + p\sigma^{2} \right\} \\ =& \frac{E[\Sres(p)]}{\sigma^{2}} - n + 2p.
\end{split}
\nonumber
\end{equation}

Reemplazando $E[\Sres(p)]$ por $\Sres(p)$, y asumiendo que $\MSres(p^{*})$ (calculado usando el modelo completo) es un buen estimador de $\sigma^{2}$:
\[
C_{p} = \frac{\Sres(p)}{\MSres(p^{*})} - n + 2p.
\]
Por lo tanto, para el modelo completo $C_{p} = p^{*}$. Si $E[\Sres(p)] = (n-p)\sigma^{2}$ (asumiendo que $SS_{B}(p)=0$), tenemos que:
\[
E[C_{p}| \mbox{Sesgo}=0] = \frac{(n-p)\sigma^{2}}{\sigma^{2}} - n +2p = p.
\]
Si el modelo propuesto es insesgado se espera que el $C_p$ esté cercano a $p$. Aunque se espera que el $C_p=p$, es deseable que $C_p < p$. Por lo tanto, modelos con valores pequeños de $C_p$ son mejores. 

### Estadístico PRESS
El estadístico PRESS (prediction error sum of squares) está definido como: 
$$
\mbox{PRESS} = \sum_{i=1}^{n} (y_{i} - \haty_{(i)})^{2} = \sum_{i=1}^{n} \left( \frac{e_{i}}{1-h_{ii}} \right)^{2}.
$$
Para comparar modelos, menor valor del PRESS indica que el modelo es mejor para hacer predicciones. A partir del PRESS se puede calcular el $R^{2}$ de predicción:
$$
R^{2}_{pred} = 1 - \frac{PRESS}{SST}.
$$
Basado en este criterio, mayor es el valor de $R^{2}_{pred}$ mejor es el modelo para hacer predicciones. La ventaja del PRESS y $R_{pred}^{2}$ es que evitan el sobreajuste dado que se calculan utilizando observaciones no incluidas en la estimación del modelo.

### Criterios de información
La idea es comparar modelos estimados teniendo en cuenta la bondad de ajuste del modelo (verosimilitud, $L$) y su complejidad (número de parámetros). El criterio de información de Akaike está definido como:
$$
\mbox{AIC} = -2\log (L) + 2p.
$$
El criterio de información bayesiano (o de Schwarz - SBC):
$$
\mbox{BIC} = -2\log (L) + p\log n.
$$
Es preferible modelos con valores menores de AIC o BIC. Dado que la penalización del BIC es mayor (si $n > 7$), este indicador prefiere modelos con menor número de covariables.

Recordemos que la log-verosimilitud es:
$$
\log L(\bbeta,\sigma^{2}) = - \frac{n}{2}\log (2\pi) - n\log(\sigma) - \frac{1}{2\sigma^{2}}(\by - \bX\bbeta)'(\by-\bX\bbeta).
$$
El estimador por máxima verosimilitud de $\sigma^{2}$ es $\hatsigma=\Sres/n$. Por lo tanto, el máximo valor de la log-verosimilitud es:
$$
\log L(\hatbbeta,\hatsigma^{2}) = -\frac{n}{2}\log (2\pi) - \frac{n}{2}\log\hatsigma^{2} - \frac{1}{2\hatsigma^{2}}\Sres = -\frac{n}{2}\log (\Sres/n) + \mbox{constante}.
$$
Por lo tanto:
$$
AIC \propto n\log(\Sres/n) + 2p \mbox{ y } BIC \propto n\log(\Sres/n) + p\log n.
$$
Hay varias adaptaciones de estos criterios de información definiendo diferentes penalizaciones.

## Comparación de los modelos
### Todos los posibles modelos
Con la función ``ols_step_all_possible()`` de la librería ``olsrr`` es posible ajustar todos posibles modelos y determinar el mejor bajo diferentes criterios. Otra alternativa es la función ``regsubsets()`` de la librería ``leaps``. Está función es más rápida (se basa en un algoritmo más eficiente), pero no es user-friendly.

#### Datos de unidad quirúrgica
A través de la función ``ols_step_all_possible`` podemos ajustar los $255$ modelos que se pueden ajustar usando las ocho posibles covariables:

```{r surgicalFitAll, echo=T, warning=FALSE,message = FALSE}
surgical.all.mods=ols_step_all_possible(mod.surgical.completo)
```
Además de ajustar los modelos, se calculan varios criterios ($R^{2}$,$R^{2}_{adj}$, $R^{2}_{pred}$,AIC,BIC,...) para cada uno de ellos.

Puesto que son muchos modelos, podemos organizar los resultados de tal forma que obtengamos los mejores modelos basándonos en cada uno de los criterios. Por ejemplo, los 5 mejores ajustes según el $R^{2}_{adj}$ son:

```{r surgicalFitAll2, echo=T, warning=FALSE,message = FALSE}
R2adj.order = order(surgical.all.mods$adjr,decreasing = T)
as.data.frame(surgical.all.mods)[R2adj.order[1:5],c(2:8,10)]
```
Basándonos en este criterio el mejor ajuste se obtiene considerando las covariables ``bcs``, ``pindex``, ``enzyme_test``, ``age``, ``gender``, y ``alc_heavy``. Es decir, eliminando las covariables función hepática y consumo de alcohol moderado. Note que no todos los demás críterios sugieren el mismo modelo. Si eliminamos las covariables ``gender`` obtenemos un modelo con un $R^{2}_{pred}$ más alto.

Ahora, si nos apoyamos en el AIC, los mejores 5 ajustes son: 
```{r surgicalFitAll3, echo=T, warning=FALSE,message = FALSE}
AIC.order = order(surgical.all.mods$aic,decreasing = F)
as.data.frame(surgical.all.mods)[AIC.order[1:5],c(2:8,10)]
```
Con este críterio se escoge el mismo modelo que con el $R^{2}_{adj}$. Sin embargo, podemos observar que el BIC sugiere eliminar la covariable asociada a la edad. 

En la Figura 4.2 muestra los $R^{2}$,$R^{2}_{adj}$, $R^{2}_{pred}$,C$_p$, AIC y BIC (SBC) para todos los posibles ajustes. Note que, dentro de cada subgrupo de modelos (determinado por el número de covariables), los criterios eligen los modelos en el mismo orden. La diferencia está en el número de covariables a elegir. Generalmente, el BIC prefiere modelos más parsimoniosos. Esto no ocurre con criterios de validación cruzada, como el PRESS o $R^{2}_{pred}$.

```{r surgicalAll, echo=T, fig.height = 5, fig.width = 7,fig.align = "center",fig.cap = "Valores de los criterios de selección calculados para cada uno de todos los posibles modelos.",warning=FALSE,message = FALSE}
par(mfrow=c(1,2))
plot(surgical.all.mods)
```

Teniendo en cuenta esto, con la función ``ols_step_best_subset()`` selecciona el mejor modelo para cada subconjunto de número de covariables basándose en los diferentes criterios:

```{r surgicalFitAll4, echo=T, warning=FALSE,message = FALSE}
ols_step_best_subset(mod.surgical.completo)
```

A partir de estos resultados, y con la ayuda de expertos en el tema, se puede hacer una selección del mejor modelo para hacer las predicciónes.

### Algorítmos de selección
Para el proceso de selección, la mejor opción es evaluar todos los posibles modelos. Sin embargo, en la presencia de muchas posibles covariables este proceso puede requerir una carga computacional muy alta. Por esta razón, se han desarrollado varios algoritmos para evaluar solo un subconjunto de modelos agregando o eliminando covariables una a la vez.


#### Selección hacia delante (forward selection)
Este algoritmo parte del modelo sin ninguna covariable (es decir, solo el intercepto) y el ajuste ``óptimo'' se encuentra ingresando covariables una a la vez basándose en algún criterio (por ejemplo AIC).

La primera covariable se escoge luego de ajustar los $(p-1)$ modelos simples con cada uno de los regresores. Por ejemplo, seleccionado la covariable que proporciona el mejor AIC.

Luego se ajustan los modelos combinando la covariable previamente seleccionada con cada una de los restantes $(p-2)$ regresores. Si el mejor ajuste con dos covariables proporcina un menor AIC que en el paso anterior, continuamos seleccionando la tercer covariable de la misma forma. 

El algoritmo continua seleccionando covariables hasta que se satisface un criterio de parada (por ejemplo, hasta que el AIC aumente).

#### Selección hacia atrás (backward selection)
Aquí se empieza evaluando el modelo con todas las covariables candidatas y se van eliminando covariables una a una hasta que un criterio de parada se satisface (por ejemplo, hasta que el AIC aumenta).

#### Selección por segmentos (stepwise selection)
Aquí se siguen los mismos pasos que la selección hacia delante. Pero en cada paso se evalúan de nuevo los candidatos que ya habían ingresado en el modelo. Por lo tanto, una covariable que ya esté en el modelo puede ser eliminada en algún paso posterior.


#### Unidad quirúrgica
Consideremos el modelo anterior adicionando las interacciones de las covariables continuas con las categóricas:
\begin{equation}
\begin{split}
\log y_{i} =& \beta_{0}+\mbox{bcs}_{i}\beta_{1} + \mbox{pindex}_{i}\beta_{2}+ \mbox{enzyme}_{i}\beta_{3} +  \mbox{liver}_{i}\beta_{4} + \mbox{age}_{i}\beta_{5} +  \mbox{gender}_{i}\beta_{6}+ \mbox{alc_mod}_{i}\beta_{7} + \\ &  \mbox{alc_heavy}_{i}\beta_{8} \mbox{bcs}_{i}\mbox{gender}_{i}\beta_{9} + \mbox{pindex}_{i}\mbox{gender}_{i}\beta_{10} + \mbox{enzyme}_{i}\mbox{gender}_{i}\beta_{11} + \mbox{liver}_{i}\mbox{gender}_{i}\beta_{12}  + \\ & \mbox{age}_{i}\mbox{gender}_{i}\beta_{13} + \mbox{bcs}_{i}\mbox{alc_mod}_{i}\beta_{14} + \mbox{pindex}_{i}\mbox{alc_mod}_{i}\beta_{15} + \mbox{enzyme}_{i}\mbox{alc_mod}_{i}\beta_{16} +  \\ & \mbox{liver}_{i}\mbox{alc_mod}_{i}\beta_{17} + \mbox{age}_{i}\mbox{alc_mod}_{i}\beta_{18} + \mbox{bcs}_{i}\mbox{alc_heavy}_{i}\beta_{19} + \mbox{pindex}_{i}\mbox{alc_heavy}_{i}\beta_{20} + \\ & \mbox{enzyme}_{i}\mbox{alc_heavy}_{i}\beta_{21} + \mbox{liver}_{i}\mbox{alc_heavy}_{i}\beta_{22} + \mbox{age}_{i}\mbox{alc_heavy}_{i}\beta_{23} + \varepsilon_{i}.
\end{split}
\nonumber
\end{equation}
En este caso tenemos $2^{23}=8'388,608$ posibles modelos. Lo que hace que sea difícil ajustarlos todos (aunque es posible usando la librería ``leaps``). Por lo tanto, vamos a utilizar los algortimos de selección.

**Selección hacia delante**. Podemos utilizar la función ``ols_step_forward_aic`` de la librería ``olsrr``:

```{r surgicalForward, echo=T, warning=FALSE,message = FALSE}
mod.surgical.completo2 = lm(log(y)~bcs*gender+pindex*gender+enzyme_test*gender+liver_test*gender+age*gender+ bcs*alc_mod+pindex*alc_mod+enzyme_test*alc_mod+liver_test*alc_mod+age*alc_mod+bcs*alc_heavy+pindex*alc_heavy+enzyme_test*alc_heavy+liver_test*alc_heavy+age*alc_heavy,data=surgical)
ols_step_forward_aic(mod.surgical.completo2,details = F)
```
Con el argumento ``details = T`` se puede ver la selección con más detalle. Usando este algoritmo el modelo óptimo es:
\begin{equation}
\begin{split}
\log y_{i} =& \beta_{0} + \mbox{bcs}_{i}\beta_{1} + \mbox{pindex}_{i}\beta_{2} + \mbox{enzyme}_{i}\beta_{3} + \mbox{age}_{i}\beta_{4} + \mbox{gender}_{i}\beta_{5} + \\
&\mbox{gender}_{i}\mbox{pindex}_{i}\beta_{6} + \mbox{gender}_{i}\mbox{enzyme}_{i}\beta_{7} + \mbox{bcs}_{i}\mbox{alc_heavy}_{i}\beta_{8} + \varepsilon_{i}.
\end{split}
\nonumber
\end{equation}

**Selección hacia atrás**. Podemos utilizar la función ``ols_step_backward_aic`` de la librería ``olsrr``:

```{r surgicalBackward, echo=T, warning=FALSE,message = FALSE}
ols_step_backward_aic(mod.surgical.completo2,details = F)
```
Por lo tanto, el modelo seleccionado es:
\begin{equation}
\begin{split}
\log y_{i} =& \beta_{0} + \mbox{bcs}_{i}\beta_{1} + \mbox{pindex}_{i}\beta_{2} + \mbox{enzyme}_{i}\beta_{3} + \mbox{gender}_{i}\beta_{4} + \mbox{liver}_{i}\beta_{5} + \\
& \mbox{gender}_{i}\mbox{enzyme}_{i}\beta_{6} + \mbox{enzyme}_{i}\mbox{alc_mod}_{i}\beta_{7} + \mbox{liver}_{i}\mbox{alc_mod}_{i}\beta_{8} + \mbox{liver}_{i}\mbox{gender}_{i}\beta_{9} + \varepsilon_{i}.
\end{split}
\nonumber
\end{equation}
Con este algoritmo no se considera la edad del paciente pero si la función hepática y otras interacciones.

**Selección por segmentos**. Aquí tenemos la función ``ols_step_both_aic`` de la librería ``olsrr``:

```{r surgicalBoth, echo=T, warning=FALSE,message = FALSE}
ols_step_both_aic(mod.surgical.completo2,details = F)
```
Note que este método sigue los mismos pasos que la selección hacia delante hasta el paso 8 donde se elimina la interacción entre el índice de pronostico y el genero. Por lo que aquí obtenemos el siguiente modelo:

\begin{equation}
\begin{split}
\log y_{i} =& \beta_{0} + \mbox{bcs}_{i}\beta_{1} + \mbox{pindex}_{i}\beta_{2} + \mbox{enzyme}_{i}\beta_{3} + \mbox{age}_{i}\beta_{4} + \mbox{gender}_{i}\beta_{5} + \\
& \mbox{gender}_{i}\mbox{enzyme}_{i}\beta_{6} + \mbox{bcs}_{i}\mbox{alc_heavy}_{i}\beta_{7} + \varepsilon_{i}.
\end{split}
\nonumber
\end{equation}

Dado que los algoritmos hacen la busqueda del modelo "óptimo" evaluando diferentes subconjuntos de covariables, se obtuvieron diferentes ajustes. Si observamos el AIC de las tres opciones, el modelo obtenido con el algoritmo stepwise presenta el mejor resultado.

## Regresión de LASSO
Otra alternativa para encontrar las variables relevantes es a través del estimador de LASSO (Least Absolute Selection and Shrinkage Operator). Este es un método de regularización que se implementa se tiene cientos de covariables disponibles y se cree que pocas tienen un aporte relevante (*sparsity principle*). Por ejemplo, en estudios de genética se tienen miles de genes disponibles pero solo unos pocos son activos para una mutación de interés.

Aquí se asume el modelo de regresión usual, donde:
\[
E(y | \bx) = \bx'\bbeta, \mbox{ y } V(y | \bx) = \sigma^2,
\]
pero muchos de los elementos de $\bbeta$ son iguales a cero. El objetivo del estimador de LASSO seleccionar los coeficientes que tienen valores diferentes de cero. Esto se obtiene minimizando la siguiente expresión:

\begin{equation}
S_{lasso}(\beta)= \sum_{i=1}^{n}(y_{i}-x_{i}'\bbeta)^{2}+ \lambda\sum_{j=1}^{p-1}|\beta_{j}|,
(\#eq:SSLASSO)
\end{equation}
con $\lambda \geq 0$. Note que \@ref(eq:SSLASSO) es la suma de cuadrados del estimador por MCO más una penalización (dada por $\lambda$) a la suma del valor absoluto de los coeficientes (exceptuando el intercepto). Si $\lambda=0$, se obtiene el estimador por MCO. A medida que $\lambda$ aumenta la penalización tendrá mas peso sobre la estimación de los coeficientes. Por lo que si este parámetro es suficientemente grande, todas las estimaciones serán iguales a cero (con excepción del intercepto). También se puede probar que, cuando $\lambda \rightarrow \infty$, la varianza de $\hatbbeta_{LASSO}$ disminuye, pero el sesgo aumenta.


De forma de equivalente, el estimador de LASSO minimiza:

\[
\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\bbeta)^{2} \quad \mbox{ sujeto a } \quad
\sum_{j=1}^{p-1}|\beta_{j} | \leq t.
\]

Mientras que, en la regresión de ridge minimiza:

\[
\sum_{i=1}^{n}(y_{i}-x_{i}^{′}\bbeta)^{2} \quad \mbox{ sujeto a } \quad
\sum_{j=1}^{p-1}\beta_{j}^2\leq t.
\]

La Figura \@ref(fig:LASSOrest) muestra como intervienen las restricciones del estimador de LASSO (izquierda) y ridge (derecha) en un modelo con dos covariables. Los contornos muestras la suma de cuadrado de los errores, mientras que, las restricciones están dadas por el diamante $(|\beta_1|+|\beta_2| \leq t)$ y la circunferencia $(\beta_1^2+\beta_2^2 \leq t)$ rojas para el estimador LASSO y ridge, respectivamente. La solución de ambos estimadores se encuentra en el punto donde el contorno elíptico intercepta la región de la restricción. Para la regresión LASSO, si la solución ocurre en una esquina, entonces la estimación del parámetro es igual a cero.

```{r LASSOrest, echo=F, fig.height = 5, fig.width = 9,fig.align = "center",fig.cap = "Estimación del estimador de LASSO (izquierda) y ridge (derecha). Las elipses rojas son los contornos de la suma de cuadrados de los errores. Mientras que, la región roja son las restricciones para cada estimador)."} 
library(mnormt)
x = seq(-2,4,length.out=200)
y = seq(-2,5,length.out=200)

mu    <- c(1.5, 3)
sigma <- matrix(c(1, 0.6, 0.6, 1), nrow = 2)
f     <- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     <- outer(x, y, f)

par(mfrow=c(1,2))
contour(x, y, z,nlevels=4,drawlabels=F,xaxt='n',yaxt='n',asp=1,xlab=expression(beta[1]),ylab=expression(beta[2]))
abline(h=0)
abline(v=0)
a = 1.511
lines(a*c(-1,0,1,0,-1),a*c(0,-1,0,1,0),col=2)
text(1.5,3,label=expression(hat(beta)[mco]),cex=0.8)

text(0,a,label='t',cex=0.7,pos=2)
text(a,0,label='t',cex=0.7,pos=3)
points(x=0, y=a,pch=19,col=2)

contour(x, y, z,nlevels=4,drawlabels=F,asp=1,xaxt='n',yaxt='n',xlab=expression(beta[1]),ylab=expression(beta[2]))
text(1.5,3,label=expression(hat(beta)[mco]),cex=0.8)
abline(h=0)
abline(v=0)

a = 1.391
draw.circle(0, 0, a,border=2)
points(x=0.3, y=1.35,pch=19,col=2)

text(0,a,label='t',cex=0.7,pos=2.5)
text(a,0,label='t',cex=0.7,pos=4)
```


Dado que \@ref(eq:SSLASSO) es una función no lineal en $y$, no hay solución analítica para  $\hatbbeta_{LASSO}$. Sin embargo, pero hay algoritmos eficientes para su estimación. De igual forma, se recomienda escalar las variables para remover el efecto de las unidades de medida. 

En R, la estimación del modelo por medio del estimador de LASSO se hace por medio de la función `glmnet` de la librería `glmnet`. 

### Datos de grasa corporal 
La estimación de los coeficientes de regresión del modelo \@ref(eq:modeloFat) por medio del estimador de LASSO para diferentes valores de $\lambda$ se puede observar en la Figura \@ref(fig:Vgrasalasso). Se puede observar que a medida que $\lambda$ aumenta, mas coeficientes de regresión son iguales a cero.

```{r Vgrasalasso, echo=T, warning=FALSE,message = FALSE, fig.height = 5, fig.width = 8, fig.align = "center", fig.cap="Datos de grasa corporal. Estimación de los coeficientes de regresión vs el logaritmo de $\\lambda$"}
#se debe especificar alpha=1
X = model.matrix(mod.fat)[,-1]
lasso.mod <- glmnet(X, fat$brozek, alpha = 1,nlambda = 100)
plot(lasso.mod,xvar='lambda',label=T,lwd=2,ylab='coeficientes de regresión')
abline(h=0,lty=2)
```
La pregunta sería que valor de $lambda$ se debe seleccionar para determinar los coeficientes que son diferentes de cero. Una alternativa que tenemos para ello es por medio de validación cruzada.

### Validación cruzada
La validación cruzada (CV) es un método general para evaluar que tan bueno es un modelo para predecir observaciones futuras de la población objetivo del estudio. La idea es dividir la muestra en dos grupos:

- **(1) Entrenamiento:** se usa para ajusta el modelo.
- **(2) Validación:** se utiliza para validar el modelo ajustado.

Para no perder información, en la CV se realiza en $K$ iteraciones dividiendo los datos en $K$ subconjuntos. En cada iteración, uno de los subconjutos es utilizado como datos de validación y el resto de subgrupos $(K-1)$ como datos de entrenamiento. 

Para cada división,$k = 1, . . . , K$, y para cada valor de $\lambda$, se estima el modelo basado en la **muestra de entrenamiento**. Mientras que con cada **muestra de validación**, y para cada valor de $\lambda$, se utiliza para calcular el error cuadrático medio:

\[
EMC_{k}(\lambda) = \sum_{i=1}^{n_k} \frac{[y_{i}^{(k)}-\bx_{i}^{(k)}\hatbbeta_{lasso}^{(k)}(\lambda)]^2}{n}           
\]

donde $y_{i}^{(k)}$ son las observaciones de la muestra de validación $k$, y $\hatbbeta_{lasso}^{(k)}(\lambda)$ es la estimación utilizando la muestra de entrenamiento $k$. Finalmente, para cada $\lambda$, se calcula:
\[
CV(\lambda) = \frac{1}{K}\sum_{i=1}^{K}EMC_{k}(\lambda)              
\]
y la desviación estándar:
\[
SD(\lambda) = \sqrt{\sum_{i=1}^{K} \frac{[EMC_{k}(\lambda)-CV(\lambda)]^2}{K-1}}              
\]
Luego, el valor de $\lambda$ seleccionado corresponde al valor que minimiza $CV(\lambda)$:
\[
\hat{\lambda}_{cv}=arg\quad mín_{\lambda}-CV(\lambda)
\]
También, se puede aplicar la regla de una desviación estándar:

\[
\hat{\lambda}_{cv1sd}=máx \{\lambda:CV(\hat{\lambda})<CV(\hat{\lambda}_{cv})+SD(\hat{\lambda}_{cv})\}
\]
El segundo método es preferible dado que tiene en cuenta la variabilidad debida a la selección de las submuestras.

### Ejemplo grasa corporal
La estimación de $\lambda$ por medio de CV se puede hacer con la función `cv.glmnet` determinando del número de subconjuntos con el argumento `nfolds`. Por ejemplo, si consideramos $K=10$, se obtienen los siguientes resultados:
```{r kgrasa, echo=T, warning=FALSE,message = FALSE, fig.align = "center", fig.cap="Datos de grasa corporal. Valiación cruzada (puntos rojos) para diferentes valores de $\\lambda$. Las lineas cortadas verticales representan los valores seleccionados para $\\lambda$ por mínimo valores de CV y la regla de una desviación estándar."}
lasso.cv <-cv.glmnet(X, fat$brozek, nfolds = 10, alpha = 1,nlambda = 100)
plot(lasso.cv)
```
Las covariables seleccionadas al utlizar el estimador de LASSO con el $\lambda$ óptimo (regla una desviación estándar) son:
```{r}
est = glmnet(X, fat$brozek, alpha = 1,lambda = lasso.cv$lambda.1se)
est$beta
```

La selección de variables por medio del estimador LASSO son `age`, `height`,`abdom` y `wrist`. Ahora ajustamos el modelo con estas variables por medio de MCO:

````{r}
mod.lasso = lm(brozek ~ age+height+abdom+wrist,data=fat)
summary(mod.lasso)
```` 
Las covariables seleciondas en este modelo son todas significativas. Además, $R^2=0.7223$ y $R^2_{adj}=0.7178$. Comparado con el modelo completeo, estás cantidades han disminuido ligeramente.

La Tabla \@ref(tab:tablaEstFat1) muestra las estimaciones de los modelos ajustados utilizando diferentes métodos de selección de variables. En los tres modelos coinciden con la inclusión de la circunferencia del abdomen y de la muñeca. Las variables peso y estatura se encuentran en algunos de estos modelos, pero no de forma simultanea. Esto sugiere que proporcionan la misma información y no es necesario incluirlas juntas. Finalmente, se puede observar en la Tabla \@ref(tab:tablaEstFat2) que los tres modelos proporcionan similar ajuste.


```{r tablaEstFat1, echo=FALSE, include=TRUE, results="asis"}
kable(mathy.df, escape=FALSE,format = "html", booktabs = T,
      caption='datos de grasa corporal. Estimación de coeficientes de los modelos seleccionados')%>% kable_styling(bootstrap_options = "striped",full_width = F)%>%
  add_header_above(c("","AIC y PRESS" = 3,"BIC"=3,"LASSO"=3))%>%
  add_header_above(c("Método de selección" = 10))

```

```{r tablaEstFat2, echo=FALSE, include=TRUE, results="asis"}
mathy.df <- data.frame(b0=c("AIC y PRESS","BIC","LASSO"),
                       b1=c( 0.7467, 0.7351, 0.7223),
                       b2=c( 4139.682, 4209.14, 4447.052),
                       b3=c(1420.225,1423.471,1435.389),
                       b4=c(1455.52,1444.647,1456.566)
                       )

colnames(mathy.df)<-c("Modelo","$R^2$","PRESS","AIC","BIC")
kable(mathy.df, escape=FALSE,format = "html", 
      caption='datos de grasa corporal. Valores de los criterios de selección para los modelos seleccionados.', booktabs = T)%>% kable_styling(bootstrap_options = "striped",full_width = F)
```


La selección del modelo más adecuado no solo debe depender de un análisis estadístico, sino también de la opinión de expertos en el tema.
