# Modelo lineal generalizado
## Introducción
```{r preamble6, include=FALSE}
library(faraway)
logdose <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
dead <- c(6, 13, 18, 28, 52, 53, 61, 60) # numbers dead
n <- c(59, 60, 62, 56, 63, 59, 62, 60) # binomial sample sizes
Datos=data.frame(logdose,n,dead)
```
### Modelos lineales
Modelo lineal:

$$
y_{i}=\bx_{i}^{'}\bbeta+\epsilon, \quad \text{donde}\quad \epsilon \sim N(\bZERO,\sigma^2\bI)
$$

Lo que implica que $E(y_{i}|\bx_{i})=\bx_{i}\bbeta$ y $V(y_{i}|\bx_{i})=\sigma^2$

En algunos casos es difícil que se cumplan esas propiedades (incluso luego de hacer transformaciones).

Si la variable respuesta $Y$ solo puede tomar dos valores $(y_{i}\in\{0,1\})$ no podríamos representar $E(y|\bx)$ como una función lineal.

Los modelos lineales generalizados (GLM) son una clase de
modelos que permite modelar variables aleatorias con distribución
de probabilidad diferentes a la normal.

### Mortalidad de escarabajos
Número de escarabajos muertos después de cinco horas de
exposición a disulfuro de carbono gaseoso $(CS2mgl^{-1})$ en diversas concentraciones:

```{r TmortEsc, echo=FALSE}
knitr::kable(
  head(Datos[, 1:3], 8), booktabs = TRUE,
  caption = 'Datos mortalidad de escarabajos'
)
```

¿Hay una relación entre la dosis y la mortalidad de escarabajos?

```{r grafMortEsc}
plot(logdose,dead/n,xlab='log dosis',ylab='proporción de muertos',ylim=c(0,1),pch=16)

```

### Modelo logístico

Definiendo $y_{i}=\sum_{i=1}^ny_{ij}/n_{i}$ (la proporción de éxitos en ni ensayos independientes), tenemos que:

$$
n_{i}y_{i}\sim binomial(n_{i},\pi_{i}), \quad i=1,...,n, \quad \text{donde} \quad \pi_{i}=g(\bx_{i},\bbeta).
$$

Entonces: $E(y_{i}|\bx_{i})=\pi_{i}$ y $V(y_{i}|\bx_{i})=\pi_{i}(1-\pi_{i})/n_{i}$.

El modelo ligístico es un GLM y asume que:

$$
\pi_{i}=g^{-1}(\bx_{i},\bbeta)=\frac{exp(\bx_{i}^{'}\bbeta)}{1+exp(\bx_{i}^{'}\bbeta)}=\frac{1}{1+exp(-\bx_{i}^{'}\bbeta)}
$$

Por lo que: $g(\pi_{i})=log(\frac{\pi_{i}}{1-\pi_{i}})=\bx_{i}^{'}\bbeta$ **(función logit).**

```{r logfun, echo=FALSE, fig.align = "center",fig.cap = "Función logística"}
x= seq(from=-8,to=8,length.out = 1000)
curve(1/(1+exp(-x)),-8,8,lwd=2,ylab='y',xlab='log[y/(1-y)]')
abline(h=0,lty=2,lwd=1)
abline(h=1,lty=2,lwd=1)
```
### ataques de epilepsia

Ensayo clínico para evaluar el impacto de progabida sobre las crisis
epilépticas (```data(epilepsy)``` de la librería ```HSAUR2```).

Datos:

* ```age```: edad del paciente.

* ```base```: número de ataques epilépticos (x 8 semanas) antes del
ensayo.

* ```treatment```: tratamiento (placebo, progabida).

* ```seizure.rate```(variable respuesta): número de ataques epilépticos (x dos semanas) luego de 8 semanas.

"insertar graficos de epilepsia"

#### Modelo poisson

Aquí podemos suponer que:

$$
y_{i}\sim Poisson(\lambda_{i}), \quad i=1,...,n,\quad \text{donde} \quad \lambda_{i}=g(\bx_{i},\bbeta).
$$

Entonces:$E(y_{i}|\bx_{i})=V(y_{i}|\bx_{i})=\lambda_{i}).$

El modelo Pooisson es un GLM y asume que:

$$
\lambda_{i}=g^{-1}(\bx_{i},\bbeta)=exp(\bx_{i}^{'}\bbeta).
$$

Por lo que: $g(\lambda_{i}=log\lambda_{i}=\bx_{i}^{'}\bbeta$ **(función log)**.

## Modelo lineal generalizado (GLM)

Un modelo lineal generalizado (GLM) tiene tres componentes:

* **Componente aleatorio:** variable respuesta $Y$ y su distribución de probabilidad.

* **Predictor lineal:**

$$
\eta=\bx^{'}\bbeta,
$$

donde $\bbeta$ es un vector de parámetros y $\bx$ un vector de
covariables.

* **Función de enlace:** una función g que conecta $E(Y)$ con el predictor lineal, 

$$
g[E(Y|\bx)]=\bx^{'}\bbeta
$$

### Componente aleatorio

Observaciones independientes $(y_{i},...,y_{n})$ de una variable aleatoria $Y$ cuya distribución de probabilidad pertenece a la **familia exponencial.** 

Restringir un GLM a la familia exponencial permite tener expresiones generales para:

* La función de verosimilitud y funciones score.

* distribución asintótica de los estimadores de los parámetros del modelo

* Algoritmo para ajustar el modelo.

### Predictor lineal

Para cada observación $i$,

$$
\bx_{i}=(1,x_{i1},...,x_{i(p-1)})^{'},
$$

donde $x_{ij}$ es la observación $i$ de la covariable $j,\quad j=1,...,p-1$.

Predictor lineal es una combinación lineal de las covariables:

$$
\eta_{i}=\beta_{0}+\sum_{j=1}^{p-1}\beta_{j}x_{ij}=\bx_{i}^{'}\bbeta
$$

Un GLM asume que las covariables no son aleatorias.

### Función de enlace

Sea:

$$
E(Y|\bx_{i})=\mu_{i}, \quad i=1,...,n.
$$

En algunas distribuciones,$\mu_{i}$ está acotada en un intervalo. Por ejemplo, en la distribución binomial, $0\leq \pi_{i}\leq 1$, o en la Poisson, $\lambda_{i}>0$.

El GLM conecta $\mu_{i}$ con $\eta_{i}$:

$$
\mu_{i}=g^{-1}(\eta_{i})=g^{-1}(\bx_{i}^{'}\bbeta)
$$

La función $g(\cdot)$ es monótona y diferenciable.

$g(\cdot)$ esta determinada generalmente por la distribución que se asume para $Y$.

### Ejemplos de GLM

Algunos ejemplos de GLM:

|Tipo de respuesta|Distribución|función de enlace|Modelo|
|:----------------|:------------|:--------------|:------|
|Continuo | Normal    | Identidad | Modelo lineal    |
| Binaria | Bernoulli | Logit     | Modelo logístico |
| Conteo  | Poisson   | Log       | Modelo Poisson   |

Otros ejemplos son **beta** , **gamma**, **exponencial**, ...

hay extensiones del GLM para distribuciones:

* **binomial negativa** para conteo con sobredispersión.

* **beta-binomial** para ensayos Bernoulli correlacionados (sobredispersión).

* **multinomial** para variables nominales (ordinales) con más de
dos categorías

* **Weibull** para tiempos de falla.

### Ajuste de un GLM

El proceso de ajustar un GLM incluye:

* **Especificación del modelo**.Definición del componente
aleatorio, predictor lineal y función de enlace.

* **Estimación de los parámetros** del modelo.

* **Evaluación del modelo**. ¿El modelo describe bien los datos?

* **Inferencia**. Intervalos de confianza, pruebas de hipótesis e
interpretación de los resultados.

### Familia exponencial

La distribución de probabilidad de una variable aleatoria Y pertenece
a la **familia exponencial** si la función de densidad (o masa) de Y toma esta forma:

$$
f(y;\theta,\phi)exp\{[y\theta-b(\theta)]/a(\phi)+c(y,\phi)\}
$$

donde:

* $\theta$ es el parámetro natural.
* $\phi>0$ parámetro de dispersión.

Además se tiene que:

$$
E[Y]=b^{'}(\theta) \quad \text{y} \quad V[Y]=b^{''}(\theta)a(\phi).
$$

#### Ejemplo

La distribución Poisson:

$$
f(y;\mu)=\frac{\mu^yexp(-\mu)}{y!}, \quad \theta>0.
$$

se puede re-escribir como:

$$
f(y;\mu)=exp(-\mu+ylog\mu-logy!).
$$

Por lo tanto:

* $\theta=log\mu, \quad b(\theta)=exp(\theta)$.
  
* $a(\phi)=1$ y $c(y,\phi)=-lny!$.

La distribución normal:

$$
f(y;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma}}exp[-\frac{()}{}]
$$

Se puede re-escribir como:

$$
f(y;\mu,\sigma^2)=exp[\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-\frac{1}{2}log(2\pi\sigma^2)-\frac{y^2}{2\sigma^2}]
$$

donde:

* $\theta=\mu, \quad b(\theta)=\frac{1}{2}\theta^2$.

* $a(\phi)=\sigma^2$ y $c(y,\phi)=-\frac{1}{2}log(2\pi\sigma^2)-\frac{y^2}{2\sigma^2}$.

La distribución binomial:

$$
f(y;\pi)=\left(\begin{array}{c}n\\y\end{array}\right)\pi^y(1-\pi)^{n-y}.
$$

se puede re-escribir como:

$$
f(y;\pi)=exp\{y[log\pi-log(1-\pi)]+nlog(1-\pi)+log\left(\begin{array}{c}n\\y\end{array}\right)\}
$$
donde:

* $\theta=log(\frac{\pi}{1-\pi}), \quad b(\theta)=-nlog[1+exp(\theta)]$. 

* $c(\phi)=1$ y $d(y,\phi)=log\left(\begin{array}{c}n\\y\end{array}\right)$.


#### Familia exponencial


```{r, echo=FALSE, include=TRUE, results="asis"}
library(knitr)

mathy.df <- data.frame(b4=c("Normal","Binomial", "Poisson"), 
                       b0=c("$\\mu$","$log(\\frac{\\pi}{1-\\pi})$","$log(\\lambda)$"), 
B1=c("$\\sigma^2$","$\\frac{1}{n}$","$1$"),
B2 = c("$\\frac{1}{2}\\mu^2$","$log(1+exp\\beta)$","$exp\\lambda$"),
B3 = c("$-\\frac{1}{2}log(2\\pi\\sigma^2)-\\frac{y^2}{2\\sigma^2}$","$log\\left(\\begin{array}{c}n \\\\ y\\end{array}\\right)$","$-log(y!)$"))

colnames(mathy.df) <- c("Distribución", "$\\beta$", "$a(\\phi)$","$b(\\beta)$","$c(y,\\phi)$")

kable(mathy.df, escape=FALSE)
```
Relación media-varianza:

```{r, echo=FALSE, include=TRUE, results="asis"}
library(knitr)

mathy.df <- data.frame(site = c("Normal","Binomial", "Poisson"), 
                       b0 = c("$\\sigma^2$", "$\\mu(1-\\mu)/n$","$\\mu$"))

colnames(mathy.df) <- c("Distribución", "$v(\\mu)$")

kable(mathy.df, escape=FALSE)
```
#### Estimador de máxima verosimilitud

**Modelo** $Y\sim f(y,\btheta)$.

Estamos interesados en estimar $\btheta$.

Para ello tomamos una muestra independiente $(y_{1},y_{2},...,y_{n}).$

Si las observaciones son independientes, la **función de verosimilitud** es:

$$
L(\btheta)=\prod_{i=1}^nf(y_{i};\btheta).
$$

La función de log-verosimilitud:

$$
 \cl(\btheta)=\sum_{i=1}^nlnf(y_{i},\btheta).
$$

El objetivo es encontrar el $\hat{\btheta}$ que maximiza $L(\btheta)$ (o $\cl(\btheta)$).

$\hat{\btheta}$ se obtiene calculando las derivadas de $\cl(\btheta)$ con respecto a cada elemento de $\btheta$ y resolviendo las **ecuaciones score**:

$$
U(\btheta)=\frac{\partial\cl(\btheta)}{\partial\btheta}=\bZERO.
$$

s necesario verificar si la solución corresponde a un máximo de $\cl(\btheta)$. 
Se requiere que la matriz de segundas derivadas **(matriz Hessiana)**:

$$
H(\btheta)=\frac{\partial^2\cl(\btheta)}{\partial\btheta\partial\btheta^{'}},
$$

evaluada en $\btheta=\hat{\btheta}$, sea negativa definida.

#### Propiedades asintóticas de los MLEs

$\hat{\btheta}$ es asintóticamente $(n\to \infty)$ insesgado. Esto es, $E(\hat{\btheta})=\btheta$.

La varianza asintótica $(n\to \infty)$ de $\hat{\btheta}$ se calcula como la inversa de **la matriz de información**:

$$
v(\hat{\btheta})=I(\btheta)^{-1},\quad \text{donde} \quad I(\btheta)=-E[H(\btheta)]
$$
 **Teorema de Cramer-Rao:** La varianza de cualquier estimador es insesgado de un parámetro $\btheta$ debe:
 
 
 $$
 V(\hat{\btheta})\geq-E[H(\btheta)]^{-1}.
 $$
 
 Por lo cual, es MLE es eficiente.
 
 Otras propiedades importantes:
 
 * **Asintoticamente normal:**$\hat{\btheta}\sim N[\btheta,I(\btheta)^{-1}]$
 
 * **Invarianza:**si $\hat{\btheta}$ es el MLE de $\btheta$, entonces $g(\hat{\btheta})$ es el MLE de $g(\btheta)$
 
 
 
 


























