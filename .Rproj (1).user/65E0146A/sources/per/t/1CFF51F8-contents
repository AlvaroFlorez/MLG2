# Modelo lineal generalizado
## Introducción
```{r preamble6, include=FALSE}
library(faraway)
logdose <- c(1.6907, 1.7242, 1.7552, 1.7842, 1.8113, 1.8369, 1.8610, 1.8839)
dead <- c(6, 13, 18, 28, 52, 53, 61, 60) # numbers dead
n <- c(59, 60, 62, 56, 63, 59, 62, 60) # binomial sample sizes
Datos=data.frame(logdose,n,dead)
DatosEsc = Datos
colnames(DatosEsc) = c('log. dosis','expuestos', 'muertos')
library(HSAUR2)
data(epilepsy)
```
### Modelos lineales
Los modelos lineales se expresan como:
$$
\by=\bX\bbeta+\bvarepsi, \quad \text{donde}\quad \bvarepsi \sim N(\bZERO,\sigma^2\bI).
$$
Lo que implica que $\by \sim N(\bX\bbeta, \sigma^{2}\bI)$. Esto implica que:

- hay una relación lineal entre el valor esperado de la variable respuesta y el conjunto de covariables,
- la varianza de la variable respuesta es constante,
- la inferencia asume que la variable respuesta sigue una distribución normal.

Sin embargo, hay situaciones en donde estas propiedades no son factibles, incluso luego de hacer transformaciones sobre las variables. Por ejemplo, si la variable de respuesta es dicotómica $(y_{i}\in\{0,1\})$ no podríamos representar $E(y|\bx)$ como una función lineal. Además, la distribución asociada díficilmente serían normal. Aquí es mas conveniente, proponer una distribución de probabilidad diferente, por ejemplo, una Bernoulli, y ajustar un modelo lineal generalizado (GLM).

Los GLM son una clase de modelos que permite modelar variables aleatorias con distribución de probabilidad diferentes a la normal.

### Mortalidad de escarabajos
En un estudio de toxicología, se está interesado en la tasa de mortalidad de escarabajos expuestos a disulfuro de carbono gaseoso. La Tabla \@ref(tab:TmortEsc) muestra el número de escarabajos muertos después de cinco horas de exposición de este tóxico a diferentes concentraciones. Podemos ver que a medida que aumenta la dosis, la mortalidad va aumentando.

```{r TmortEsc, echo=FALSE}
knitr::kable(
  head(DatosEsc[, 1:3], 8), booktabs = TRUE,
  caption = 'Número de escarabajos expuestos y muertos a diferentes dosis (escala logarítmica)'
)
```
En la Figura \@ref(fig:grafMortEsc) podemos observar como es el aumento de la tasa de mortalidad con respecto a la dosis del tóxico. Note que la relación no es lineal, si no que tiene una forma de 'S'. Esto hace que un modelo de regresión lineal sea inadecuado.

```{r grafMortEsc, echo=F,, fig.height = 4, fig.width = 6, fig.align='center', fig.cap="Datos escarabajos. Relación entre la concentración de disulfuro de carbono gaseoso (en escala logarítmica) y la proporción de muertes (muertos/expuestos) de los escarabajos."}
plot(logdose,dead/n,xlab='log dosis',ylab='proporción de muertos',ylim=c(0,1),pch=16)

```
Una alternativa es utilizar una transformación que describa la relación de forma adecuada y que garantice que la respuesta se encuentre dentro de la región del parámetro. Es decir, que la estimación de la proporción de escarabajos muertos esté entre $0$ y $1$.

<!--
### Modelo logístico


-->
### ataques de epilepsia
Los datos ```data(epilepsy)``` de la librería ```HSAUR2``` corresponden a un ensayo clínico para evaluar el efecto del fármaco progabida sobre los ataques epilépticos. Al inicio del estudio, los $59$ pacientes epilépticos fueron observados durante 8 semenas, y se registró el número de convulsiones. Luego, fueron aleatorizados al tratamiento con el fármaco Progabide ($31$ pacientes) o al grupo de placebo ($28$ pacientes). A los pacientes de ambos grupos se les observo durante cuatro períodos de 2 semanas y se registró el número de convulsiones.

Estos datos son de naturaleza longitudinal (cada paciente cuenta con 4 observaciones tomadas en el tiempo), lo que requiere metodologías que tengan en cuenta la correlación que hay en los datos. En es caso, no consideraremos todas las observaciones de cada paciente, sino solamente las mediciones tomadas luego de 8 semanas.

Las variables que se tendrán en cuenta son las siguientes:

* ```age```: edad del paciente.

* ```base```: número de ataques epilépticos (x 8 semanas) antes de iniciar los tratamientos.

* ```treatment```: tratamiento (placebo, progabida).

* ```seizure.rate```(variable respuesta): número de ataques epilépticos (x dos semanas) luego de 8 semanas.

```{r seizurePlot, echo=F,, fig.height = 4, fig.width = 10,fig.align = "center", fig.cap="Datos epilepsia. Relación entre el número de ataques epilépticos con la edad, ataques previos y tratamiento."}
epilepsy=subset(epilepsy, period==4)#selecionamos la semana 4

par(mfrow=c(1,3))
plot(seizure.rate~age, col=treatment, pch=16, data=epilepsy, xlab="edad",
     ylab = "núm. de ataques post-tratamiento (ataques x 2 semanas)")
plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy,
     xlab="núm. de ataques pre-tratamiento (ataques x 4 semanas)",
     ylab = "núm. de ataques post-tratamiento (ataques x 2 semanas)")
boxplot(seizure.rate~treatment,data=epilepsy, xlab = "tratamiento",
        ylab = "núm. de ataques post-tratamiento (ataques x 2 semanas)")

```
La Figura \@ref(fig:seizurePlot) muestra la relación del número de ataques epiléticos al final del estudio con las posibles covariables. Aquí vemos que no parece haber relación con la edad, hay una relación fuerte con el número de ataques pre-tratamiento, y que los pacientes del grupo placebo parecen  presentar un poco más de convulsiones.

Antes de introducir los GLM, vamos a recordar algunos conceptos sobre distribuciones de probabilidad:

### Familia exponencial
La distribución de probabilidad de una variable aleatoria $Y$ pertenece a la **familia exponencial** si la función de densidad (o masa) de $Y$ se puede formular de la siguiente forma:
$$
f(y;\theta,\phi)=exp\{[y\theta-b(\theta)]/a(\phi)+c(y,\phi)\}
$$

donde $\theta$ es llamado parámetro natural, y $\phi>0$ es un parámetro de dispersión. Además se tiene que:

$$
E[Y]=b'(\theta) \quad \text{y} \quad V[Y]=b''(\theta)a(\phi).
$$
#### Ejemplo
La distribución Poisson:

$$
f(y;\mu)=\frac{\mu^y\exp(-\mu)}{y!}, \quad \mu>0,
$$

se puede re-escribir como:

$$
f(y;\mu)=exp(-\mu+y\log\mu-\log y!).
$$
Por lo tanto, pertenece a la familia exponencial con:

* $\theta=\log\mu, \quad b(\theta)=\exp(\theta)$.
  
* $a(\phi)=1$ y $c(y,\phi)=-\log y!$.

La distribución normal:

$$
f(y;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma}}\exp\left[-\frac{(y-\mu   )^2}{2\sigma^2}\right],
$$

se puede re-escribir como:

$$
f(y;\mu,\sigma^2)=\exp\left[\frac{y\mu-\frac{1}{2}\mu^2}{\sigma^2}-\frac{1}{2}\log(2\pi\sigma^2)-\frac{y^2}{2\sigma^2}\right],
$$

donde:

* $\theta=\mu, \quad b(\theta)=\frac{1}{2}\theta^2$,

* $a(\phi)=\sigma^2$ y $c(y,\phi)=-\frac{1}{2}log(2\pi\sigma^2)-\frac{y^2}{2\sigma^2}$.

La distribución binomial:

$$
f(y;\pi)=\left(\begin{array}{c}n\\y\end{array}\right)\pi^y(1-\pi)^{n-y}.
$$

se puede re-escribir como:

$$
f(y;\pi)=\exp\left\{y[\log\pi-log(1-\pi)]+n\log(1-\pi)+\log\left(\begin{array}{c}n\\y\end{array}\right)\right\},
$$
donde:

* $\theta=log\left(\frac{\pi}{1-\pi}\right), \quad b(\theta)=-n\log\left[1+\exp(\theta)\right]$,

* $c(\phi)=1$ y $d(y,\phi)=\log\left(\begin{array}{c}n\\y\end{array}\right)$.

### Relación media-varianza:
Para algunas distribuciones de probabilidad hay una relación directa entre la media y la varianza. Por ejemplo, para la distribución binomial tenemos que:
\[
E(Y) = \mu = n\pi \mbox{ y } V(Y) =  n\pi(1-\pi),
\]
por lo que $V(Y) = \nu(\mu) = \mu(1-\mu)/n$.

Para la distribución Poisson, tenemos que $V(Y) = \nu(\mu) = \mu$. Es decir, la varianza es igual a la media. Mientras que, para la distribución normal, la media y la varianza son parámetros independientes.



## Estimador de máxima verosimilitud

Se asume que $Y\sim f(y,\btheta)$ y estamos interesados en estimar $\btheta$. Para ello tomamos una muestra independiente $(y_{1},y_{2},...,y_{n})$. Por lo que, la **función de verosimilitud** está definida como:
$$
L(\btheta)=\prod_{i=1}^nf(y_{i};\btheta).
$$

La función de log-verosimilitud es:
$$
 \ell(\btheta)=\sum_{i=1}^n\log f(y_{i},\btheta).
$$

El objetivo del estimador por máxima verosimilitud (MLE) es encontrar el $\widehat{\btheta}$ que maximiza $L(\btheta)$ (o $\ell(\btheta)$). Para esto, tenemos que calcular las derivadas de $\ell(\btheta)$ con respecto a cada elemento de $\btheta$ y resolver las **ecuaciones score**:

$$
U(\btheta)=\frac{\partial\ell(\btheta)}{\partial\btheta}=\bZERO,
$$

para $\btheta$. Es necesario verificar si la solución corresponde a un máximo de $\elll(\btheta)$, evaluando si la matriz de segundas derivadas **(matriz Hessiana)**:

$$
H(\btheta)=\frac{\partial^2\ell(\btheta)}{\partial\btheta\partial\btheta^{'}},
$$

evaluada en $\btheta=\widehat{\btheta}$, es negativa-definida.

#### Propiedades asintóticas de los MLEs

$\widehat{\btheta}$ es asintóticamente $(n\to \infty)$ insesgado. Esto es, $E(\widehat{\btheta})=\btheta$ cuando $n\to \infty$. La varianza asintótica $(n\to \infty)$ de $\widehat{\btheta}$ se calcula como la inversa de **la matriz de información**:

$$
V(\widehat{\btheta})=I(\btheta)^{-1},\quad \text{donde} \quad I(\btheta)=-E[H(\btheta)].
$$
 Por lo cual, es MLE es un estimador eficiente (Teorema de Cramer-Rao). Otras propiedades importantes son:
 
 * **Asintoticamente normal:**$\widehat{\btheta}\sim N[\btheta,I(\btheta)^{-1}]$.
 
 * **Invarianza:**si $\widehat{\btheta}$ es el MLE de $\btheta$, entonces $g(\widehat{\btheta})$ es el  MLE de $g(\btheta)$.
 
#### Métodos iterativos de maximización
En algunos casos no es posible encontrar los MLEs de forma analítica, por lo que debemos hacerlo de forma iterativa. Los métodos que vamos a utilizar están basados en expansiones de series de Taylor.

<!--
.... series de Taylor ...
-->

Considere una expansión de series de Taylor de orden 1 para la función score, alrededor de $\btheta=\btheta^{(t)}$:

$$
U\left(\btheta\right)\approx U\left(\btheta^{(t)}\right)+H\left(\btheta^{(t)}\right)+\left(\btheta-\btheta^{(t)}\right).
$$

Igualando la expresión anterior a cero tenemos que:

$$
 U\left(\btheta^{(t)}\right)+H\left(\btheta^{(t)}\right)+\left(\btheta-\btheta^{(t)}\right)=\bZERO,
$$
 
Ahora, encontramos la solución para $\btheta^{(t+1)}$:

$$
\btheta^{(t+1)}=\btheta^{t}-H\left(\btheta^{t}\right)^{-1}U\left(\btheta^{t}\right).
$$
Podemos encontrar la estimación de $\btheta$ repitiendo los pasos anteriores hasta que se cumpla un criterio de convergencia. Si reemplazamos $H(\btheta)$ por $I(\btheta)$, el algoritmo recibe el nombre de **Fisher's scoring**.


## Modelo lineal generalizado (GLM)
Un GLM está definido por tres componentes:

* **Componente aleatorio:** variable respuesta $Y$ y su distribución de probabilidad.

* **Predictor lineal:** $\eta=\bx^{'}\bbeta$, donde $\bbeta$ es un vector de parámetros y $\bx$ un vector de covariables.

* **Función de enlace:** una función $g$ que conecta $E(Y)$ con el predictor lineal, $g[E(Y|\bx)]=\bx^{'}\bbeta$.

### Componente aleatorio
Se asume que se cuenta con $n$ observaciones independientes $(y_{i},...,y_{n})$ de una variable aleatoria $Y$ cuya distribución de probabilidad pertenece a la **familia exponencial.** Restringir un GLM a esta familia permite tener expresiones generales para: 

* La función de verosimilitud y funciones score.

* distribución asintótica de los estimadores de los parámetros del modelo

* Algoritmo para ajustar el modelo.

Por ejemplo, para los datos de los escarabajos podemos asumir que el número de escarabajos que mueren sigue una distribución binomial. Mientras que en el caso del ensayo clínico en epilepsia, se podría asumir que el número de ataques epilépticos sigue una distribución Poisson.

#### Familia exponencial

### Predictor lineal
Para cada observación $i$, tenemos un conjunto de covariables observadas. Esto es:

$$
\bx_{i}=(1,x_{i1},...,x_{i,p-1})^{'},
$$

donde $x_{ij}$ es la $j$-ésima covariable asociada al individuo $i$. El predictor lineal está definido como:
$$
\eta_{i}=\beta_{0}+\sum_{j=1}^{p-1}\beta_{j}x_{ij}=\bx_{i}^{'}\bbeta
$$

### Función de enlace
Sea:

$$
E(Y|\bx_{i})=\mu_{i}, \quad i=1,...,n.
$$

En algunas distribuciones,$\mu_{i}$ está acotada en un intervalo. Por ejemplo, en la distribución binomial, $0\leq \pi_{i}\leq 1$, o en la Poisson, $\lambda_{i}>0$. Esto hace que no siempre es razonable asumir una relación lineal entre $E(Y | \bx)$ y $\bx$. Por lo que la función de enlace conecta $\mu_{i}$ con el predictor lineal $\eta_{i}$ a través de una función $g()$:

$$
\mu_{i}=g^{-1}(\eta_{i})=g^{-1}(\bx_{i}^{'}\bbeta),
$$

donde, la función $g(\cdot)$ es monótona y diferenciable. Generalmente, $g(\cdot)$ esta determinada por la distribución que se asume para $Y$.

### Ejemplos de GLM
#### modelo logístico
Definiendo $y_{i}=\sum_{i=1}^ny_{ij}/n_{i}$ (la proporción de éxitos en $n_i$ ensayos independientes), tenemos que:

$$
n_{i}y_{i}\sim \mbox{binomial}(n_{i},\pi_{i}), \quad i=1,...,n, \quad \text{donde} \quad \pi_{i}=g(\bx_{i},\bbeta).
$$

Entonces: $E(y_{i}|\bx_{i})=\pi_{i}$ y $V(y_{i}|\bx_{i})=\pi_{i}(1-\pi_{i})/n_{i}$. El modelo ligístico es un GLM que asume:

$$
\pi_{i}=g^{-1}(\bx_{i},\bbeta)=\frac{exp(\bx_{i}^{'}\bbeta)}{1+exp(\bx_{i}^{'}\bbeta)}=\frac{1}{1+exp(-\bx_{i}^{'}\bbeta)}
$$

Por lo que: $g(\pi_{i})=log\left(\frac{\pi_{i}}{1-\pi_{i}}\right)=\bx_{i}^{'}\bbeta$. Esta función recibe el nombre de **función logit** y su forma la podemos ver gráficamente en la Figura \@ref(fig:logfun). Como vemos, está función está acotada entre $0$ y $1$, garantizando que las probabilidades estimadas estén en este rango, y tiene forma de 'S'.

```{r logfun, echo=FALSE, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Función logística"}
x= seq(from=-8,to=8,length.out = 1000)
curve(1/(1+exp(-x)),-8,8,lwd=2,ylab='y',xlab='log[y/(1-y)]')
abline(h=0,lty=2,lwd=1)
abline(h=1,lty=2,lwd=1)
```

Este modelo se puede utilizar para estimar la probabilidad de que el escarabajo muera en función de la concentración de disulfuro de carbono gaseoso.

#### modelo Poisson
Aquí asumimos que:

$$
y_{i}\sim Poisson(\lambda_{i}), \quad i=1,...,n,\quad \text{donde} \quad \lambda_{i}=g(\bx_{i},\bbeta).
$$

Entonces, $E(y_{i}|\bx_{i})=V(y_{i}|\bx_{i})=\lambda_{i}$. Dado que $\lambda_ i > 0$, se puede asumir que:
$$
\lambda_{i}=g^{-1}(\bx_{i},\bbeta)=exp(\bx_{i}^{'}\bbeta).
$$

Por lo que, $g(\lambda_{i})=log\lambda_{i}=\bx_{i}^{'}\bbeta$. La función de enlace escogida es logarítmica. 

Este modelo puede ser una opción para estimar el número medio de ataques epiléticos de los pacientes del ensayo clínico en epilepsia.

Un modelo lineal se puede representar como un GLM. Aquí asumimos que $y_i \sim N(\mu_i, \sigma^2)$, con $\mu_i = \bx_i'\bbeta$. Es decir, la función de enlace es la identidad.


Otros ejemplos de distribuciones que se pueden utilizar son:

* **binomial negativa** para conteo con sobredispersión.

* **beta-binomial** para ensayos Bernoulli correlacionados (sobredispersión).

* **multinomial** para variables nominales (ordinales) con más de dos categorías.

* **Weibull** para tiempos de falla.


## Ajuste de un GLM
Al igual que en los modelos lineales, los pasos para ajustar un GLM incluyen:

* **Especificación del modelo**.Definición del componente aleatorio, predictor lineal y función de enlace.

* **Estimación de los parámetros**. Para los GLM, la estimación se hace por máxima verosimilitud.

* **Evaluación del modelo**. Verificar si el modelo estimado se ajusta bien a los datos.

* **Inferencia**. Intervalos de confianza, pruebas de hipótesis e interpretación de los resultados según los objetivos del estudio.


### Especificación del modelo 
El modelo lineal generalizado (GLM) asume que:

$$
f(y_{i};\theta_{i},\phi)=exp\left\{\frac{y_{i}\theta_{i}-b(\theta_{i})}{a(\phi)} +c(y_{i},\phi)\right\},
$$

donde $\phi>0$. Además, $E(y_{i})=b'(\theta_{i})$ y $V(y_{i})=b''(\theta_{i})a(\phi).$ El **predictor lineal** $\eta_{i}=\bx'_{i}\bbeta$ se relaciona con $\mu_{i}$ a través la **función de enlace,** $\eta_{i}=g(\mu_{i})$. 

La función de enlace $g(\cdot)$  que transforma $\mu_{i}$ en el parámetro natural $\theta_{i}$ es llamada **función de enlace canónica.** Por ejemplo:

Para la distribución normal, $\eta_{i}=\mu_{i}$ (identidad), por lo tanto: $\mu_{i}=\eta_{i}$.

Para la distribución Poisson, $\eta_{i}=log\mu_{i}$ (función logarítmica), por lo tanto: $\mu_{i}=exp(\eta_{i})$.

Para la distribución binomial, $\eta_{i}=\log\left(\frac{\pi_{i}}{1-\pi_{i}}\right)$ (función logit), por lo tanto: $\pi_{i}=\frac{\exp(\eta_{i})}{1+\exp(\eta_{i})}$.

##### Mortalidad de escarabajos
Para los datos de mortalidad de escarabajos, la variable respuesta es la proporción de escarabajos muertos, $y_{i}=\sum_{j=1}^{n_{i}}y_{ij}/n{i}$, donde $y_{ij}=1$ si el $j$-ésimo escarabajo asociado a la $i$-ésima concentración del tóxico está muerto, $y_{ij}=0$ si lo contrario.

Podemos asumir que $y_i$ sigue una distribución binomial. Entonces, tenemos la siguiente función de densidad para $y_i$:
$$
f(n_{i}y_{i};\pi_{i})= \begin{pmatrix} n_{i}\\y_{i} \end{pmatrix}\pi_{i}^{y_{i}}(1-\pi_{i})^{(n_{i}-y_{i})}.
$$

El predictor lineal es $\eta_i = \beta_{0}+\beta_{1}\log\text{dosis}_{i}$. La función de enlace (logit) está definida como:

$$
log\left(\frac{\pi_{i}}{1-\pi_{i}}\right)=\beta_{0}+\beta_{1}\log\text{dosis}_{i}.
$$
Por lo que:
$$
\pi_i = \frac{\exp(\beta_{0}+\beta_{1}\log\text{dosis}_{i})}{1+\exp(\beta_{0}+\beta_{1}\log\text{dosis}_{i})}.
$$

### Ataques epilépticos
Para el ensayo clínico sobre epilepsia, la variable respuesta es número de ataques epilépticos. Dado que es un conteo, podemos asociar la variable respuesta con una distribución Poisson. La función de densidad es:
$$
f(y_{i};\mu_{i})=\frac{\mu_{i}^{y_{i}}\exp(-\mu_{i})}{y_{i}!}.
$$

El predictor lineal que podemos utilizar es $\eta_i = \beta_{0}+\beta_{1}\text{treat}_{i}+\beta_{2}\text{base}_{i}+\beta_{3}\text{base}_{i}\text{treat}_{i}$. La función de enlace (logarítmica) está definida como:

$$
\log\mu_{i}=\beta_{0}+\beta_{1}\text{treat}_{i}+\beta_{2}\text{base}_{i}+\beta_{3}\text{base}_{i}\text{treat}_{i}.
$$
Por lo que:
$$
\mu_{i}=\exp\left(\beta_{0}+\beta_{1}\text{treat}_{i}+\beta_{2}\text{base}_{i}+\beta_{3}\text{base}_{i}\text{treat}_{i} \right).
$$
### Estimador por máxima verosimilitud para $\bbeta$
dado que asumimos que la distribución de probabilidad asociada a la variable resputa pertenece a la familia exponencial, la función de log-verosimilitud está definida como:
$$
\ell(\bbeta)=\sum_{i=1}^{n}\ell_{i}(\bbeta)=\sum_{i=1}^{n}\log f(y_{i};\theta_{i},\phi)=\sum_{i=1}^{n}\left[\frac{y_{i}\theta_{i}-b(\theta_{i})}{a(\phi)}+c(y_{i},\phi)\right]
$$

Las funciones score están definidas como:

$$
U(\beta_{j})=\sum_{i=1}^{n}\frac{(y_{i}-\mu_{i})x_{ij}}{V(y_{i})}\frac{\partial\mu_{i}}{\partial\eta_{i}}=0, \quad \text{para} \quad j=0,...,p-1,
$$
donde $\eta_{i}=\bx_{i}'\bbeta=g(\mu_{i})$. Dado que las funciones score son no-lineales, es necesario estimar $\bbeta$ iterativamente usando el algoritmo de Newton-Raphson.

En forma matricial, tenemos que la función score es:
$$
U(\bbeta)=\bX'\bD\bV^{-1}(\by-\bmu),
$$

la función Hessiana es: 
$$
H(\bbeta)=\bI(\bbeta)=\bX'\bW\bX,
$$
donde:

* $\bV$ es una matriz diagonal con valores $v_{ii}=V(y_{i})$ en la diagonal,

* $\bD$ es una matriz diagonal con valores $d_{ii}=\frac{\partial\mu_{i}}{\partial\eta_{i}}$,

* $\bW$ es una matriz diagonal con  $w_{ii}=\frac{(\partial\mu_{i}/\partial\eta_{i})^2}{V(y_{i})}$.


A través del algoritmo de Newton-Raphson (o Fisher's scoring) podemos encontrar la estimación de $\bbeta$ de forma iterativa:
$$
\bbeta^{(t+1)}=\bbeta^{(t)}+[\bI(\bbeta^{(t)})]^{-1}U(\bbeta^{(t)}).
$$

Equivalentemente, podmeos utilizar el **método de mínimos cuadrados iterativamente reponderados:**

$$
\bbeta^{(t+1)}=(\bX'\bW^{(t)}\bX)^{-1}\bX'\bW^{(t)}\bz^{(t)},
$$
donde, los elementos de $\bz^{(t)}$ son:

$$
z_{i}^{(t)}=\bx_{i}'\bbeta^{(t)}+(y_{i}-\mu_{i}^{(t)})\frac{\partial\eta_{i}^{(t)}}{\partial \mu_{i}^{(t)}}, \mbox{ para }i=1,\ldots,n.
$$

La distribución asintótica del MLE de $\bbeta$ es:

$$
\widehat{\bbeta}\sim N\left[\bbeta,(\bX'\bW\bX)^{-1}\right],
$$

donde $\bW$ es una matriz diagonal con:

$$
w_{ii}=\frac{(\partial\mu_{i}/\partial\eta_{i})^2}{V(y_{i})},
$$
en la diagonal. Note que los valores de $\bW$ dependen de la función de enlace $g()$:

$$
\partial\mu_{i}/\partial\eta_{i}=g'( \mu_{i}).
$$
#### Mortalidad de escarabajos
Para los datos de mortalidad de escarabajos, tenemos el siguiente modelo:

$$
y_i \sim \mbox{binomial}(n_i,\pi_i), \mbox{ donde } \pi_{i}=\frac{\exp(\beta_0+\beta_1\log\text{dosis})}{1+\exp(\beta_0+\beta_0\log\text{dosis})}.
$$
La estimación de los parámetros del GLM por MLE se puede hacer utilizando la función ``glm()``. En este función debemos determinar la distribución de probabilidad asociada a los datos y la función de enlace con el argumento ``family``. Para el modelo logístico, usamos ``family=binomial`` (por defecto la función de enlace es logit):
```{r escarabajosEstimacion}
modBin = glm(cbind(dead,n-dead)~logdose,family=binomial)
summary(modBin)
```
Aquí vemos que el efecto del logarítmo de la dosis es positivo y significativo. Es decir que, un incremento en la dosis significa un aumento en la probabidad de que el escarabajo muera. La relación la podemos ver de forma gráfica en la figura \@ref(fig:escarabajosEstimacionFig).

```{r escarabajosEstimacionFig, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de escarabajos. Estimación de la probabilidad de que el escarabajo muera en función del logarítmo de la dosis"}
pred.x = data.frame(logdose = seq(min(logdose),max(logdose),length.out = 50))
pred = predict(modBin,pred.x,type='response')

plot(logdose,dead/n,xlab='log dosis',ylab='proporción de escarabajos muertos',ylim=c(0,1))
lines(pred.x$logdose,pred,col=2,lwd=2)
```

### interpretación de parámetros - modelo logístico
La interpretación de los coeficientes del modelo logístico se hacen a través de los **odds ratio**. Primero, un **odd** está definido como:

$$
\text{odd}=\frac{P(Y=1)}{P(Y=0)}.
$$
Es decir, es una razón entre la probabilidad de éxito sobre la probabilidad de fracaso. Por ejemplo, si el odd es igual a dos, estaríamos diciendo que la probabilidad de éxito es dos veces mayor a la probabilidad de fracaso. Para el caso del modelo logístco simple, tenemos que:

$$
\text{odd}(x)=\frac{P(Y=1|x)}{P(Y=0|x)}= \frac{\frac{\exp(\beta_0+x\beta_1)}{1+\exp(\beta_0+x\beta_1)}}{1-\frac{\exp(\beta_0+x\beta_1)}{1+\exp(\beta_0+x\beta_1)}} = \exp(\beta_0+x\beta_1).
$$

Ahora, los **odds ratio** están definido como $\text{OR}=\frac{\text{odd}(x=a+1)}{\text{odd}(x=a)}$. Para el caso del modelo logístico simple:

$$
\mbox{OR}(x)=\frac{\text{odd}(x=a+1)}{\text{odd}(x=a)}=\frac{\exp\left[\beta_{0}+(a+1)\beta_{1}\right]}{\exp(\beta_0+a\beta_1)}=\exp(\beta_1).
$$

Por lo tanto, por cada cambio unitario en $x$, los odds (`chances') de morir incrementan por un factor de $exp(\beta_1)$. Por un cambio en $x$ de $a$ a $a+\delta$, tenemos que $OR=\exp(\delta\beta_1)$.

Para los datos de los escarabajos un cambio unitario en el log dosis es un grande, por lo que podríamos interpetra $\beta_1$ usando un aumento en la log dosis de $\delta=0.02$:

$$
\text{OR}=\exp ( 0.01\times  34.270) = 1.984
5.
$$
Es decir, si el log de la concentración de aumenta en 0.02, entonces la probabilidad de que el escarabajo muera aumenta a casi el doble.

#### Ataques epilépticos
Para el ensayo clínico en epilepsia, el modelo propuesto es:
$$
y_i \sim \mbox{Poisson}(\mu_i), \mbox{ donde } \mu_i=\exp(\beta_0+\beta_1\text{treat}_i+\beta_2\text{base}_i+\beta_2\text{base}_i\text{treat}_i).
$$
El ajuste del modelo usamos la función ´´glm()´´ con el argumento ´´family=poisson´´:
```{r ajusteSeizure}
modPois=glm(seizure.rate~treatment+base+base:treatment,family=poisson,
            data =epilepsy)
summary(modPois)
```
El ajuste vemos que el número de ataques pre-tramiento tiene un efecto positivo sobre la media del número de ataques epilépticos. Es decir, esta aumenta entre más ataques previos tenga el paciente. Mientras que, el tratamiento parece tener un efecto negativo. Es decir, reduce la media del número de ataques epilépticos. Esto lo podemos ver graficamente en la Figura \@ref(fig:ajusteSeizureFig).

```{r ajusteSeizureFig,fig.height = 4, fig.width = 6,fig.align = "center",fig.cap="Datos de epilepsia. Estimación de la media de ataques epilépticos (por dos semanas) luego de cuatro semanas de tratamiento. La línea negra representa el placebo, mientras que, la linea roja al tratamiento con Progabide."}
y = data.frame(base=seq(min(epilepsy$base),max(epilepsy$base),length.out=100),
               treatment = factor(1:2, levels = 1:2, labels = levels(epilepsy$treatment)))
predpois = predict(modPois,y,type='response')
x=seq(min(epilepsy$base),max(epilepsy$base),length.out=50)

plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy,
     xlab="ataques pre-tratamiento (x 4 semanas)",
     ylab = "ataques post-tratamiento (x 2 semanas)")
lines(x,predpois[order(y$treatment, y$base)[1:50]], lwd = 2)
lines(x,predpois[order(y$treatment, y$base)[51:100]],col=2, lwd = 2)
```

#### Interpretación de parámetros - modelo Poisson
En el modelo Poisson, tenemos que: $E(Y|x)=\exp(\beta_0+x\beta_1)$. Si aumentamos a $x$ en $\delta$ unidades, tenemos que $E(Y|x+\delta)=\exp(\beta_0+(x+\delta)\beta_1)$. Ahora si calculamos el logarítmo de la razón de los valores esperados obtenemos que:

$$
\log\left[\frac{E(Y|x=a+\delta)}{E(Y|x=a)}\right]=\delta\beta_1.
$$

Por lo tanto, $\exp(\delta\beta_1)=\frac{E(Y|x=a+\delta)}{E(Y|x=a)}$. Es decir que, $exp(\delta\beta_1)$ es una tasa de crecimiento del valor esperado de $Y$ por un aumento de $x$ de $\delta$ unidades.

Por ejemplo, si el número de ataques epilépticos aumenta $20$ casos, entonces el valor esperado de ataques epilépticos post-tramiento aumenta un 52\% $(\exp(20 \times0.021)=1.522)$. El tramiento con Progabida reduce en un poco más del 40\% el número de episodios de epilepsia $(exp(-0.3635857) = 0.695)$. Finalmente, $exp(0.0009) \approx 1$, indicando que el efecto de los casos pre-tramiento es el mismo para los pacientes en el grupo tratamiento y el control.

<!--
### Algunas consideraciones

Para GLM con función de enlace canónica:

* Matriz hessiana = -matriz de información

* Por lo tanto los algoritmos Newton-Raphson y Fisher's scoring
son equivalentes

* La función de log-verosimilitud es concava.

* Por lo tanto, no hay posibilidad de múltiples máximos.
-->

## Pruebas de hipótesis
Al igual que en los modelos lineales, uno puede estar interesado en realizar pruebas hipótesis sobre los coeficientes del modelo. Por ejemplo, en el estudio sobre epilépsia estamos interesados en evaluar:
$$
\qquad H_0:\beta_1=\beta_3=0.
$$
Si rechazamos $H_0$ podemos concluir que el tratamiento con Progabida tiene un efecto sobre el número de ataques epilépticos. Particularmente, si $\beta_1 < 0$, este fármaco reduce los episodios.

Para un modelo con:
$$
\eta_i = \bx_1'\bbeta_1+\bx_2'\bbeta_2,
$$
podemos plantear las siguientes hipótesis:
$$
\qquad H_0:\bbeta_2=\bZERO \qquad H_1:\bbeta_2 \ne \bZERO
$$

Para estimadores basados en verosimilitud, podemos utilizar los siguientes métodos:

* Método del score (multiplicadores de Lagrange).

* Método de Wald.

* Método de razón de verosimilitud.

<!--



**Modelo completo:**$\boldsymbol\eta=\bX_1\bbeta_1+\bX_2\bbeta_2$.

**Modelo restringido:**$\boldsymbol\eta_0=\bX_1\bbeta_1+\bX_2\bbeta_2^0.$

En este caso, quiero probar:

$$
\qquad H_0:\bbeta_2=\bbeta_2^0 \qquad H_1:\bbeta_2 \ne \bbeta_2^0
$$
donde $\bbeta_2$ es de dimensión $q$ y $\bbeta_2^0$ es de dimensión $p-q$

#### Prueba del score

**Hipótesis:**

$$
\qquad H_0:\beta_2=\beta_2^0 \qquad \beta_2\ne\beta_2^0 
$$

Asintóticamente,$U(\bbeta)\sim N[\bZERO,\bI(\beta)]$, entonces:

$$
U(\bbeta)'\bI(\beta)^{-1}U(\bbeta)\sim \chi^2.
$$

**Estadístico de prueba:**

$$
U_2(\hat{\beta}^0)'\left\{V\left[U_2(\hat{\beta}^0)\right]\right\}^{-1}U_2(\hat{\beta}^0)\sim \chi^2_{p-q},
$$
donde $\hat{\beta}^0=(\hat{\beta}_1,\beta^0_2)'.$

#### Prueba de wald

**Hipótesis:**

$$
\qquad H_0:\bbeta_2=\bbeta_2^0 \qquad H_1:\bbeta_2 \ne \bbeta_2^0
$$

Asintóticamente, $\hat{\bbeta}\sim N[\bbeta,\bI(\bbeta)^{-1}]$. por lo tanto:

$$
(\hat{\bbeta}-\bbeta)'\bI(\bbeta)(\hat{\bbeta}-\bbeta)\sim\chi^2.
$$

**Estadístico de prueba:**

$$
\left(\hat{\bbeta}_2-\bbeta_2^0\right)'\left[V\left(\hat{\bbeta}_2\right)\right]^{-1}\left(\hat{\bbeta}_2-\bbeta_2^0\right)\sim \chi^2_{p-q}.
$$

#### Prueba de razón de verosimilitud

**Hipótesis:**

$$
\quad H_0:\bbeta_2=\bbeta_2^0 \qquad H_1:\bbeta_2\ne\bbeta_2^0
$$

Asintóticamente (y usando aproximación por series Taylor de orden 1):

$$
2\left[\cl(\hat{\bbeta})-\cl(\hat{\bbeta}^0)\right]\sim\chi^2.
$$

**Estadístico de prueba:**

$$
2\left[\cl(\hat{\bbeta})-\cl(\hat{\bbeta}^0)\right]
$$

donde $\hat{\bbeta}^0=(\hat{\bbeta}_1,\bbeta_2^0)'.$

#### Ejemplo ataques epilépticos

```{r, echo=FALSE, include=TRUE, results="asis"}
mathy.df <- data.frame(b0=c("Intercepto",                  "tratamiento","base","tratamientoxbase"),                      b1=c("$\\beta_0$","$\\beta_1$","$\\beta_2$","$\\beta_3$"),
b2=c(1.2453,-0.3636,0.0209,0.0009),
b3=c(0.1176,0.1625,0.0019,0.0023),
b4=c(10.59,-2.24,10.94,0.37),                      
b5=c(0.0000,0.0253,0.0000,0.7083))

colnames(mathy.df)<-c("Efecto","Parm.","est.","err. std.","Valor-z","Valor-p")

kable(mathy.df, escape=FALSE,format = "html", booktabs = T)
```

Prueba de hipótesis:
$$
\qquad H_0:\beta_1=\beta_3=0
$$
```{r, echo=FALSE, include=TRUE, results="asis"}
mathy.df <- data.frame(b0=c("Razón de verosimilitud",                  "Score","Wald"),                      
b1=c(10.405,10.418,10.325),
b2=c(2,2,2),
b3=c(0.005502,0.005466,0.005726))

colnames(mathy.df)<-c("Prueba","Estadístico","g.l.","Valor-p")

kable(mathy.df, escape=FALSE,format = "html", booktabs = T)
```

## intervalos de confianza

A partir de los estadísticos de prueba anteriores se pueden encontrar intervalos de confianza para $\hat{\bbeta}$. Por ejemplo usando el estadístico de Wald:

$$
\hat{\beta}\pm z_{\alpha/2}SE(\hat{\beta})
$$

Otra alternativa es a partir de los perfiles de log-verosimilitud, el IC está definido a partir de los valores de $\beta_0$ que satisfacen:

$$
-2[\cl(\beta_0,\hat{\psi}(\beta_0))-\cl(\hat{\bbeta,\hat{\bpsi}})]<\chi^2_1,
$$

donde $\psi$ son los demás parámetros del modelo.

### Intervalo de confianza para la media

Dado que $\hat{\eta_0}=\bx_0'\hat{\bbeta}$ ,por lo tanto (asintóticamente):

$$
\hat{\eta_0}\sim N[\bx_0'\bbeta,\bx_0'\bI(\bbeta)^{-1}\bx_0].
$$

Entonces un intervalo de confianza para $\hat{\eta_0}$ es:

$$
\hat{\eta_0}\pm z_{\alpha/2}\sqrt{\bx_0'I(\bbeta)^{-1}\bx_0}
$$

Para encontrar el intervalo de confianza para $\mu_i$, se hace la transformación $g^{-1}$ a los límites de confianza.

#### Ejemplo mortalidad de escarabajos

```{r grafintervalosescarabajos, include=TRUE}
summary(modBin)

pred.link = predict(modBin,newdata = pred.x,type='link',se.fit = T)

lim.sup = ilogit(pred.link$fit + qnorm(0.975)*pred.link$se.fit)
lim.inf = ilogit(pred.link$fit - qnorm(0.975)*pred.link$se.fit)

plot(logdose,dead/n,xlab='log dosis',ylab='proporción de escarabajos muertos',ylim=c(0,1))
lines(pred.x$logdose,pred)

lines(pred.x$logdose,lim.sup, col=2)
lines(pred.x$logdose,lim.inf, col=2)

```

#### Ataques epilépticos
```{r}
summary(modPois)
```


```{r grafintervalosepilep, fig.align='center',fig.cap="Ajuste del modelo Poisson. Placebo(negro), Progabide(rojo)"}
plot(seizure.rate~base, col=treatment, pch=16,data=epilepsy,
     xlab="Num. ataques epiléptico(pre tratamiento)",
     ylab = "Num. ataques epiléptico(4ta semana)")
lines(x,predpois[order(y$treatment, y$base)[1:50]], lwd = 2)
lines(x,predpois[order(y$treatment, y$base)[51:100]],col=2, lwd = 2)

pred.link = predict(modPois,newdata = y,type='link',se.fit = T)

lim.sup = exp(pred.link$fit + qnorm(0.975)*pred.link$se.fit)
lim.inf = exp(pred.link$fit - qnorm(0.975)*pred.link$se.fit)

lines(x,lim.sup[order(y$treatment, y$base)[1:50]], lty=2)
lines(x,lim.inf[order(y$treatment, y$base)[1:50]], lty=2)

lines(x,lim.sup[order(y$treatment, y$base)[51:100]],col=2, lty=2)
lines(x,lim.inf[order(y$treatment, y$base)[51:100]],col=2, lty=2)

```

## Devianza

Considere un GLM con observaciones $\by_i=(y_1,...,y_n)$.

Sea $\cl(\bmu)$ la log-verosimilitud (expresada en función de $\bmu$).

Por lo tanto, $\cl(\bmu)$ es la log-verosimilitud evaluada en el MLE.

La máxima verosimilitud que se puede alcanzar corresponde a $\cl(\by)$ (un modelo con ajuste perfecto $y_i=\mu_i$).

A este modelo, se le conoce como el **modelo saturado.**

La idea es comparar el modelo propuesto contra el modelo saturado:

$$
D=-2[\ell(\hat{\bmu})-\ell(\by)]
$$

Donde:

* $\ell(\by)=\sum_{i=1}^n[y_i\tildetheta_i-b(\tildetheta_i)]/a(\phi)$.

*  $\ell(\hat{\bmu})=\sum_{i=1}^n[y_i\hat{\theta_i}-b(\hat{\theta_i})]/a(\phi)$. 

$\tildetheta_i$ corresponde a la estimación de $\theta_i$ en el modelo saturado $(\tildemu_i=y_i)$

Si el ajuste es pobre, se espera que $D$ sea grande.

**Modelo binomial:**

$$
D=2\sum_{i=1}^n[y_ilog(\frac{y_i}{\hat{\mu}_i})+(n_i-y_i)log(\frac{n_i-y_i}{n_i-\hat{\mu}_i})],
$$

donde $\hat{\mu}_i=n_i\hat{\pi}_i.$

**Modelo Poisson:**

$$
D=2\sum_{i=1}^ny_ilog(\frac{y_i}{\hat{\mu}_i}), \quad \text{donde} \quad \hat{\mu}_i=\hat{\lambda}_i.
$$

**Modelo normal:**

$$
D=2\sum_{i=1}^n(y_i-\hat{\mu}_i)^2.
$$


### El estadístico chi-cuadrado de Pearson

El estadístico chi-cuadrado de Pearson:

$$
X^2=\sum_{i=1}^n\frac{(y_i-\hat{\mu}_i)^2}{v(\hat{\mu}_i)}.
$$

Para el modelo Binomial, tenemos que:

$$
X^2=\sum_{i=1}^n\frac{(y_i-n_i\hat{\pi}_i)^2}{n_i\hat{\pi}_i(1-\hat{\pi})}.
$$

y para el modelo Poisson:

$$
X^2=\sum_{i=1}^n\frac{(y_i-\hat{\mu}_i)^2}{\hat{\mu}_i}.
$$

## Bondad de ajuste

Tanto $D$ como $X^2$ se pueden utilizar para evaluar la bondad del ajuste.

$H_0$ indica que el modelo ajusta bien a los datos, y $H_1$  lo contrario. Por ejemplo, en el caso de un modelo logístico:

$$
H_0:logit(\pi_i)=\beta_0+\sum_{i=1}^k\beta_kx_{ik}
$$

Si $H_0$ es cierta, entonces $D$ y $X_2$ siguen una distribución $\chi_2$ con $(n - p)$ grados de libertadad*.

*La aproximación es buena para datos agrupados.

**Mortalidad de escarabajos**

Los valores de la devianza y el $\chi^2$ de Pearson:

```{r devianzaescarabajos, include=TRUE}
# devianza
D = deviance(modBin)
1-pchisq(D,6) # valor p
# chi-cuadrado de Pearson
X2 = sum(residuals(modBin,type='pearson')^2)
1-pchisq(X2,6) # valor p
```

```{r, echo=FALSE, include=TRUE, results="asis"}
library(knitr)

mathy.df <- data.frame(b4=c("Devianza","$X^2$"), 
                       b0=c(6,6),                    B1=c(11.232,10.026),B2 = c(0.0814,0.1235))

colnames(mathy.df) <- c("", "est","gl","valor-p")

kable(mathy.df, escape=FALSE)
```
## Pseudo $R^2$

El pseudo $R^2$ está definido como:

$$
\qquad \text{pseudo}R^2=\frac{\ell_M-\ell_0}{\ell_S-\ell_0}
$$

donde:

* $\ell_M$: log-versimilitud del modelo ajustado (evaluada en $\hat{\beta}$.

* $\ell_0$: log-versimilitud del modelo nulo (solo con intercepto).

* $\ell_S$: log-versimilitud del modelo saturado.

El pseudo $R^2$ se encuentra entre 0 y 1. Se utiliza para comparar modelos.

## Selección de variables

La selección de variables se puede realizar de la misma forma que en los LMs basándose en indicadores como el AIC y BIC (o alguna modificación de ellos).

**AIC:**

$$
\text{AIC}=-2\ell_M+2p.
$$

**BIC:**

$$
\text{BIC}=-2\ell_M+plog(n).
$$

-->

