# Multicolinealidad
```{r preamble3, include=FALSE}
library(MASS)
library(alr4)
library(isdals)

data(cement)
mod.cement = lm(y ~ x1+x2+x3+x4,data=cement)
s.mod.cement = summary(mod.cement)

data(bodyfat)
```

## Ejemplos

### Cemento
Los datos `data(cement)` en la librería `MASS` corresponden a un experimento sobre calor emanado en el fraguado de diferentes combinaciones químicas de cemento. Se tiene una muestra de $13$ fraguados de cemento Portlan. En cada muestra, se midió con precisión los porcentajes de los cuatro ingredientes químicos principales (covariables). Mientras el cemento fraguaba, también se midió la cantidad de calor desprendido (cals/gm, variable respuesta).


Los cuatro ingredientes químicos son:

- **`x1`** aluminato tricálcico (\%).
- **`x2`** silicato tricálcico (\%).
- **`x3`** tetra-aluminio ferrita de calcio (\%).
- **`x4`** silicato dicálcico (\%). 

En la Figura \@ref(fig:CementFigure) podemos observar que hay relaciones lineales positivas entre la variable respuesta, y las covariables aluminato tricálcico y silicato tricálcico. Mientras que la relación con la variable silicato dicálcico es negativa. También podemos notar que hay una relación negativa fuerte en las covariables aluminato tricálcico y tetra-aluminio ferrita de calcio, y entre silicato tricálcico y silicato dicálcico.

```{r CementFigure, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de cemento. Diagrama de dispersión.",warning=FALSE,message = FALSE}
library(MASS)
data(cement)
plot(cement[,c(5,1:4)])
```
Para estos datos, se propone el siguiente modelo:
\[
y_{i} = \beta_{0} + x_{1i}\beta_{1} + x_{2i}\beta_{2} + x_{3i}\beta_{3} + x_{4i}\beta_{4} + \varepsilon_{i}.
\]
Los resultados del ajuste son:
```{r CementFit, echo=T, warning=FALSE,message = FALSE}
mod.cement = lm(y ~ x1+x2+x3+x4,data=cement)
summary(mod.cement)
```
Note que el ajuste es bueno, el $`r round(s.mod.cement$r.squared*100,1)`$\% de la variabilidad de la cantidad de calor desprendido durante la fraguado es explicada por el modelo. Sin embargo, los resultados de las pruebas de hipótesis individuales sobre los coeficientes muestran que no son significantes. 

### Grasa corporal
Se tiene una muestra de $20$ mujeres saludables con edades entre $25$ y $34$ años (`data(bodyfat)`  en la librería `isdals`). La medición del porcentaje de grasa corporal es caro y engorroso, por lo tanto se quiere buscar un modelo que proporcione predicciones fiables. Como variables de este modelo se utiliza:

- **`Triceps`:** pliegue cutáneo del tríceps (cm).
- **`Thigh`:** circunferencia del muslo (cm).
- **`Midarm`:** circunferencia del brazo medio (cm).

La Figura \@(fig:bodyfatFigure) muestra la relación entre variables. Podemos observar que hay una relación fuerte del \% de masa corporal con las covariables Triceps y Thigh. Además hay también una relación lineal fuerte entre esas dos covariables.

```{r bodyfatFigure, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de grasa corporal. Diagrama de dispersión.",warning=FALSE,message = FALSE}
library(isdals)
data(bodyfat)
plot(bodyfat)
```
El modelo propuesto es:
\[
\mbox{Fat}_{i} = \beta_{0} + \mbox{Triceps}_{i}\beta_{1} + \mbox{Thigh}_{i}\beta_{2} + \mbox{Midarm}_{i}\beta_{3} + \varepsilon_{i}.
\]
El ajuste del modelo es el siguiente:
```{r BodyfatFit, echo=T, warning=FALSE,message = FALSE}
mod.bodyfat = lm(Fat ~ Triceps+Thigh+Midarm,data=bodyfat)
summary(mod.bodyfat)
```
Asi como en el caso anterior, observamos que el modelo explica gran parte de la variabilidad del \% de grasa corporal. Sin embargo, los valores $p$ de las pruebas individuales sobre los coeficientes son altos.

## Multicolinealidad
El estimador de $\bbeta$ es $\hatbbeta = (\bX'\bX)^{-1}\bX'\by$. Además, $V(\hatbbeta) = \sigma^{2}(\bX'\bX)^{-1}$. Por lo tanto requiere que $\bX$ sea de rango completo. Es decir, que hayan columnas que sean linealmente dependientes. En algunos casos, las columnas de $\bX$ son casi linealmente dependientes o colineales,lo que lleva a que $\bX'\bX$ se casi singular, lo que provoca problemas a la hora de hacer inferencias. 

Sea $\bx_j$ la $j$-ésima columna de la matriz $\bX$, por lo tanto $\bX = (\bONE,\bx_{1},\ldots,\bx_{p-1})$. Los vectores $\bx_{1},\bx_{2},\ldots,\bx_{p-1}$ son linealmente dependientes si hay conjunto de constantes $a_{1},a_{2},\ldots, a_{p-1}$ no todas igual a cero, tal que:
\[
\sum_{j=1}^{p-1}a_{j}\bx_{j} = c, \mbox{ donde }c\mbox{ es una constante.}
\]
Si esto se cumple para un subconjunto de $\bX$, el rango de la matriz $\bX'\bX$ es menor que $p$, y por lo tanto, $(\bX'\bX)^{-1}$ no existe.

Si la relación es aproximada $(\sum_{j=1}^{p-1}a_{j}\bx_{j} \approx c)$, existe el problema de multicolinealidad.

Vamos a ilustrar el efecto de la multicolinealidad con un ejemplo sencillo considerando un modelo lineal con dos covariables:
$$
\by^{*} = \bZ\bbeta + \bvarepsi,
$$
donde $\bZ$ es la matriz de covariables escaladas con longitud unitaria, esto es:
$$
y_{i}^{*} = \frac{y_{i}-\bar{y}}{\sqrt{SST}}, \mbox{ y } z_{ij} = \frac{x_{ij} - \bar{x}_{j}}{\sqrt{s_{jj}}},
$$
donde $s_{jj} = \sum_{i=1}^{n}(x_{ij}-\bar{x}_{j})^{2}$. De esta forma la matriz $\bZ'\bZ$ es la matriz de correlación de las covariables, 
$$
\bZ'\bZ = \begin{pmatrix}
1 & \rho_{12} \\ \rho_{12} & 1
\end{pmatrix},
$$
y $\bZ'\by$ es el vector de correlaciones entre $y$ y las dos covariables:
$$
\bZ'\by^{*} = \begin{pmatrix}
\rho_{y1} \\ \rho_{y2}
\end{pmatrix}.
$$
Por lo que el estimador por MCO de $\bb$ es:
$$
\hatbb = \begin{pmatrix}
1 & \rho_{12} \\ \rho_{12} & 1
\end{pmatrix}^{-1}\begin{pmatrix}
\rho_{y1} \\ \rho_{y2}
\end{pmatrix} = \begin{pmatrix}
\frac{1}{1-r_{12}^{2}} & \frac{-r_{12}}{1-r_{12}^{2}} \\ \frac{-r_{12}}{1-r_{12}^{2}} & \frac{1}{1-r_{12}^{2}} \end{pmatrix} \begin{pmatrix}
r_{1y} \\ r_{2y}
\end{pmatrix}.
$$
Particularmente, tenemos:
$$
\hatb_{1} = \frac{r_{1y}-r_{12}r_{2y}}{1-r_{12}^{2}} \mbox{ y } \hatb_{2} = \frac{r_{2y}-r_{12}r_{1y}}{1-r_{12}^{2}}.
$$
Además, la matriz de varianzas-covarianzas de $\hatbb$ es:
\begin{equation}
V(\hatbb) = \sigma^{2} \begin{pmatrix}
\frac{1}{1-r_{12}^{2}} & \frac{-r_{12}}{1-r_{12}^{2}} \\ \frac{-r_{12}}{1-r_{12}^{2}} & \frac{1}{1-r_{12}^{2}} \end{pmatrix}.
(\#eq:varbb)
\end{equation}
En \@ref(eq:varbb) podemos observar que la varianza de los coeficientes $\hatbb$ tienden a infinito cuando $|r_{12}|\rightarrow 1$. Mientras que se hace mínima cuando las covariables están incorrelacionadas $(r_{12}=0)$. Es decir, una fuerte correlación entre $x_1$ y $x_2$ da como resultado grandes varianzas y covarianzas para $\hatbb$. 

Por esta razón es preferible tener covariables que sean ortogonales. Puesto que así se garantiza la menor varianza para $\hatbbeta$. Sin embargo, las covariables son díficiles de controlar en estudios observacionales. 

Ahora consideremos un modelo con $p-1$ covariables,
$$
y_{i}^{*} = b_{1}z_{i1} + b_{2}z_{i2} + \ldots + b_{p-1}z_{i,p-1} + \varepsilon_{i}.
$$
Las ecuaciones normales son:
$$
\begin{pmatrix}
1 & r_{12} & r_{13} & \ldots & r_{1,p-1}  \\ r_{12} & 1 & r_{23} & \ldots & r_{2,p-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
r_{1,p-1} & r_{2,p-1} & r_{3,p-1} & \ldots & 1
\end{pmatrix} \begin{pmatrix}
b_{1} \\ b_{2} \\ \vdots \\ b_{k}
\end{pmatrix} = \begin{pmatrix}
r_{y1} \\ r_{y2} \\ \vdots \\ r_{y,p-1}
\end{pmatrix}.
$$
Por lo que el estimador de $\bb$ es: 
$$
\hatbb = \bR^{-1}\bZ'\by^{*}, \mbox{donde }\bR = \bZ'\bZ.
$$
Se puede demostrar que los elementos de la diagonal de la matriz $\bR^{-1}$ son:
$$
\{\bR^{-1}\}_{jj} = \frac{1}{1-R^{2}_{j}}, j=1,\ldots,p-1,
$$
donde $R^{2}_{j}$ es el coeficiente de determinación de la regresión de $z_{j}$ en función de las $(p-2)$ covariables restantes. Es decir, coeficiente de determinacón del siguiente ajuste:
$$
z_{ij} = z_{i1}\alpha_1 + z_{i2}\alpha_2 + \ldots + z_{i,j-1}\alpha_{j-1} + z_{i,j+1}\alpha_{j+1} + \ldots + z_{i,p-1}\alpha_{p-1} + \varepsilon_i.
$$

Por lo que la varianza del estimador de $\bb$ es: 
$$
V(\hatb_{j}) = \frac{\sigma^{2}}{1-R^{2}_{j}}.
$$
Por lo que, si $z_j$ se puede expresar como una combinación lineal aproximada de las demás covariables, la varianza del estimador de $b_j$ Si $R^{2}_{j} \rightarrow 1$, entonces $V(\hatb_{j})\rightarrow \infty$.

La multicolinealidad también produce que los estimadores $\hatb_{j}$, para $j=1,\ldots,p$, sean muy grandes en valor absoluto. Considere: 
$$
L_{1}^{2} = (\hatbb - \bb)'(\hatbb - \bb) = \sum_{j=1}^{k}(\hatb_{j}-b_{j})^{2}.
$$ 
El valor esperado de $L_{1}^{2}$ es:
$$
E(L_{1}^{2}) = \sum_{j=1}^{p-1}E(\hatb_{j}-b_{j})^{2} = \sum_{j=1}^{k}\sigma^{2}\tr\bR^{-1}
$$
Dado que la traza de una matriz es igual a la suma de sus valores propios, tenemos que:
\[
E(L_{1}^{2}) = \sigma^{2}\sum_{j=1}^{p-1}1/\lambda_{j},
\]
donde $\lambda_{j} > 0$, para $j=1,\ldots,p-1$, son los valores propios de $\bR$. Por lo tanto, si $\bR$ está mal condicionada debido a la multicolinealidad, al menos un $\lambda_{j}$ será muy pequeño.

Cabe notar que, la multicolinealidad no viola ningún supuesto sobre los errores. Por lo tanto los estimadores por MCO siguen siendo BLUE. Pero, en presencia de multicolinealidad:

- Las varianzas de los coeficientes $(\hatbeta_{j})$ se incrementan. Lo que reduce los valores $t$ asociados.

- Los $\hatbeta_{j}$ son sensibles a las especificaciones. Estos pueden cambiar drásticamente cuando se agregan o eliminan covariables.

- El ajuste general del modelo, y por lo tanto los pronósticos y las predicciones, no se verá afectado en gran medida. Pero los intervalos de confianza que se calculen pueden ser muy amplios.

### Causas de la multicolinealidad
La multicolinealidad se puede deber a múltiples razones, entre las que están:

- Con frecuencia se presentan problemas donde intervienen procesos de producción o químicos, donde los regresores son los componentes de un producto y ésos suman una constante.

- Variables que son componentes de un sistema pueden mostrar dependencias casi lineales debido a las limitaciones biológicas, físicas o químicas del sistema.

- Variables que tienen una tendencia común y evolucionan de forma muy parecida en el tiempo.

- Inclusión de variables irrelevantes en el modelo. La información que contienen estas variables ya estarían incluidas en otras y no aportan a la explicación de la variabilidad de la variable respuesta.

Por ejemplo, en los datos del cemento, tenemos que el problema de multicolinealidad se presenta porque:
$$
x_{i1} + x_{i2} + x_{i3} + x_{i4} \approx \mbox{constante}.   
$$
En los datos de grasa corporal, el problema es causado por la alta correlación entre el pliegue cutáneo del triceps $(X_{1})$ y la circunferencia del muslo $(X_{2})$.

## Detección de multicolinealidad
Dado que la multicolinealidad provoca una inflación de la varianza, un indicio de este problema esta en que aunque el modelo presenta un buen ajuste (un $R^2$ por ejemplo), las estimaciones de coeficientes asociados a covariables releventas tienen valores $t$ pequeños. Además, al eliminar covariables, las estimacioes cambian considerablemente. 

Los métodos mas usados para detectar multicolinealidad son:

- la matriz de correlación de las covariables.

- los factores de inflación de varianza.

- los índices de condiciones de $\bR$ o $\bZ$.

### Factores de inflación de varianza
Dado que $V(\hatbb) = \sigma^2\bR^{-1}$, la diagonal de $\bR^{-1}$ es un buen indicador de multicolinealidad:
$$
\{\bR^{-1}\}_{jj} = (1-R_{j}^{2})^{-1},
$$
donde $R_{j}^{2}$ es el coeficiente de determinación de ajustar un modelo de $x_{j}$ en función de las demás covariables. Si $x_{j}$ es casi ortogonal a las demás covariables, $R^{2}_{j}$ es pequeño, y por lo tanto, $\{\bR^{-1}\}_{jj}$ cercano a $1$. Mientras que, si $x_{j}$ es casi linealmente dependiente a las demás covariables, entonces $R^{2}_{j}$ es cercano a $1$ y $\{\bR^{-1}\}_{jj}$ grande.

Es decir, $\{\bR^{-1}\}_{jj}$ puede ser visto como un factor de cuanto se incrementa $V(\hatb_{j})$ debido a la colinealidad entre las covariables. De aquí el nombre de factor de inflación de varianza (VIF). Generalmente, uno o mas valores grandes de VIF ($5$ o $10$) es un indicador de problemas de multicolinealidad.

### Valores propios de $\bZ'\bZ$
Dado que $\bR = \bZ'\bZ$ es una matriz simétrica y positiva semi-definida, entonces se puede descomponer:
$$
\bR = \bT\bLambda\bT',
$$
donde $\bT = (\bt_{1},\ldots,\bt_{k})$ es una matriz ortogonal de vectores propios, y $\bLambda$ una matriz diagonal con los valores propios $\lambda_{j}$, para $j=1,\ldots,p-1$, en la diagonal principal. Por lo tanto,
$$
V(\hatbbeta) = \sigma^{2}\bT \bLambda^{-1}\bT'.
$$
Entonces, 
$$
V(\hatb_{l}) = \sigma^{2}\sum_{j=1}^{p-1}\frac{t_{lj}^{2}}{\lambda_{j}}.
$$
De la expresión anterior,
$$
VIF_{j} = \sum_{j=1}^{k}\frac{t_{ij}^{2}}{\lambda_{j}},
$$
Por lo tanto, uno o más valores propios pequeños pueden inflar la varianza de $\hatb_{j}$. De aquí salen los indicadores llamados índices de condición y número de condición.

El índice de condición está definido como:
$$
\eta_{j} = \frac{\lambda_{\mbox{max}}}{\lambda_{j}}.
$$
El número de condición está definido como el máximo índice de condición. Un número de condición mayor de 100 es un indicador de multicolinealidad.

### Valores singulares de $\bZ$
El número de condición también se puede calcular a partir de la descomposición en valores singulares (SVD) de la matriz de covariables $\bZ$. Esto es:
$$
\bZ =\bU\bD\bT',
$$
donde $\bU$ y $\bT$ son matrices $(n\times p)$ y $(p\times p)$, respectivamente, que $\bU\bU = \bI$ y $\bT'\bT = \bI$, y $\bD$ es una matriz diagonal con elementos $\mu_{j}$, $j=1,\ldots,p$, llamados valores singulares. 
Hay una relación entre los valoes singulares de $\bZ$ y los valores propios de $\bZ'\bZ$:
$$
\bZ'\bZ = (\bU\bD\bT')'\bU\bD\bT' = \bT\bD^2\bT' = \bT\bLambda\bT'.
$$
El índice de condición de $\bZ$ está definido como:
$$
\kappa_{j} = \frac{\mu_{\mbox{max}}}{\mu_{j}}.
$$
El número de condición de $\bZ$ está definido como el máximo $\kappa_{j}$. Un número de condición mayor de 10,15 o 30 es un indicador de problemas de multicolinealidad.

Note que $\kappa_{j} = \sqrt{\eta_{j}}$. Por lo que el punto de corte de $10$ para el número de condición de $\bZ$ es equivalente al número de condición de $100$ para el número de condición de $\bZ'\bZ$.

### Proporciones de descomposición de varianza
Hay una relación entre el VIF, los valores propios y los valores singulares. Esta es:
$$
VIF_{j} = \sum_{j=1}^{k}\frac{t_{ij}^{2}}{\lambda_{j}} = \sum_{j=1}^{k}\frac{t_{ij}^{2}}{\mu_{j}^2}.
$$
De aquí salen los indicadores llamados proporciones de descomposición de varianza. Estos se calculan de la siguiente forma:
$$
\pi_{ij} = \frac{t_{ij}^2 / \mu_{i}^2 }{VIF_{j}}, j = 1,\ldots, p-1.
$$
Si se agrupan los $\pi_{ij}$ en una matriz $(p \times p)$ $\bpi$, la columna $j$ son proporciones de la varianza de $\hatbb$. Valores $\pi_{ij}$ mayores de 0.5 indican problemas de multicolinealidad.  

## Datos de cemento
Los VIF para el modelo ajustado para los datos de cemento son los siguientes:
```{r CementVIF, echo=T, warning=FALSE,message = FALSE}
car::vif(mod.cement)
```
Aquí vemos que todos los VIFs son muy altos, mostrando que hay problemas graves de multicolinealidad.

Para calcular los índices de condición (a partir de los valores singulares de $\bZ$) y las proporciones de descomposición de varianza se utiliza la función ``colldiag()`` de la librería ``perturb``:
```{r CementIindiceCondicion,echo=T, warning=FALSE,message = FALSE}
library(perturb)
colldiag(mod.cement,scale = TRUE,center = TRUE,add.intercept = FALSE)
```
El número de condición $(37.106)$ es muy alto reafirmando el problema de multicolinealidad. Además, también se puede observar que varias de las proporciones de descomposición de varianza son mayores de $0.5$.

## Datos de grasa corporal
Para el modelo ajustado a losdatos de grasa corporal, los VIFs son:
```{r BodyfatVIF, echo=T, warning=FALSE,message = FALSE}
car::vif(mod.bodyfat)
```
Además, los índices de condición y las proporciones de descomposición de varianza son:

```{r BodyfatIindiceCondicion,echo=T, warning=FALSE,message = FALSE}
library(perturb)
colldiag(mod.bodyfat,scale = TRUE,center = TRUE,add.intercept = FALSE)
```
Todos estos indicadores muestran que hay problemas de multicolinealidad.

## Solución al problema de multicolinealidad
Una forma sencilla de resolver los problemas de multicolinealidad son:

- la recolección de datos adicionales: si se tiene control sobre las covariables, es posible tomar más observaciones para romper con la casi dependencia en $\bX$. Pero esto en muchas casos es díficl por la naturaleza de las covariables. Por ejemplo, sería imposible buscar personas con circunferencia del muslo grande y pliegue cutáneo del tríceps bajo (o viceversa). 

- la re-especificación del modelo: se pueden eliminar del modelo las covariables que me generan el problema de multicolinealidad y que pueden tener poco aporte explicativo dentro del modelo. 

```{r BodyfatFit0, echo=F, warning=FALSE,message = FALSE}
mod.bodyfatSummary = summary(mod.bodyfat)
mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat)
mod.bodyfat0Summary = summary(mod.bodyfat0)
```

Por ejemplo, en los datos de la grasa corporal podemos eliminar la variable asociada al muslo (o al tríceps, pues ambas proporcionan la misma información). Si quitamos esta covariable:
```{r BodyfatFit2, echo=T, warning=FALSE,message = FALSE}
mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat)
car::vif(mod.bodyfat0)
```

los VIFs disminuyen considerablemente. Además, no hay una reducción notable del $R^2$. Con las tres covariables temenos que $R^{2}=`r round(mod.bodyfatSummary$r.squared,3)`$. Mientras que al remover ``Thigh``, tenemos que $R^{2} = `r round(mod.bodyfat0Summary$r.squared,3)`$


Considerando que el estimador por MCO es el mejor estimador lineal insesgado de $\bbeta$, podemos relajar la condición de insesgamiento y buscar estimadores que aunque sean sesgados tengan menor varianza que los estimadores por MCO. 


Dos de estas alternativas son:

- Estimador de ridge.

- Estimadores por componentes principales.


## Estimador de ridge
El estimador de ridge tiene como objetivo minimizar la siguiente suma de cuadrados penalizada:
\begin{equation}
\begin{split}
S_{R}(\bb) &= \sum_{i=1}^{n}(y_{i} - \bz_{i}'\bb)^{2} + k \sum_{j=1}^{p-1}\bb_{j}^2 \\
&= (\by - \bZ\bb)'(\by - \bZ\bb) + k ||\bb||_{2}^{2},
\end{split}
\nonumber
\end{equation}
con $k \geq 0$ (el cual es seleccionado por el investigador). Lo que lleva a las siguientes ecuaciones normales:
\[
(\bR + k \bI)\hatbb_{R} = \bZ'\by.
\]
La solución de las ecuaciones normales lleva al estimador ridge:
\[
\hatbb_{R} = (\bR + k \bI)^{-1}\bZ'\by.
\]
Note que, incluso si $\bR$ es no es invertible, un $k > 0$ resuelve el problema. Además, la solución depende de $k$ (por cada $k$, hay una estimación diferente). 

$k$ es un parámetro de contracción:

- si $k \rightarrow 0$, encontramos que $\hatbb_{R} \rightarrow \hatbb$.
- si $k \rightarrow \infty$, encontramos que $\hatbb_{R} \rightarrow \bZERO$ (excepto el intercepto).

Ahora veamos las propiedades del estimador de ridge. El valor esperado de $\hatbb_{R}$ es:
$$
E(\hatbb_{R}) = (\bR + k \bI)^{-1}\bR \bb.
$$
De aquí vemos que $\hatbb_{R}$ es sesgado, y aumenta con $k$ (por lo que este parámetro también es llamado de sesgo). La varianza de $\hatbb_{R}$ es:
$$
V(\hatbb_{R}) = \sigma^{2}(\bR + k \bI)^{-1}\bR(\bR + k \bI)^{-1}.
$$
A partir de las cantidades anteriores se puede determinar el error cuadrático medio (ECM) de $\hatbb_{R}$:
\begin{equation}
\begin{split}
\mbox{ECM}(\hatbb_{R}) &= \sigma^{2}\tr [(\bR + k\bI)^{-1}\bR(\bR + k\bI)^{-1}] + k^{2}\bb'(\bR + k\bI)^{-2}\bb \\
&=\sigma^{2}\sum_{i=1}^{p-1}\frac{\lambda_{j}}{(\lambda_{j}+ k)^2} + k^{2} \bb'(\bR + k\bI)^{-2}\bb,
\end{split}
\nonumber
\end{equation}
donde $\lambda_{1},\ldots,\lambda_{k}$ son los valores propios de $\bR$.

Finalmente, la suma de cuadrados de los residuos usando $\hatbb_{R}$ es:
$$
\Sres = (\by - \bX\hatbb)'(\by - \bX\hatbb) + (\hatbb_{R}-\hatbb)'\bR(\hatbb_{R}-\hatbb).
$$
A partir de estos resultados, vemos que: (1) Si $k$ crece, disminuye la varianza, pero aumenta el sesgo. (2) Si $k$ disminuye, aumenta la varianza, pero disminuye el sesgo. (3) el estimador ridge proporciona un $R^{2}$ mas pequeño que el estimador por MCO. Además, $R^2$ disminuye con $k$. Sin embargo, la regresión ridge proporciona estimaciones de $\bb$ más estables.

La idea de la regresión de ridge es encontrar un valor de $k$ tal que  $\mbox{ECM}(\hatbb_{R}) < V(\hatbb)$. En la Figura \@ref(fig:ECMrepresentacion) podemos ver la representación del ECM del estimador de ridge. Aquí vemos que a medida que aumenta $k$, el sesgo incrementa y la varianza disminuye. También, hay una región de $k$ donde se puede obtener un ECM del estimador de ridge menor que el que se obtiene por medio de MCO. La idea es encontrar el valor de $k$ que minimiza el ECM, o por lo menos algún valor en la región donde $\mbox{ECM}(\hatbb_{R}) < V(\hatbb)$.


```{r ECMrepresentacion, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Representación del sesgo al cuadrado (linea cortada), varianza (linea punteada) y error cuadrático medio (linea solida) del estimador de ridge. La linea roja representa el error cuadrático medio del estimador por MCO.",warning=FALSE,message = FALSE}
x= seq(from=0,to=0.3,length.out = 100)
sesgo = 0.25/(1+exp(-20*(x-0.001)))
sesgo = sesgo - min(sesgo) 
vari = 0.06*exp(-70*x)+ 0.0025
ecm =sesgo+vari
plot(x,sesgo,type = 'l',xlim=c(0,0.2),ylim=range(c(sesgo,vari,ecm)),lty=2, xaxt='n',yaxt='n',
     xlab='k',ylab='Sesgo, varianza y MSE')
lines(x,vari,lty=3)
lines(x,ecm,lwd=2)
abline(h=max(vari),col=2)
lines(c(x[ecm==min(ecm)],x[ecm==min(ecm)]),c(-1,min(ecm)),col=4)
lines(c(-1,x[ecm==min(ecm)]),c(min(ecm),min(ecm)),col=4)
axis(1,x[ecm==min(ecm)],'k óptimo')
```
Algunos métodos de selección de $k$ son:

- Traza de ridge $k$: el efecto de $k$ sobre las estimaciones de $\hatbb_{R}$ es mas fuerte para valores bajos. De igual forma, si $k$ es muy grande introducimos mucho sesgo. Por lo que se puede hacer es incrementar $k$ hasta que parezca que su influencia sobre $\hatbb_{R}$ se atenúe.

- Validación cruzada (CV): sea $\haty_{(i),k}$ la estimación de $E(y_i)$ por medio del estimador de ridge con el parámetro $k$ y usando una muestra excluyendo la i-ésima observación. La validación cruzada está definida como:
$$
CV(k) = \sum_{i=1}^{n} (y_i - \haty_{(i),k})^2.
$$
Por lo que la selección de $k$ es:
$$
k_{CV} = \arg \min_{k} CV(k).
$$

### Datos de cemento
Para ajustar el modelo usando el estimador de ridge podemos usar la función ``lmridge`` del paquete ``lmridge``. Primero, ajustamos el modelo usando diferentes valores de $k$:
```{r cementridge1,echo=T, warning=FALSE,message = FALSE}
library(lmridge)
K = seq(from=0,to=0.3,length.out = 100)
ridge.cement = lmridge(y~., data=cement,K=K,scaling='sc')
```
En el objeto ``ridge.cement`` tenemos las estimaciones por el estimador de ridge para $100$ valores de $k$ entre $0$ y $0.3$. Para observar como cambian las estimaciones para los diferentes valores de $k$ podemos graficar la traza de ridge así (en términos de las covariables en su escala orginal):
```{r CementTrazaRidge,echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de cemento. Traza de ridge. Coeficiente asociado a ``X1`` (negro), coeficiente asociado a ``X2`` (rojo), coeficiente asociado a ``X3`` (verde) y coeficiente asociado a ``X4`` (azul)",warning=FALSE,message = FALSE}
EstRidge.cement = coef(ridge.cement)
plot(K,EstRidge.cement[,2],type='l',ylim=range(EstRidge.cement[,-1]),lwd=2,
     ylab='Estimaciones de los coeficientes',xlab='k')
lines(K,EstRidge.cement[,3],col=2,lwd=2)
lines(K,EstRidge.cement[,4],col=3,lwd=2)
lines(K,EstRidge.cement[,5],col=4,lwd=2)
abline(h=0,lty=2)
```
En la Figura \@ref(fig:CementTrazaRidge) podemos observar que, cuando incrementamos $k$, las estimaciones cambian rápidamente y luego parecen estabilizarse cuando $k$ es grande. Además hay un cambio de signo para el coeficiente asociado a ``X3``. También puede usarse ``plot(mod.r)`` (traza de ridge para las estimaciones de los coeficientes asociados a las covariables escaladas).

La selección del $k$ óptimo por medio de validación cruzada (CV) se hace de la siguiente manera:
```{r cementridge3, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "\\label{fig:CementCVridge} Datos de cemento. Validación cruzada.",warning=FALSE,message = FALSE}
Criterios.cement = kest(ridge.cement)
plot(K,Criterios.cement$CV,type='l',xlab='K',ylab='validación cruzada')
K[Criterios.cement$CV==min(Criterios.cement$CV)]
```
Aquí podemos ver que el valorde $k$ que minimiza la validación cruzada es $0.009$. El valor óptimo de $k$ por medio de otros criterios son: 
```{r cementridge4,echo=T, warning=FALSE,message = FALSE}
Criterios.cement
```
La estimación con $K=0.0101$ se puede obtener usando la función ``lmridge`` usando $K=0.0101$ como argumento:
```{r cementridge5,echo=T, warning=FALSE,message = FALSE}
ridge.cement2 = lmridge(y~., data=cement,K=0.0101,scaling='sc')
summary(ridge.cement2)
```
Vemos algunas diferencias con las estimaciones por MCO. Hay un cambio de signo en la estimación del coeficiente asociado a ``x3``. Si embargo, no hay mucha diferencia en las estimaciones de los otros coeficientes. También podemos observar que las covariables ``x1``, ``x2`` y ``x3`` ahora tienen un aporte significativo. Como era de esperarse, hay una disminución del $R^{2}$, sin embargo es muy leve.