# Multicolinealidad
```{r preamble3, include=FALSE}
library(MASS)
library(alr4)
library(isdals)

data(cement,package = 'MPV')
mod.cement = lm(y ~ x1+x2+x3+x4,data=cement)
s.mod.cement = summary(mod.cement)

data(bodyfat)
```

## Ejemplos

### Cemento
Los datos `data(cement)`, de la librería `MASS`, corresponden a un experimento sobre el calor emanado en el fraguado de diferentes combinaciones químicas de cemento. Se tiene una muestra de $13$ fraguados de cemento Portland. En cada muestra, se midió con precisión los porcentajes de los cuatro ingredientes químicos principales (covariables). Mientras el cemento fraguaba, también se midió la cantidad de calor desprendido (cals/gm, variable respuesta). Los cuatro ingredientes químicos son: **`x1`** aluminato tricálcico (\%), **`x2`** silicato tricálcico (\%), **`x3`** tetra-aluminio ferrita de calcio (\%), y **`x4`** silicato dicálcico (\%). 

En la Figura \@ref(fig:CementFigure) podemos observar que hay relaciones lineales positivas entre la variable respuesta, y las covariables aluminato tricálcico y silicato tricálcico. Mientras que la relación con la variable silicato dicálcico es negativa. También podemos notar que hay una relación negativa fuerte en las covariables aluminato tricálcico y tetra-aluminio ferrita de calcio, y entre silicato tricálcico y silicato dicálcico.

```{r CementFigure, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de cemento. Diagrama de dispersión.",warning=FALSE,message = FALSE}
data(cement,package = 'MASS')
plot(cement[,c(5,1:4)])
```
Para estos datos, se propone el siguiente modelo:
\[
y_{i} = \beta_{0} + x_{1i}\beta_{1} + x_{2i}\beta_{2} + x_{3i}\beta_{3} + x_{4i}\beta_{4} + \varepsilon_{i}.
\]
Los resultados del ajuste son:
```{r CementFit, echo=T, warning=FALSE,message = FALSE}
mod.cement = lm(y ~ x1+x2+x3+x4,data=cement)
summary(mod.cement)
```
Note que el ajuste es muy bueno, el $`r round(s.mod.cement$r.squared*100,1)`$\% de la variabilidad de la cantidad de calor desprendido durante la fraguado es explicada por el modelo. Sin embargo, los resultados de las pruebas de hipótesis individuales sobre los coeficientes muestran valores $p$ muy grandes. Estos resultados parecen contradictorios. 

### Grasa corporal
Se tiene una muestra de $20$ mujeres saludables con edades entre $25$ y $34$ años (`data(bodyfat)`  en la librería `isdals`). La medición del porcentaje de grasa corporal es caro y engorroso, por lo tanto se quiere buscar un modelo que proporcione predicciones fiables. Como variables de este modelo se utiliza: **`Triceps`:** pliegue cutáneo del tríceps (cm),  **`Thigh`:** circunferencia del muslo (cm), y **`Midarm`:** circunferencia del brazo medio (cm).

La Figura \@ref(fig:bodyfatFigure) muestra la relación entre variables. Podemos observar que hay una relación fuerte del \% de masa corporal con las covariables Triceps y Thigh. Además, hay una relación lineal fuerte entre esas dos covariables.

```{r bodyfatFigure, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de grasa corporal. Diagrama de dispersión.",warning=FALSE,message = FALSE}
library(isdals)
data(bodyfat)
plot(bodyfat)
```
El modelo propuesto es:
\[
\mbox{Fat}_{i} = \beta_{0} + \mbox{Triceps}_{i}\beta_{1} + \mbox{Thigh}_{i}\beta_{2} + \mbox{Midarm}_{i}\beta_{3} + \varepsilon_{i}.
\]
El ajuste del modelo es el siguiente:
```{r BodyfatFit, echo=T, warning=FALSE,message = FALSE}
mod.bodyfat = lm(Fat ~ Triceps+Thigh+Midarm,data=bodyfat)
summary(mod.bodyfat)
```
Asi como en el caso anterior, observamos que el modelo explica gran parte de la variabilidad del \% de grasa corporal. Sin embargo, los valores $p$ de las pruebas individuales sobre los coeficientes son altos.

## Multicolinealidad
Para el modelo:
\[
\by = \bX\bbeta + \bvarepsi,
\]
el estimador de $\bbeta$ es $\hatbbeta = (\bX'\bX)^{-1}\bX'\by$. Además, se tiene que $V(\hatbbeta) = \sigma^{2}(\bX'\bX)^{-1}$. Por lo tanto, se requiere que $\bX$ sea de rango completo. Es decir, que no hayan columnas que sean linealmente dependientes. En algunos casos, las columnas de $\bX$ son casi linealmente dependientes o colineales,lo que lleva a que $\bX'\bX$ sea casi singular, lo que puede provocar problemas a la hora de hacer inferencias. 

Sea $\bx_j$ la $j$-ésima columna de la matriz $\bX$, por lo tanto $\bX = (\bONE,\bx_{1},\ldots,\bx_{p-1})$. Los vectores $\bx_{1},\bx_{2},\ldots,\bx_{p-1}$ son linealmente dependientes si hay conjunto de constantes $a_{1},a_{2},\ldots, a_{p-1}$ no todas igual a cero, tal que:
\[
\sum_{j=1}^{p-1}a_{j}\bx_{j} = c, \mbox{ donde }c\mbox{ es una constante.}
\]
Si esto se cumple para un subconjunto de $\bX$, el rango de la matriz $\bX'\bX$ es menor que $p$, y por lo tanto, $(\bX'\bX)^{-1}$ no existe. Si la relación es aproximada $(\sum_{j=1}^{p-1}a_{j}\bx_{j} \approx c)$, existe el problema de multicolinealidad.

Vamos a ilustrar el efecto de la multicolinealidad con un ejemplo sencillo. Consideremos un modelo lineal con dos covariables:
$$
y_{i}^{*} = z_{i1}\beta_1 +z_{i2}\beta_2 +\varepsilon_i, 
$$
donde las covariables están escaladas con longitud unitaria. Esto es:
$$
y_{i}^{*} = \frac{y_{i}-\bar{y}}{\sqrt{SST}}, \mbox{ y } z_{ij} = \frac{x_{ij} - \bar{x}_{j}}{\sqrt{s_{jj}}},
$$
donde $s_{jj} = \sum_{i=1}^{n}(x_{ij}-\bar{x}_{j})^{2}$. Además, sea $\bZ$ la matriz de covariables. Con esta transformación, tenemos que $\bZ'\bZ$ es la matriz de correlación de las covariables, 
$$
\bZ'\bZ = \begin{pmatrix}
1 & r_{12} \\ r_{12} & 1
\end{pmatrix},
$$
y $\bZ'\by$ es el vector de correlaciones entre $y$ y las dos covariables:
$$
\bZ'\by^{*} = \begin{pmatrix}
r_{y1} \\ r_{y2}
\end{pmatrix}.
$$
Por lo que el estimador por MCO de $\bb$ está dado por:
$$
\hatbb = \begin{pmatrix}
1 & r_{12} \\ r_{12} & 1
\end{pmatrix}^{-1}\begin{pmatrix}
r_{y1} \\ r_{y2}
\end{pmatrix} = \begin{pmatrix}
\frac{1}{1-r_{12}^{2}} & \frac{-r_{12}}{1-r_{12}^{2}} \\ \frac{-r_{12}}{1-r_{12}^{2}} & \frac{1}{1-r_{12}^{2}} \end{pmatrix} \begin{pmatrix}
r_{1y} \\ r_{2y}
\end{pmatrix}.
$$
Particularmente, tenemos que:
$$
\hatb_{1} = \frac{r_{1y}-r_{12}r_{2y}}{1-r_{12}^{2}} \mbox{ y } \hatb_{2} = \frac{r_{2y}-r_{12}r_{1y}}{1-r_{12}^{2}}.
$$
Además, la matriz de varianzas-covarianzas de $\hatbb$ es:
\begin{equation}
V(\hatbb) = \sigma^{2} \begin{pmatrix}
\frac{1}{1-r_{12}^{2}} & \frac{-r_{12}}{1-r_{12}^{2}} \\ \frac{-r_{12}}{1-r_{12}^{2}} & \frac{1}{1-r_{12}^{2}} \end{pmatrix}.
(\#eq:varbb)
\end{equation}
En \@ref(eq:varbb) podemos observar que la varianza de los coeficientes $\hatbb$ tienden a infinito cuando $|r_{12}|\rightarrow 1$. Mientras que se hace mínima cuando las covariables están incorrelacionadas $(r_{12}=0)$. Es decir, una fuerte correlación entre $x_1$ y $x_2$ da como resultado grandes varianzas y covarianzas para los coeficientes $\hatbb$. 

Por esta razón es preferible tener covariables que sean ortogonales. Puesto que así se garantiza la menor varianza posible para $\hatbbeta$. Sin embargo, las covariables son díficiles de controlar en estudios observacionales. 

Ahora consideremos un modelo con $p-1$ covariables,
$$
y_{i}^{*} = b_{1}z_{i1} + b_{2}z_{i2} + \ldots + b_{p-1}z_{i,p-1} + \varepsilon_{i}.
$$
Las ecuaciones normales son:
$$
\begin{pmatrix}
1 & r_{12} & r_{13} & \ldots & r_{1,p-1}  \\ r_{12} & 1 & r_{23} & \ldots & r_{2,p-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
r_{1,p-1} & r_{2,p-1} & r_{3,p-1} & \ldots & 1
\end{pmatrix} \begin{pmatrix}
b_{1} \\ b_{2} \\ \vdots \\ b_{p-1}
\end{pmatrix} = \begin{pmatrix}
r_{y1} \\ r_{y2} \\ \vdots \\ r_{y,p-1}
\end{pmatrix}.
$$
Por lo que el estimador de $\bb$ es: 
$$
\hatbb = \bR^{-1}\bZ'\by^{*}, \mbox{donde }\bR = \bZ'\bZ.
$$
La Varianza de $\hatbb$ está determinada por:
\[
V(\hatbb)=\sigma^{2}\bR^{-1} = \sigma^{2} \begin{pmatrix}
1 & \br' \\ \br & \bR_{22},
\end{pmatrix}^{-1},
\]
donde $\br = (r_{12},r_{13},\ldots,r_{1,p-1})$ y $\bR_{22}$ es la matriz $\bR$ eliminando la primer fila y columna. Particularmente para $\hatb_1$, tenemos que:
\[
V(\hatb_1) = \sigma^{2}\frac{1}{1 - \br'\bR_{22}^{-1}\br}.
\]

Ahora asumamos que hacemos una regresión de $Z_1$ en función de los demás regresores. Esto es:
\[
z_1 = \alpha_1 z_2 + \alpha_2 z_3 + \ldots + \alpha_{p-2} z_{p-1}+\varepsilon_1.
\]
Las sumas de cuadrados totales y de los residuos de este modelo son:
\[
SST = \sum_{i=1}^{n}z_1^2 = 1,
\]
y
\[
\Sres = SST - \Sreg = 1 - \bz_1' \bZ_{(1)}(\bZ_{(1)}'\bZ_{(1)})^{-1}\bZ_{(1)}'\bz_1= 1 - \br'R_{22}^{-1}\br,
\]
respectivamente. Además, el coeficiente de determinación es:
\[
R^{2}_1 = 1 - \frac{\Sres}{\Stotal} = \br'R_{22}^{-1}\br,
\]
donde $\bZ_{(1)}$ es la matriz $\bZ$ sin la primer columna.

De aquí obtenemos que:
\[
V(\hatb_1) = \sigma^{2} \frac{1}{1-R^{2}_1}.
\]
De forma similar se puede demostrar que:
\[
V(\hatb_j) = \sigma^{2} \frac{1}{1-R^{2}_j}, j=1,\ldots,p-1,
\]
donde $R^{2}_{j}$ es el coeficiente de determinación de la regresión de $z_{j}$ en función de las $(p-2)$ covariables restantes. Por lo que, si $R^{2}_{j} \rightarrow 1$, entonces $V(\hatb_{j})\rightarrow \infty$.

La multicolinealidad también produce que los estimadores $\hatb_{j}$ sean muy grandes en valor absoluto. Considere: 
$$
L_{1}^{2} = (\hatbb - \bb)'(\hatbb - \bb) = \sum_{j=1}^{p}(\hatb_{j}-b_{j})^{2}.
$$ 
El valor esperado de $L_{1}^{2}$ es:
$$
E(L_{1}^{2}) = \sum_{j=1}^{p-1}E(\hatb_{j}-b_{j})^{2} = \sum_{j=1}^{p-1}\sigma^{2}\tr\bR^{-1}
$$
Dado que la traza de una matriz es igual a la suma de sus valores propios, tenemos que:
$$
E(L_{1}^{2}) = \sigma^{2}\sum_{j=1}^{p-1}1/\lambda_{j},
$$
donde $\lambda_{j} > 0$, para $j=1,\ldots,p-1$, son los valores propios de $\bR$. Por lo tanto, si $\bR$ está mal condicionada debido a la multicolinealidad, al menos un $\lambda_{j}$ será muy pequeño.

Cabe notar que, la multicolinealidad no viola ningún supuesto sobre los errores. Por lo tanto, el estimador por MCO siguen siendo BLUE. Pero, en presencia de multicolinealidad:

- Las varianzas de los coeficientes estimados $(\hatbeta_{j})$ se incrementan. Lo que reduce los valores $t$ asociados.

- Los $\hatbeta_{j}$ son sensibles a las especificaciones. Estos pueden cambiar drásticamente cuando se agregan o eliminan covariables.

- Sin embargo, el ajuste general del modelo, y por lo tanto las predicciones, no se verá afectado.

### Causas de la multicolinealidad
La multicolinealidad se puede deber a múltiples razones, entre las que están:

- Con frecuencia se presentan problemas donde intervienen procesos de producción o químicos, donde los regresores son los componentes de un producto y ésos suman una constante.

- Variables que son componentes de un sistema pueden mostrar dependencias casi lineales debido a las limitaciones biológicas, físicas o químicas del sistema.

- Variables que tienen una tendencia común y evolucionan de forma muy parecida en el tiempo.

- Inclusión de variables irrelevantes en el modelo. La información que contienen estas variables ya estarían incluidas en otras y no aportan a la explicación de la variabilidad de la variable respuesta.

Por ejemplo, en los datos del cemento, tenemos que el problema de multicolinealidad se presenta porque:
$$
x_{i1} + x_{i2} + x_{i3} + x_{i4} \approx \mbox{constante}.   
$$
En los datos de grasa corporal, el problema es causado por la alta correlación entre el pliegue cutáneo del triceps $(X_{1})$ y la circunferencia del muslo $(X_{2})$.

## Detección de multicolinealidad
Dado que la multicolinealidad provoca una inflación de la varianza, un indicio de este problema esta en que aunque el modelo presenta un buen ajuste (un $R^2$ por ejemplo), las estimaciones de coeficientes asociados a covariables releventas tienen valores $t$ pequeños. Además, al eliminar covariables, las estimacioes cambian considerablemente. 

A manera inicial, la multicolinealidad se puede detectar por medio de la evaluación de la matriz de correlación. Sin embargo, hay indicadores más formales para ello. Estos son los factores de inflación de varianza (VIF), y los índices de condiciones de $\bR$ o $\bZ$.

### Factores de inflación de varianza
Dado que $V(\hatbb) = \sigma^2\bR^{-1}$, los elementos de la diagonal de $\bR^{-1}$ son buenos indicadores de multicolinealidad:
$$
\{\bR^{-1}\}_{jj} = (1-R_{j}^{2})^{-1},
$$
donde $R_{j}^{2}$ es el coeficiente de determinación de ajustar un modelo de $x_{j}$ en función de las demás covariables. Si $x_{j}$ es casi ortogonal a las demás covariables, $R^{2}_{j}$ es pequeño, y por lo tanto, $\{\bR^{-1}\}_{jj}$ cercano a $1$. Mientras que, si $x_{j}$ es casi linealmente dependiente a las demás covariables, entonces $R^{2}_{j}$ es cercano a $1$ y $\{\bR^{-1}\}_{jj}$ grande.

Por lo tanto, $\{\bR^{-1}\}_{jj}$ puede ser visto como un factor de cuanto se incrementa $V(\hatb_{j})$ debido a la colinealidad entre las covariables. De aquí el nombre de factor de inflación de varianza (VIF). Generalmente, uno o mas valores grandes de VIF ($5$ o $10$) es un indicador de problemas de multicolinealidad.

### Valores propios de $\bZ'\bZ$
Dado que $\bR = \bZ'\bZ$ es una matriz simétrica y positiva semi-definida, entonces se puede descomponer de la siguiente forma:
$$
\bR = \bT\bLambda\bT',
$$
donde $\bT = (\bt_{1},\ldots,\bt_{k})$ es una matriz ortogonal de vectores propios, y $\bLambda$ una matriz diagonal con los valores propios $\lambda_{j}$, para $j=1,\ldots,p-1$, en la diagonal principal. Por lo tanto:
$$
V(\hatbbeta) = \sigma^{2}\bT \bLambda^{-1}\bT'.
$$
Entonces, 
$$
V(\hatb_{j}) = \sigma^{2}\sum_{k=1}^{p-1}\frac{t_{jk}^{2}}{\lambda_{k}}.
$$
De la expresión anterior,
$$
VIF_{j} = \sum_{j=1}^{p-1}\frac{t_{ij}^{2}}{\lambda_{j}},
$$
Por lo tanto, uno o más valores propios pequeños pueden inflar la varianza de $\hatb_{j}$. De aquí salen los indicadores llamados índices de condición y número de condición. Los índices de condición están definidos como:
$$
\eta_{j} = \frac{\lambda_{\mbox{max}}}{\lambda_{j}}.
$$
El número de condición está definido como el máximo índice de condición. Un número de condición mayor de $100$ es un indicador de multicolinealidad.

### Valores singulares de $\bZ$
El número de condición también se puede calcular a partir de la descomposición en valores singulares (SVD) de la matriz de covariables $\bZ$. Esto es:
$$
\bZ =\bU\bD\bT',
$$
donde $\bU$ y $\bT$ son matrices $n\times (p-1)$ y $(p-1)\times (p-1)$, respectivamente, $\bU\bU = \bI$ y $\bT'\bT = \bI$, y $\bD$ es una matriz diagonal con elementos $\mu_{j}$, $j=1,\ldots,p-1$, llamados valores singulares. Hay una relación entre los valores singulares de $\bZ$ y los valores propios de $\bZ'\bZ$:
$$
\bZ'\bZ = (\bU\bD\bT')'\bU\bD\bT' = \bT\bD^2\bT' = \bT\bLambda\bT'.
$$
El índice de condición de $\bZ$ está definido como:
$$
\kappa_{j} = \frac{\mu_{\mbox{max}}}{\mu_{j}}.
$$
El número de condición de $\bZ$ está definido como el máximo $\kappa_{j}$. Un número de condición mayor de 10, 15 o 30 es un indicador de problemas de multicolinealidad.

Note que $\kappa_{j} = \sqrt{\eta_{j}}$. Por lo que el punto de corte de $10$ para el número de condición de $\bZ$ es equivalente al número de condición de $100$ para el número de condición de $\bZ'\bZ$.

### Proporciones de descomposición de varianza
Hay una relación entre el VIF, los valores propios y los valores singulares. Esta es:
$$
VIF_{j} = \sum_{j=1}^{p-1}\frac{t_{ij}^{2}}{\lambda_{j}} = \sum_{j=1}^{p-1}\frac{t_{ij}^{2}}{\mu_{j}^2}.
$$
De aquí salen los indicadores llamados proporciones de descomposición de varianza. Estos se calculan de la siguiente forma:
$$
\pi_{ij} = \frac{t_{ij}^2 / \mu_{i}^2 }{VIF_{j}}, j = 1,\ldots, p-1.
$$
Si se agrupan los $\pi_{ij}$ en una matriz $(p-1) \times (p-1)$ $\bpi$, la columna $j$ son proporciones de la varianza de $\hatbb$. Valores $\pi_{ij}$ mayores de $0.5$ indican problemas de multicolinealidad.  

## Datos de cemento
Los VIF para el modelo ajustado para los datos de cemento son los siguientes:
```{r CementVIF, echo=T, warning=FALSE,message = FALSE}
car::vif(mod.cement)
```
Aquí vemos que todos los VIFs son muy altos, mostrando que hay problemas graves de multicolinealidad.

Para calcular los índices de condición (a partir de los valores singulares de $\bZ$) y las proporciones de descomposición de varianza se utiliza la función ``colldiag()`` de la librería ``perturb``:
```{r CementIindiceCondicion,echo=T, warning=FALSE,message = FALSE}
library(perturb)
colldiag(mod.cement,scale = TRUE,center = TRUE,add.intercept = FALSE)
```
El número de condición $(37.106)$ es muy alto reafirmando el problema de multicolinealidad. Además, también se puede observar que varias de las proporciones de descomposición de varianza son mayores de $0.5$.

## Datos de grasa corporal
Para el modelo ajustado a los datos de grasa corporal, los VIF son:
```{r BodyfatVIF, echo=T, warning=FALSE,message = FALSE}
car::vif(mod.bodyfat)
```
Además, los índices de condición y las proporciones de descomposición de varianza son:

```{r BodyfatIindiceCondicion,echo=T, warning=FALSE,message = FALSE}
library(perturb)
colldiag(mod.bodyfat,scale = TRUE,center = TRUE,add.intercept = FALSE)
```
Todos estos indicadores muestran que hay problemas de multicolinealidad.

## Solución al problema de multicolinealidad
Los problemas de multicolinealidad se pueden solucionar de las siguientes formas:

- **recolección de datos adicionales:** si se tiene control sobre las covariables, es posible tomar más observaciones para romper con la casi dependencia en $\bX$. Pero esto en muchas casos es díficil por la naturaleza de las covariables. Por ejemplo, sería imposible buscar personas con circunferencia del muslo grande y pliegue cutáneo del tríceps bajo (o viceversa). 

- **re-especificación del modelo:** se pueden eliminar del modelo las covariables que me generan el problema de multicolinealidad y que pueden tener poco aporte explicativo dentro del modelo. 

```{r BodyfatFit0, echo=F, warning=FALSE,message = FALSE}
mod.bodyfatSummary = summary(mod.bodyfat)
mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat)
mod.bodyfat0Summary = summary(mod.bodyfat0)
```

Por ejemplo, en los datos de la grasa corporal podemos eliminar la variable asociada al muslo (o al tríceps, pues ambas proporcionan la misma información). Si quitamos esta covariable:
```{r BodyfatFit2, echo=T, warning=FALSE,message = FALSE}
mod.bodyfat0 = lm(Fat ~ Triceps+Midarm,data=bodyfat)
car::vif(mod.bodyfat0)
```

los VIF disminuyen considerablemente. Además, no hay una reducción notable del $R^2$. Con las tres covariables temenos que $R^{2}=`r round(mod.bodyfatSummary$r.squared,3)`$. Mientras que al remover ``Thigh``, tenemos que $R^{2} = `r round(mod.bodyfat0Summary$r.squared,3)`$

- **Uso de estimadores sesgados:** Considerando que el estimador por MCO es el mejor estimador lineal insesgado de $\bbeta$, podemos relajar la condición de insesgamiento y buscar estimadores que aunque sean sesgados tengan menor varianza que los estimadores por MCO. Dos de estas alternativas son el estimador de ridge y el estimadores por componentes principales.


## Estimador de ridge
El estimador de ridge tiene como objetivo minimizar la siguiente suma de cuadrados penalizada:
\begin{equation}
\begin{split}
S_{k}(\bb) &= \sum_{i=1}^{n}(y_{i} - \bz_{i}'\bb)^{2} + k \sum_{j=1}^{p-1}\bb_{j}^2 \\
&= (\by - \bZ\bb)'(\by - \bZ\bb) + k ||\bb||_{2}^{2},
\end{split}
\nonumber
\end{equation}
con $k \geq 0$ (el cual es seleccionado por el investigador). Lo que lleva a las siguientes ecuaciones normales:
\[
(\bR + k \bI)\hatbb_{k} = \bZ'\by.
\]
La solución de las ecuaciones normales lleva al estimador ridge:
\[
\hatbb_{k} = (\bR + k \bI)^{-1}\bZ'\by.
\]
Note que, incluso si $\bR$ es no es invertible, un $k > 0$ resuelve el problema. Además, la solución depende de $k$ (por cada $k$, hay una estimación diferente). 

Se puede considerar a $k$ como un parámetro de contracción. Si $k \rightarrow 0$, tenemos que $\hatbb_{R} \rightarrow \hatbb$. Mientras que, si $k \rightarrow \infty$, encontramos que $\hatbb_{R} \rightarrow \bZERO$ (excepto el intercepto).

Ahora veamos las propiedades de $\hatbb_{k}$. Este se puede expresar como:
\[
\hatbb_k = (\bR + k \bI)^{-1}\bR\by = (\bI + k \bR^{-1})^{-1}\hatbb = \bC \hatbb.
\]
Por lo que, su valor esperado es:
\[
E(\hatbb_{k}) =\bC\bb \neq \bb,
\]
de aquí vemos que $\hatbb_{k}$ es sesgado, y el sesgo aumenta con $k$ (por lo que este parámetro también es llamado de sesgo). La varianza de $\hatbb_k$ está dada por:
\[
V(\hatbb_{k}) = \sigma^2 \bC' \bR^{-1} \bC.
\]
A partir de las cantidades anteriores se puede determinar el error cuadrático medio (ECM) de $\hatbb_{R}$:
\begin{equation}
\begin{split}
\mbox{MSE}(\hatbb_{k}) &= \tr V(\hatbb_k) + \left[ E(\hatbb_{k}) - \bb \right]'\left[ E(\hatbb_{k}) - \bb \right] \\
&= \sigma^2\tr \left( R^{-1}\bC'\bC \right) + \bb'(\bC-\bI)'(\bC-\bI)\bb \\
&=\sigma^{2}\sum_{j=1}^{p-1}\frac{\lambda_{j}}{(\lambda_{j}+ k)^2} + \sum_{j=1}^{p-1}\frac{\alpha_j^{2}k^2}{(\lambda_{j}+ k)^2},
\end{split}
\nonumber
\end{equation}
donde $\balpha = \bT'\bb$.

Finalmente, la suma de cuadrados de los residuos usando $\hatbb_{k}$ es:
\[
SS_{res} =(\by - \bZ\hatbb)'(\by - \bZ\hatbb) + (\hatbb_{k}-\hatbb)'\bR(\hatbb_{k}-\hatbb).
\]
A partir de estos resultados, vemos que: (1) Si $k$ crece, disminuye la varianza, pero aumenta el sesgo. (2) Si $k$ disminuye, aumenta la varianza, pero disminuye el sesgo. (3) el estimador ridge proporciona un $R^{2}$ mas pequeño que el estimador por MCO. Sin embargo, las estimaciones de $\bb$ son más estables.

La idea de la regresión de ridge es encontrar un valor de $k$ tal que  $\mbox{ECM}(\hatbb_{k}) < V(\hatbb)$. En la Figura \@ref(fig:ECMrepresentacion) podemos ver la representación del ECM del estimador de ridge. A medida que aumenta $k$, el sesgo incrementa y la varianza disminuye. Además, hay una región de $k$ donde se puede obtener un ECM del estimador de ridge menor que el que se obtiene por medio de MCO. La idea es encontrar el valor de $k$ que minimiza el ECM, o por lo menos algún valor en la región donde $\mbox{ECM}(\hatbb_{k}) < V(\hatbb)$.


```{r ECMrepresentacion, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Representación del sesgo al cuadrado (linea cortada), varianza (linea punteada) y error cuadrático medio (linea solida) del estimador de ridge. La linea roja representa el error cuadrático medio del estimador por MCO.",warning=FALSE,message = FALSE}
x= seq(from=0,to=0.3,length.out = 100)
sesgo = 0.25/(1+exp(-20*(x-0.001)))
sesgo = sesgo - min(sesgo) 
vari = 0.06*exp(-70*x)+ 0.0025
ecm =sesgo+vari
plot(x,sesgo,type = 'l',xlim=c(0,0.2),ylim=range(c(sesgo,vari,ecm)),lty=2, xaxt='n',yaxt='n',
     xlab='k',ylab='Sesgo, varianza y MSE')
lines(x,vari,lty=3)
lines(x,ecm,lwd=2)
abline(h=max(vari),col=2)
lines(c(x[ecm==min(ecm)],x[ecm==min(ecm)]),c(-1,min(ecm)),col=4)
lines(c(-1,x[ecm==min(ecm)]),c(min(ecm),min(ecm)),col=4)
axis(1,x[ecm==min(ecm)],'k óptimo')
```
Algunos métodos de selección de $k$ son:

- **Traza de ridge $k$:** el efecto de $k$ sobre las estimaciones de $\hatbb_{k}$ es mas fuerte para valores bajos. De igual forma, si $k$ es muy grande se introduce mucho sesgo. Por lo que se puede hacer es incrementar $k$ hasta que parezca que su influencia sobre $\hatbb_{k}$ se atenúe.

- **Validación cruzada (CV):** sea $\haty_{(i),k}$ la estimación de $E(y_i)$ por medio del estimador de ridge con el parámetro $k$ y usando una muestra excluyendo la i-ésima observación. La validación cruzada está definida como:
$$
CV(k) = \sum_{i=1}^{n} (y_i - \haty_{(i),k})^2.
$$
Por lo que la selección de $k$ es:
$$
k_{CV} = \arg \min_{k} CV(k).
$$

### Datos de cemento
Para ajustar el modelo usando el estimador de ridge podemos usar la función ``lmridge`` del paquete ``lmridge``. Primero, ajustamos el modelo usando diferentes valores de $k$:
```{r cementridge1,echo=T, warning=FALSE,message = FALSE}
library(lmridge)
K = seq(from=0,to=0.3,length.out = 100)
ridge.cement = lmridge(y~., data=cement,K=K,scaling='sc')
```
En el objeto ``ridge.cement`` tenemos las estimaciones por el estimador de ridge para $100$ valores de $k$ entre $0$ y $0.3$. Para observar como cambian las estimaciones para los diferentes valores de $k$ podemos graficar la traza de ridge así (en términos de las covariables en su escala orginal):
```{r CementTrazaRidge,echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de cemento. Traza de ridge. Coeficiente asociado a ``X1`` (negro), coeficiente asociado a ``X2`` (rojo), coeficiente asociado a ``X3`` (verde) y coeficiente asociado a ``X4`` (azul)",warning=FALSE,message = FALSE}
EstRidge.cement = coef(ridge.cement)
plot(K,EstRidge.cement[,2],type='l',ylim=range(EstRidge.cement[,-1]),lwd=2,
     ylab='Estimaciones de los coeficientes',xlab='k')
lines(K,EstRidge.cement[,3],col=2,lwd=2)
lines(K,EstRidge.cement[,4],col=3,lwd=2)
lines(K,EstRidge.cement[,5],col=4,lwd=2)
abline(h=0,lty=2)
```
En la Figura \@ref(fig:CementTrazaRidge) podemos observar que, cuando incrementamos $k$, las estimaciones cambian rápidamente y luego parecen estabilizarse cuando $k$ es grande. Además hay un cambio de signo para el coeficiente asociado a ``X3``. También puede usarse ``plot(mod.r)`` (traza de ridge para las estimaciones de los coeficientes asociados a las covariables escaladas).

La selección del $k$ óptimo por medio de validación cruzada (CV) se hace de la siguiente manera:
```{r cementridge3, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "\\label{fig:CementCVridge} Datos de cemento. Validación cruzada.",warning=FALSE,message = FALSE}
Criterios.cement = kest(ridge.cement)
plot(K,Criterios.cement$CV,type='l',xlab='K',ylab='validación cruzada')
K[Criterios.cement$CV==min(Criterios.cement$CV)]
```
Aquí podemos ver que el valor de $k$ que minimiza la validación cruzada es $0.009$. El valor óptimo de $k$ por medio de otros criterios son: 
```{r cementridge4,echo=T, warning=FALSE,message = FALSE}
Criterios.cement
```
La estimación con $K=0.0101$ se puede obtener usando la función ``lmridge`` usando $K=0.0101$ como argumento:
```{r cementridge5,echo=T, warning=FALSE,message = FALSE}
ridge.cement2 = lmridge(y~., data=cement,K=0.0101,scaling='sc')
summary(ridge.cement2)
```
Vemos algunas diferencias con las estimaciones por MCO. Hay un cambio de signo en la estimación del coeficiente asociado a ``x3``. Si embargo, no hay mucha diferencia en las estimaciones de los otros coeficientes. También podemos observar que las covariables ``x1``, ``x2`` y ``x3`` ahora tienen un aporte significativo. Como era de esperarse, hay una disminución del $R^{2}$, sin embargo es muy leve.

## Estimador por componentes principales
Considere el modelo en su forma canónica:
$$
\by^{*} = \bZ\bT\balpha + \bvarepsi,
$$
donde $\balpha = \bT'\bb$, $(\bZ\bT)'\bZ\bT=\bLambda$, $\bLambda$ es una matriz diagonal de valores propios $(\lambda_1,\ldots,\lambda_p)$ de $\bZ'\bZ$, y $\bT = (\bt_{1},\ldots,\bt_{p-1})$ es la matriz ortogonal de vectores propios asociados $\bLambda$.

Las columnas de $\bP = \bZ\bT = (\bp_{1},\ldots,\bp_{p-1})$ son un conjunto de regresores ortogonales (llamados componentes principales):
$$
\bp_{k} = \sum_{j=1}^{p-1} t_{kj}\bz_{j}.
$$



El estimador de $\balpha$ por MCO es:
$$
\hatbalpha = (\bP'\bP)^{-1}\bP'\by^{*} = \bLambda^{-1}\bP'\by^{*},
$$
y la varianza de $\hatbalpha$ es:
$$
V(\hatbalpha) = \sigma^{2}(\bP'\bP)^{-1} = \sigma^{2}\bLambda^{-1}.
$$
De aquí podemos ver que valores propios de $\bR$ están asociados con la varianza de los coeficientes de regresión. Si $\lambda_j=1$ (para $j=1,\ldots,p$), las covariables originales son ortogonales. Mientras que valores propios cercanos a cero indican problemas de multicolinealidad dado que inflan la varianza de $\hatbalpha$. 

Note que, para $\hatbb = \bT\hatbalpha$, tenemos:
\[
V(\hatbb) = V(\bT\hatbalpha) =\sigma^{2}\bT\bLambda^{-1}\bT' = \sigma^{2} \sum_{j=1}^{p-1}\frac{\bt_{j}\bt_{j}'}{\lambda_{j}},
\]
lo que implica que $V(\hatb_{k}) = \sigma^{2} \sum_{j=1}^{p-1}t_{kj}^{2}/\lambda_{j}$, la varianza de $\hatb_j$ es una combinación lineal de los valores propios.

Para combatir el problema de multicolinealidad, la idea de la regresión por componentes principales es usar un subconjunto de $\bP$ como regresores (en vez de todos los compontes). Para esto, eliminamos los componentes principales $(\bp_{r+1},\ldots,\bp_{p-1})$ asociados a los valores propios cercanos a cero $(\lambda_{r+1},\ldots,\lambda_{p-1})$. Aquí estamos asumiendo que $\lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_{p-1}$. Esto es,
$$
\hatbalpha_{r} = (\overbrace{\hatalpha_{1},\hatalpha_{2},\ldots, \hatalpha_{r}}^{r},\overbrace{0, \ldots,0}^{p-1-r})'.
$$
De esta forma el estimador de $\bb$ por CP está dado por:
\[
\hatbb_{r} = \bT \hatbalpha_{r} =\sum_{l=1}^{r} \bt_l \hatalpha_l,
\]

El sesgo de $\hatbb_{r}$ es igual a:
\[
\hatbb_{r} - \bb = \sum_{l=1}^{r} \bt_l \hatalpha_l -  \sum_{l=1}^{p-1} \bt_l \alpha_l = - \sum_{l = r+1}^{p-1} \bt_l \alpha_l.
\]
La varianza de $\hatbb_{r}$ es:
\[
V(\hatbb_{r}) = \bT V(\hatbalpha_{r})\bT' =  \sigma^{2}\sum_{l=1}^{r} \lambda_{l}^{-1}\bt_{l}\bt_{l}' \leq \sigma^{2} \sum_{l=1}^{p-1}\lambda_l^{-1}\bt_{l}\bt_{l}' = V(\hatbb).
\]
Por lo tanto, al eliminar componentes principales se aumenta el sesgo, pero se disminuye la varianza de $\hatbb_r$. Finalmente, el ECM de $\hatbb_r$ está dado por:
\[
MSE(\hatbb_r) = \sigma^{2}\sum_{i=1}^{r}\frac{1}{\lambda_l} + \sum_{l=r+1}^{p-1}\alpha_l^2.
\]

### Datos de cemento
Antes de ajustar el modelo vamos a escalar las variables
```{r cementRCP,echo=T, warning=FALSE,message = FALSE}
escalar <- function(x) {(x-mean(x)) / sqrt(sum((x-mean(x))^2))}
X = as.matrix(cement[,1:4])
y.e = escalar(cement$y)
Z = apply(cement[,1:4],2,escalar)
```
A partir de las variables escaladas podemos calcular los vectores y valores propios:
```{r cementRCP2,echo=T, warning=FALSE,message = FALSE}
T.mat = eigen(t(Z)%*%Z)$vectors
lambda = eigen(t(Z)%*%Z)$values
lambda
```
Aquí podemos observar que un valor propio es cercano a cero, por lo que lo podemos eliminarlo para remediar el problema de multicolinealidad:
```{r cementRCP3,echo=T, warning=FALSE,message = FALSE}
P = Z%*%T.mat
PCR.cement = lm(y.e~P[,-4]-1)
summary(PCR.cement)
```
Las estimaciones de los coeficientes $(\bb)$ para las variables escaladas:
```{r cementRCP4,echo=T, warning=FALSE,message = FALSE}
beta.CP =T.mat%*%c(PCR.cement$coefficients,0)
beta.CP
```
La matriz de varianza de $\bb_{r}$ es:
```{r cementRCP5,echo=T, warning=FALSE,message = FALSE}
sigma2.pc = sum(PCR.cement$residuals^2)/(13-4)
Var.b = sigma2.pc*T.mat%*%diag(c(1/lambda[1:3],0))%*%t(T.mat)
Var.b
```